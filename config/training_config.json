{
    "training":{
        "batch_size":1,
        "micro_batch_size":1,
        "epochs":5,
        "learning_rate":5e-5,
        "lr_scheduler":"cosine_with_restarts",
        "weight_decay":0.01,
        "warmup_steps":200,
        "gradient_accumulation_steps":4,
        "max_grad_norm":1.0,
        "max_steps":-1,
        "gradient_checkpointing":true,
        "logging_steps":100,
        "eval_steps":300,
        "save_steps":600,
        "save_total_limit":3,
        "use_accelerate":true,
        "accelerate_config_path":"./config/accelerate_config.json",
        "cpu_offload":false,
        "dataloader_workers":4,
        "async_prefetch":true,
        "use_memory_map":true,
        "memmap_dir":"./data/memmap",
        "per_device_train_batch_size":24,
        "per_device_eval_batch_size":24,
        "fp16":true,
        "resume_from_checkpoint": "/root/autodl-tmp/zishu-sensei/output/checkpoint-7500"
    },
    "data":{
        "train_file":"./data/train/train.json",
        "validation_file":"./data/train/val.json",
        "preprocessing":{
            "max_length":1024,
            "text_column":"text",
            "add_eos_token":true
        }
    },
    "lora_config":{
        "r":16,
        "lora_alpha":32,
        "bias":"none",
        "task_type":"CAUSAL_LM",
        "target_modules":["q_proj", "k_proj", "v_proj", "o_proj"],
        "use_8bit_adam":true,
        "use_peft":true        
    }
}
