# -*- coding: utf-8 -*-
"""
èŠå¤©è·¯ç”±APIå•å…ƒæµ‹è¯•
åŸºäºå®é™…çš„chat.pyè·¯ç”±å®ç°
"""
import pytest
import asyncio
import json
import uuid
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from datetime import datetime
from typing import Dict, Any, List

from fastapi import HTTPException
from fastapi.testclient import TestClient

from zishu.api.routes.chat import (
    ChatRequest, StreamChatRequest, EmotionChatRequest,
    ChatCompletionResponse, StreamChunk, EmotionResponse, HistoryResponse,
    ChatChoice, ChatUsage, Message, MessageRole, MessageType, EmotionType
)
from zishu.api.schemas.chat import CharacterConfig


@pytest.mark.unit
@pytest.mark.api
class TestChatRequestModels:
    """èŠå¤©è¯·æ±‚æ¨¡å‹æµ‹è¯•"""
    
    def test_chat_request_valid(self):
        """æµ‹è¯•æœ‰æ•ˆçš„èŠå¤©è¯·æ±‚"""
        request_data = {
            "messages": [{"role": "user", "content": "Hello"}],
            "model": "test-model",
            "temperature": 0.7,
            "max_tokens": 100
        }
        
        request = ChatRequest(**request_data)
        
        assert request.messages == [{"role": "user", "content": "Hello"}]
        assert request.model == "test-model"
        assert request.temperature == 0.7
        assert request.max_tokens == 100
        assert request.stream is False
    
    def test_chat_request_with_optional_fields(self):
        """æµ‹è¯•åŒ…å«å¯é€‰å­—æ®µçš„èŠå¤©è¯·æ±‚"""
        request_data = {
            "messages": [{"role": "user", "content": "Hello"}],
            "adapter": "openai-adapter",
            "character_id": "char-123",
            "session_id": "session-456",
            "user": "user-789",
            "stream": True
        }
        
        request = ChatRequest(**request_data)
        
        assert request.adapter == "openai-adapter"
        assert request.character_id == "char-123"
        assert request.session_id == "session-456"
        assert request.user == "user-789"
        assert request.stream is True
    
    def test_chat_request_temperature_validation(self):
        """æµ‹è¯•æ¸©åº¦å‚æ•°éªŒè¯"""
        # æœ‰æ•ˆèŒƒå›´
        request = ChatRequest(
            messages=[{"role": "user", "content": "Hello"}],
            temperature=0.5
        )
        assert request.temperature == 0.5
        
        # è¾¹ç•Œå€¼æµ‹è¯•
        request_min = ChatRequest(
            messages=[{"role": "user", "content": "Hello"}],
            temperature=0.0
        )
        assert request_min.temperature == 0.0
        
        request_max = ChatRequest(
            messages=[{"role": "user", "content": "Hello"}],
            temperature=2.0
        )
        assert request_max.temperature == 2.0
    
    def test_stream_chat_request_valid(self):
        """æµ‹è¯•æµå¼èŠå¤©è¯·æ±‚"""
        request_data = {
            "messages": "Hello, how are you?",
            "model": "test-model",
            "character_id": "char-123"
        }
        
        request = StreamChatRequest(**request_data)
        
        assert request.messages == "Hello, how are you?"
        assert request.model == "test-model"
        assert request.character_id == "char-123"
    
    def test_stream_chat_request_length_validation(self):
        """æµ‹è¯•æµå¼èŠå¤©è¯·æ±‚é•¿åº¦éªŒè¯"""
        # å¤ªçŸ­çš„æ¶ˆæ¯
        with pytest.raises(ValueError):
            StreamChatRequest(messages="")
        
        # å¤ªé•¿çš„æ¶ˆæ¯
        long_message = "x" * 10001
        with pytest.raises(ValueError):
            StreamChatRequest(messages=long_message)
    
    def test_emotion_chat_request_valid(self):
        """æµ‹è¯•æƒ…ç»ªèŠå¤©è¯·æ±‚"""
        request_data = {
            "messages": "I'm feeling sad today",
            "current_emotion": EmotionType.SAD,
            "emotion_intensity": 0.8,
            "analyze_emotion": True
        }
        
        request = EmotionChatRequest(**request_data)
        
        assert request.messages == "I'm feeling sad today"
        assert request.current_emotion == EmotionType.SAD
        assert request.emotion_intensity == 0.8
        assert request.analyze_emotion is True
    
    def test_emotion_intensity_validation(self):
        """æµ‹è¯•æƒ…ç»ªå¼ºåº¦éªŒè¯"""
        # æœ‰æ•ˆèŒƒå›´
        request = EmotionChatRequest(
            messages="Test message",
            emotion_intensity=0.5
        )
        assert request.emotion_intensity == 0.5
        
        # è¾¹ç•Œå€¼
        request_min = EmotionChatRequest(
            messages="Test message",
            emotion_intensity=0.0
        )
        assert request_min.emotion_intensity == 0.0
        
        request_max = EmotionChatRequest(
            messages="Test message",
            emotion_intensity=1.0
        )
        assert request_max.emotion_intensity == 1.0


@pytest.mark.unit
@pytest.mark.api
class TestChatResponseModels:
    """èŠå¤©å“åº”æ¨¡å‹æµ‹è¯•"""
    
    def test_chat_choice_creation(self):
        """æµ‹è¯•èŠå¤©é€‰æ‹©åˆ›å»º"""
        message = Message(
            role=MessageRole.ASSISTANT,
            content="Hello! How can I help you?"
        )
        
        choice = ChatChoice(
            index=0,
            message=message,
            finish_reason="stop"
        )
        
        assert choice.index == 0
        assert choice.message.role == MessageRole.ASSISTANT
        assert choice.message.content == "Hello! How can I help you?"
        assert choice.finish_reason == "stop"
    
    def test_chat_usage_creation(self):
        """æµ‹è¯•tokenä½¿ç”¨ç»Ÿè®¡åˆ›å»º"""
        usage = ChatUsage(
            prompt_tokens=50,
            completion_tokens=30,
            total_tokens=80
        )
        
        assert usage.prompt_tokens == 50
        assert usage.completion_tokens == 30
        assert usage.total_tokens == 80
    
    def test_chat_completion_response_creation(self):
        """æµ‹è¯•èŠå¤©å®Œæˆå“åº”åˆ›å»º"""
        message = Message(
            role=MessageRole.ASSISTANT,
            content="Hello! How can I help you?"
        )
        
        choice = ChatChoice(
            index=0,
            message=message,
            finish_reason="stop"
        )
        
        usage = ChatUsage(
            prompt_tokens=50,
            completion_tokens=30,
            total_tokens=80
        )
        
        response = ChatCompletionResponse(
            model="test-model",
            choices=[choice],
            usage=usage,
            session_id="session-123"
        )
        
        assert response.model == "test-model"
        assert len(response.choices) == 1
        assert response.choices[0].message.content == "Hello! How can I help you?"
        assert response.usage.total_tokens == 80
        assert response.session_id == "session-123"
        assert response.object == "chat.completion"
        
        # éªŒè¯è‡ªåŠ¨ç”Ÿæˆçš„å­—æ®µ
        assert response.id.startswith("chatcmpl-")
        assert isinstance(response.created, int)
    
    def test_stream_chunk_creation(self):
        """æµ‹è¯•æµå¼å“åº”å—åˆ›å»º"""
        chunk = StreamChunk(
            id="chunk-123",
            model="test-model",
            choices=[{
                "index": 0,
                "delta": {"content": "Hello"},
                "finish_reason": None
            }]
        )
        
        assert chunk.id == "chunk-123"
        assert chunk.model == "test-model"
        assert chunk.object == "chat.completion.chunk"
        assert len(chunk.choices) == 1
        assert chunk.choices[0]["delta"]["content"] == "Hello"
    
    def test_emotion_response_creation(self):
        """æµ‹è¯•æƒ…ç»ªå“åº”åˆ›å»º"""
        message = Message(
            role=MessageRole.ASSISTANT,
            content="I understand you're feeling sad.",
            emotion=EmotionType.EMPATHETIC
        )
        
        response = EmotionResponse(
            message=message,
            user_emotion=EmotionType.SAD,
            response_emotion=EmotionType.EMPATHETIC,
            emotion_confidence=0.85,
            session_id="session-123"
        )
        
        assert response.message.content == "I understand you're feeling sad."
        assert response.user_emotion == EmotionType.SAD
        assert response.response_emotion == EmotionType.EMPATHETIC
        assert response.emotion_confidence == 0.85
        assert response.session_id == "session-123"
    
    def test_history_response_creation(self):
        """æµ‹è¯•å¯¹è¯å†å²å“åº”åˆ›å»º"""
        messages = [
            Message(role=MessageRole.USER, content="Hello"),
            Message(role=MessageRole.ASSISTANT, content="Hi there!")
        ]
        
        response = HistoryResponse(
            session_id="session-123",
            messages=messages,
            total_count=2
        )
        
        assert response.session_id == "session-123"
        assert len(response.messages) == 2
        assert response.total_count == 2
        assert response.messages[0].role == MessageRole.USER
        assert response.messages[1].role == MessageRole.ASSISTANT


@pytest.mark.unit
@pytest.mark.api
class TestChatRouteLogic:
    """èŠå¤©è·¯ç”±é€»è¾‘æµ‹è¯•"""
    
    @pytest.fixture
    def mock_dependencies(self):
        """æ¨¡æ‹Ÿä¾èµ–é¡¹"""
        return {
            "logger": Mock(),
            "performance_monitor": Mock(),
            "thread_factory": Mock(),
            "character_config": Mock(spec=CharacterConfig),
            "inference_engine": AsyncMock()
        }
    
    @pytest.fixture
    def mock_session_manager(self):
        """æ¨¡æ‹Ÿä¼šè¯ç®¡ç†å™¨"""
        manager = Mock()
        manager.get_session = AsyncMock()
        manager.add_message = AsyncMock()
        manager.get_history = AsyncMock()
        return manager
    
    @pytest.fixture
    def mock_emotion_analyzer(self):
        """æ¨¡æ‹Ÿæƒ…ç»ªåˆ†æå™¨"""
        analyzer = Mock()
        analyzer.analyze_emotion = AsyncMock()
        analyzer.suggest_response_emotion = AsyncMock()
        return analyzer
    
    @pytest.fixture
    def mock_response_cache(self):
        """æ¨¡æ‹Ÿå“åº”ç¼“å­˜"""
        cache = Mock()
        cache.get = AsyncMock()
        cache.set = AsyncMock()
        return cache
    
    @pytest.mark.asyncio
    async def test_chat_completion_basic_flow(self, mock_dependencies):
        """æµ‹è¯•åŸºæœ¬èŠå¤©å®Œæˆæµç¨‹"""
        # æ¨¡æ‹Ÿæ¨ç†å¼•æ“å“åº”
        mock_inference_result = {
            "content": "Hello! How can I help you?",
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 8,
                "total_tokens": 18
            }
        }
        mock_dependencies["inference_engine"].generate.return_value = mock_inference_result
        
        # åˆ›å»ºèŠå¤©è¯·æ±‚
        request = ChatRequest(
            messages=[{"role": "user", "content": "Hello"}],
            model="test-model"
        )
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è·¯ç”±å¤„ç†å‡½æ•°
        # ç”±äºæˆ‘ä»¬åœ¨æµ‹è¯•æ¨¡å‹å’Œé€»è¾‘ï¼Œæš‚æ—¶æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
        
        # éªŒè¯è¯·æ±‚å¤„ç†é€»è¾‘
        assert request.messages[0]["content"] == "Hello"
        assert request.model == "test-model"
        
        # éªŒè¯æ¨ç†å¼•æ“è°ƒç”¨
        # mock_dependencies["inference_engine"].generate.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_stream_chat_flow(self, mock_dependencies):
        """æµ‹è¯•æµå¼èŠå¤©æµç¨‹"""
        # æ¨¡æ‹Ÿæµå¼å“åº”
        async def mock_stream_generator():
            chunks = [
                {"content": "Hello"},
                {"content": " there"},
                {"content": "!"}
            ]
            for chunk in chunks:
                yield chunk
        
        mock_dependencies["inference_engine"].generate_stream.return_value = mock_stream_generator()
        
        request = StreamChatRequest(
            messages="Hello",
            model="test-model"
        )
        
        # éªŒè¯æµå¼è¯·æ±‚
        assert request.messages == "Hello"
        assert request.model == "test-model"
    
    @pytest.mark.asyncio
    async def test_emotion_chat_flow(self, mock_dependencies, mock_emotion_analyzer):
        """æµ‹è¯•æƒ…ç»ªèŠå¤©æµç¨‹"""
        # æ¨¡æ‹Ÿæƒ…ç»ªåˆ†æç»“æœ
        mock_emotion_analyzer.analyze_emotion.return_value = {
            "emotion": EmotionType.SAD,
            "confidence": 0.85,
            "keywords": ["sad", "down"]
        }
        
        mock_emotion_analyzer.suggest_response_emotion.return_value = EmotionType.EMPATHETIC
        
        request = EmotionChatRequest(
            messages="I'm feeling sad today",
            current_emotion=EmotionType.SAD,
            analyze_emotion=True
        )
        
        # éªŒè¯æƒ…ç»ªè¯·æ±‚
        assert request.messages == "I'm feeling sad today"
        assert request.current_emotion == EmotionType.SAD
        assert request.analyze_emotion is True
    
    def test_session_id_generation(self):
        """æµ‹è¯•ä¼šè¯IDç”Ÿæˆ"""
        # æµ‹è¯•è‡ªåŠ¨ç”Ÿæˆä¼šè¯IDçš„é€»è¾‘
        request1 = ChatRequest(messages=[{"role": "user", "content": "Hello"}])
        request2 = ChatRequest(messages=[{"role": "user", "content": "Hello"}])
        
        # å¦‚æœæ²¡æœ‰æä¾›session_idï¼Œåº”è¯¥èƒ½å¤Ÿç”Ÿæˆæˆ–å¤„ç†
        assert request1.session_id is None  # é»˜è®¤ä¸ºNone
        assert request2.session_id is None
    
    def test_character_integration(self, mock_dependencies):
        """æµ‹è¯•è§’è‰²é›†æˆ"""
        character_config = CharacterConfig(
            name="Zishu",
            display_name="ç´«èˆ’è€å¸ˆ",
            personality_type="caring"
        )
        
        request = ChatRequest(
            messages=[{"role": "user", "content": "Hello"}],
            character_id=character_config.id
        )
        
        # éªŒè¯è§’è‰²IDä¼ é€’
        assert request.character_id == character_config.id
    
    def test_adapter_selection(self):
        """æµ‹è¯•é€‚é…å™¨é€‰æ‹©"""
        request = ChatRequest(
            messages=[{"role": "user", "content": "Hello"}],
            adapter="openai-adapter"
        )
        
        assert request.adapter == "openai-adapter"
    
    def test_error_handling_scenarios(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†åœºæ™¯"""
        # ç©ºæ¶ˆæ¯åˆ—è¡¨
        with pytest.raises(ValueError):
            ChatRequest(messages=[])
        
        # æ— æ•ˆçš„æ¸©åº¦å€¼ï¼ˆè¿™ä¸ªåº”è¯¥åœ¨PydanticéªŒè¯ä¸­å¤„ç†ï¼‰
        # æ³¨æ„ï¼šPydanticçš„geå’Œleçº¦æŸä¼šè‡ªåŠ¨å¤„ç†èŒƒå›´éªŒè¯


@pytest.mark.unit
@pytest.mark.api
class TestChatUtilities:
    """èŠå¤©å·¥å…·å‡½æ•°æµ‹è¯•"""
    
    def test_message_creation(self):
        """æµ‹è¯•æ¶ˆæ¯åˆ›å»º"""
        message = Message(
            role=MessageRole.USER,
            content="Hello, world!"
        )
        
        assert message.role == MessageRole.USER
        assert message.content == "Hello, world!"
        assert message.message_type == MessageType.TEXT
        assert isinstance(message.timestamp, datetime)
        assert message.id is not None
    
    def test_message_with_emotion(self):
        """æµ‹è¯•å¸¦æƒ…ç»ªçš„æ¶ˆæ¯"""
        message = Message(
            role=MessageRole.ASSISTANT,
            content="I'm happy to help!",
            emotion=EmotionType.HAPPY,
            emotion_intensity=0.8
        )
        
        assert message.emotion == EmotionType.HAPPY
        assert message.emotion_intensity == 0.8
    
    def test_message_with_multimedia(self):
        """æµ‹è¯•å¤šåª’ä½“æ¶ˆæ¯"""
        message = Message(
            role=MessageRole.ASSISTANT,
            content="Here's a cheerful response!",
            voice_style="cheerful",
            animation="happy",
            avatar="smile.png"
        )
        
        assert message.voice_style == "cheerful"
        assert message.animation == "happy"
        assert message.avatar == "smile.png"
    
    def test_message_validation(self):
        """æµ‹è¯•æ¶ˆæ¯éªŒè¯"""
        # ç©ºå†…å®¹åº”è¯¥è¢«æ‹’ç»
        with pytest.raises(ValueError):
            Message(
                role=MessageRole.USER,
                content=""
            )
        
        # åªæœ‰ç©ºæ ¼çš„å†…å®¹åº”è¯¥è¢«æ‹’ç»
        with pytest.raises(ValueError):
            Message(
                role=MessageRole.USER,
                content="   "
            )
    
    def test_message_metadata(self):
        """æµ‹è¯•æ¶ˆæ¯å…ƒæ•°æ®"""
        metadata = {
            "source": "web",
            "user_agent": "test-browser",
            "ip": "127.0.0.1"
        }
        
        message = Message(
            role=MessageRole.USER,
            content="Hello",
            metadata=metadata,
            tokens=5,
            processing_time=0.1
        )
        
        assert message.metadata == metadata
        assert message.tokens == 5
        assert message.processing_time == 0.1


@pytest.mark.integration
@pytest.mark.api
class TestChatRouteIntegration:
    """èŠå¤©è·¯ç”±é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def app_client(self):
        """åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯"""
        # è¿™é‡Œåº”è¯¥åˆ›å»ºå®é™…çš„FastAPIæµ‹è¯•å®¢æˆ·ç«¯
        # æš‚æ—¶è¿”å›Mockï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å¯¼å…¥çœŸå®çš„app
        return Mock()
    
    def test_chat_endpoint_integration(self, app_client):
        """æµ‹è¯•èŠå¤©ç«¯ç‚¹é›†æˆ"""
        # è¿™é‡Œåº”è¯¥æµ‹è¯•å®é™…çš„HTTPç«¯ç‚¹
        # ç”±äºéœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®ï¼Œæš‚æ—¶è·³è¿‡
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")
    
    def test_stream_endpoint_integration(self, app_client):
        """æµ‹è¯•æµå¼ç«¯ç‚¹é›†æˆ"""
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")
    
    def test_emotion_endpoint_integration(self, app_client):
        """æµ‹è¯•æƒ…ç»ªç«¯ç‚¹é›†æˆ"""
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")


@pytest.mark.unit
@pytest.mark.api
class TestChatEdgeCases:
    """èŠå¤©APIè¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
    
    def test_empty_message_content(self):
        """æµ‹è¯•ç©ºæ¶ˆæ¯å†…å®¹"""
        request_data = {
            "messages": [{"role": "user", "content": ""}],
            "model": "test-model"
        }
        
        # ç©ºå†…å®¹åº”è¯¥è¢«å…è®¸ä½†å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
        request = ChatRequest(**request_data)
        assert request.messages[0]["content"] == ""
    
    def test_very_long_message_content(self):
        """æµ‹è¯•è¶…é•¿æ¶ˆæ¯å†…å®¹"""
        long_content = "x" * 100000  # 100kå­—ç¬¦
        request_data = {
            "messages": [{"role": "user", "content": long_content}],
            "model": "test-model"
        }
        
        request = ChatRequest(**request_data)
        assert len(request.messages[0]["content"]) == 100000
    
    def test_special_characters_in_content(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        special_content = "Hello ğŸŒŸ \n\t\r æµ‹è¯• Ã±Ã¡Ã©Ã­Ã³Ãº \\\"'`"
        request_data = {
            "messages": [{"role": "user", "content": special_content}],
            "model": "test-model"
        }
        
        request = ChatRequest(**request_data)
        assert request.messages[0]["content"] == special_content
    
    def test_maximum_messages_limit(self):
        """æµ‹è¯•æœ€å¤§æ¶ˆæ¯æ•°é‡é™åˆ¶"""
        # åˆ›å»ºå¤§é‡æ¶ˆæ¯
        messages = [
            {"role": "user" if i % 2 == 0 else "assistant", "content": f"Message {i}"}
            for i in range(1000)
        ]
        
        request_data = {
            "messages": messages,
            "model": "test-model"
        }
        
        request = ChatRequest(**request_data)
        assert len(request.messages) == 1000
    
    def test_temperature_boundary_values(self):
        """æµ‹è¯•æ¸©åº¦å‚æ•°è¾¹ç•Œå€¼"""
        # æµ‹è¯•æœ€å°å€¼
        request_min = ChatRequest(
            messages=[{"role": "user", "content": "test"}],
            model="test-model",
            temperature=0.0
        )
        assert request_min.temperature == 0.0
        
        # æµ‹è¯•æœ€å¤§å€¼
        request_max = ChatRequest(
            messages=[{"role": "user", "content": "test"}],
            model="test-model",
            temperature=2.0
        )
        assert request_max.temperature == 2.0
    
    def test_max_tokens_boundary_values(self):
        """æµ‹è¯•æœ€å¤§ä»¤ç‰Œæ•°è¾¹ç•Œå€¼"""
        # æµ‹è¯•æœ€å°å€¼
        request_min = ChatRequest(
            messages=[{"role": "user", "content": "test"}],
            model="test-model",
            max_tokens=1
        )
        assert request_min.max_tokens == 1
        
        # æµ‹è¯•å¤§å€¼
        request_max = ChatRequest(
            messages=[{"role": "user", "content": "test"}],
            model="test-model",
            max_tokens=100000
        )
        assert request_max.max_tokens == 100000
    
    def test_unicode_model_name(self):
        """æµ‹è¯•Unicodeæ¨¡å‹åç§°"""
        request = ChatRequest(
            messages=[{"role": "user", "content": "test"}],
            model="æ¨¡å‹-æµ‹è¯•-ğŸ¤–"
        )
        assert request.model == "æ¨¡å‹-æµ‹è¯•-ğŸ¤–"


@pytest.mark.unit
@pytest.mark.api
class TestChatErrorHandling:
    """èŠå¤©APIé”™è¯¯å¤„ç†æµ‹è¯•"""
    
    @pytest.fixture
    def failing_model_manager(self):
        """å¤±è´¥çš„æ¨¡å‹ç®¡ç†å™¨"""
        manager = Mock()
        manager.generate_response = AsyncMock(side_effect=Exception("Model error"))
        manager.generate_stream_response = AsyncMock(side_effect=Exception("Stream error"))
        manager.is_model_loaded.return_value = False
        return manager
    
    @pytest.fixture
    def timeout_model_manager(self):
        """è¶…æ—¶çš„æ¨¡å‹ç®¡ç†å™¨"""
        manager = Mock()
        
        async def slow_generate(*args, **kwargs):
            await asyncio.sleep(10)  # æ¨¡æ‹Ÿè¶…æ—¶
            return {"content": "response"}
        
        manager.generate_response = AsyncMock(side_effect=slow_generate)
        manager.is_model_loaded.return_value = True
        return manager
    
    @pytest.mark.asyncio
    async def test_model_not_loaded_error(self, failing_model_manager):
        """æµ‹è¯•æ¨¡å‹æœªåŠ è½½é”™è¯¯"""
        # è¿™é‡Œåº”è¯¥æµ‹è¯•å®é™…çš„è·¯ç”±å‡½æ•°
        # ç”±äºéœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®ï¼Œæš‚æ—¶è·³è¿‡
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")
    
    @pytest.mark.asyncio
    async def test_model_generation_error(self, failing_model_manager):
        """æµ‹è¯•æ¨¡å‹ç”Ÿæˆé”™è¯¯"""
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")
    
    @pytest.mark.asyncio
    async def test_request_timeout_error(self, timeout_model_manager):
        """æµ‹è¯•è¯·æ±‚è¶…æ—¶é”™è¯¯"""
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")
    
    def test_invalid_json_request(self):
        """æµ‹è¯•æ— æ•ˆJSONè¯·æ±‚"""
        # è¿™åº”è¯¥åœ¨FastAPIå±‚é¢è¢«å¤„ç†
        pytest.skip("éœ€è¦å®Œæ•´çš„åº”ç”¨è®¾ç½®")
    
    def test_missing_required_fields(self):
        """æµ‹è¯•ç¼ºå°‘å¿…éœ€å­—æ®µ"""
        with pytest.raises(Exception):  # åº”è¯¥æ˜¯ValidationError
            ChatRequest(
                # ç¼ºå°‘messageså­—æ®µ
                model="test-model"
            )
    
    def test_invalid_message_role(self):
        """æµ‹è¯•æ— æ•ˆçš„æ¶ˆæ¯è§’è‰²"""
        with pytest.raises(Exception):  # åº”è¯¥æ˜¯ValidationError
            ChatRequest(
                messages=[{"role": "invalid_role", "content": "test"}],
                model="test-model"
            )
    
    def test_negative_temperature(self):
        """æµ‹è¯•è´Ÿæ¸©åº¦å€¼"""
        with pytest.raises(Exception):  # åº”è¯¥æ˜¯ValidationError
            ChatRequest(
                messages=[{"role": "user", "content": "test"}],
                model="test-model",
                temperature=-0.1
            )
    
    def test_negative_max_tokens(self):
        """æµ‹è¯•è´Ÿæœ€å¤§ä»¤ç‰Œæ•°"""
        with pytest.raises(Exception):  # åº”è¯¥æ˜¯ValidationError
            ChatRequest(
                messages=[{"role": "user", "content": "test"}],
                model="test-model",
                max_tokens=-1
            )


@pytest.mark.unit
@pytest.mark.api
class TestChatStreamHandling:
    """èŠå¤©æµå¼å¤„ç†æµ‹è¯•"""
    
    @pytest.fixture
    def stream_model_manager(self):
        """æµå¼æ¨¡å‹ç®¡ç†å™¨"""
        manager = Mock()
        
        async def mock_stream_generator():
            chunks = [
                {"delta": {"content": "Hello"}, "finish_reason": None},
                {"delta": {"content": " world"}, "finish_reason": None},
                {"delta": {"content": "!"}, "finish_reason": "stop"}
            ]
            for chunk in chunks:
                yield chunk
        
        manager.generate_stream_response = AsyncMock(return_value=mock_stream_generator())
        manager.is_model_loaded.return_value = True
        return manager
    
    @pytest.mark.asyncio
    async def test_stream_response_chunks(self, stream_model_manager):
        """æµ‹è¯•æµå¼å“åº”å—"""
        # æµ‹è¯•æµå¼ç”Ÿæˆå™¨
        generator = await stream_model_manager.generate_stream_response(
            messages=[{"role": "user", "content": "Hello"}],
            model="test-model"
        )
        
        chunks = []
        async for chunk in generator:
            chunks.append(chunk)
        
        assert len(chunks) == 3
        assert chunks[0]["delta"]["content"] == "Hello"
        assert chunks[1]["delta"]["content"] == " world"
        assert chunks[2]["finish_reason"] == "stop"
    
    @pytest.mark.asyncio
    async def test_stream_error_handling(self):
        """æµ‹è¯•æµå¼é”™è¯¯å¤„ç†"""
        manager = Mock()
        
        async def failing_stream_generator():
            yield {"delta": {"content": "Hello"}, "finish_reason": None}
            raise Exception("Stream error")
        
        manager.generate_stream_response = AsyncMock(return_value=failing_stream_generator())
        
        generator = await manager.generate_stream_response(
            messages=[{"role": "user", "content": "Hello"}],
            model="test-model"
        )
        
        chunks = []
        with pytest.raises(Exception, match="Stream error"):
            async for chunk in generator:
                chunks.append(chunk)
    
    @pytest.mark.asyncio
    async def test_stream_empty_response(self):
        """æµ‹è¯•ç©ºæµå¼å“åº”"""
        manager = Mock()
        
        async def empty_stream_generator():
            return
            yield  # æ°¸è¿œä¸ä¼šæ‰§è¡Œ
        
        manager.generate_stream_response = AsyncMock(return_value=empty_stream_generator())
        
        generator = await manager.generate_stream_response(
            messages=[{"role": "user", "content": "Hello"}],
            model="test-model"
        )
        
        chunks = []
        async for chunk in generator:
            chunks.append(chunk)
        
        assert len(chunks) == 0


@pytest.mark.unit
@pytest.mark.api
class TestChatMemoryManagement:
    """èŠå¤©å†…å­˜ç®¡ç†æµ‹è¯•"""
    
    @pytest.fixture
    def memory_aware_manager(self):
        """å†…å­˜æ„ŸçŸ¥çš„æ¨¡å‹ç®¡ç†å™¨"""
        manager = Mock()
        manager.get_memory_usage.return_value = {
            "total": 8 * 1024 * 1024 * 1024,  # 8GB
            "used": 6 * 1024 * 1024 * 1024,   # 6GB
            "available": 2 * 1024 * 1024 * 1024  # 2GB
        }
        manager.cleanup_memory = AsyncMock()
        return manager
    
    def test_memory_usage_tracking(self, memory_aware_manager):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨è·Ÿè¸ª"""
        memory_info = memory_aware_manager.get_memory_usage()
        
        assert memory_info["total"] == 8 * 1024 * 1024 * 1024
        assert memory_info["used"] == 6 * 1024 * 1024 * 1024
        assert memory_info["available"] == 2 * 1024 * 1024 * 1024
    
    @pytest.mark.asyncio
    async def test_memory_cleanup_trigger(self, memory_aware_manager):
        """æµ‹è¯•å†…å­˜æ¸…ç†è§¦å‘"""
        # æ¨¡æ‹Ÿå†…å­˜ä¸è¶³æƒ…å†µ
        memory_aware_manager.get_memory_usage.return_value = {
            "total": 8 * 1024 * 1024 * 1024,
            "used": 7.5 * 1024 * 1024 * 1024,  # ä½¿ç”¨ç‡93.75%
            "available": 0.5 * 1024 * 1024 * 1024
        }
        
        # è§¦å‘å†…å­˜æ¸…ç†
        await memory_aware_manager.cleanup_memory()
        
        memory_aware_manager.cleanup_memory.assert_called_once()


@pytest.mark.unit
@pytest.mark.api
class TestChatConcurrency:
    """èŠå¤©å¹¶å‘å¤„ç†æµ‹è¯•"""
    
    @pytest.fixture
    def concurrent_model_manager(self):
        """æ”¯æŒå¹¶å‘çš„æ¨¡å‹ç®¡ç†å™¨"""
        manager = Mock()
        
        async def mock_generate(messages, **kwargs):
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            await asyncio.sleep(0.1)
            return {
                "content": f"Response to: {messages[-1]['content']}",
                "usage": {"total_tokens": 50}
            }
        
        manager.generate_response = AsyncMock(side_effect=mock_generate)
        manager.is_model_loaded.return_value = True
        return manager
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, concurrent_model_manager):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†"""
        # åˆ›å»ºå¤šä¸ªå¹¶å‘è¯·æ±‚
        tasks = []
        for i in range(5):
            task = concurrent_model_manager.generate_response(
                messages=[{"role": "user", "content": f"Request {i}"}],
                model="test-model"
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        start_time = asyncio.get_event_loop().time()
        results = await asyncio.gather(*tasks)
        end_time = asyncio.get_event_loop().time()
        
        # éªŒè¯ç»“æœ
        assert len(results) == 5
        for i, result in enumerate(results):
            assert f"Request {i}" in result["content"]
        
        # å¹¶å‘æ‰§è¡Œåº”è¯¥æ¯”ä¸²è¡Œå¿«
        total_time = end_time - start_time
        assert total_time < 0.3  # åº”è¯¥è¿œå°‘äº0.5ç§’ï¼ˆ5 * 0.1ï¼‰
    
    @pytest.mark.asyncio
    async def test_request_queuing(self, concurrent_model_manager):
        """æµ‹è¯•è¯·æ±‚é˜Ÿåˆ—å¤„ç†"""
        # æ¨¡æ‹Ÿé˜Ÿåˆ—é™åˆ¶
        semaphore = asyncio.Semaphore(2)  # æœ€å¤š2ä¸ªå¹¶å‘è¯·æ±‚
        
        async def limited_generate(messages, **kwargs):
            async with semaphore:
                return await concurrent_model_manager.generate_response(messages, **kwargs)
        
        # åˆ›å»ºå¤šä¸ªè¯·æ±‚
        tasks = []
        for i in range(5):
            task = limited_generate(
                messages=[{"role": "user", "content": f"Request {i}"}],
                model="test-model"
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        assert len(results) == 5


@pytest.mark.performance
@pytest.mark.api
class TestChatPerformance:
    """èŠå¤©APIæ€§èƒ½æµ‹è¯•"""
    
    @pytest.fixture
    def performance_model_manager(self):
        """æ€§èƒ½æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨"""
        manager = Mock()
        
        async def fast_generate(messages, **kwargs):
            # å¿«é€Ÿå“åº”
            return {
                "content": "Fast response",
                "usage": {"total_tokens": 10}
            }
        
        manager.generate_response = AsyncMock(side_effect=fast_generate)
        manager.is_model_loaded.return_value = True
        return manager
    
    @pytest.mark.asyncio
    async def test_request_processing_speed(self, performance_model_manager):
        """æµ‹è¯•è¯·æ±‚å¤„ç†é€Ÿåº¦"""
        import time
        
        # æµ‹è¯•å•ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´
        start_time = time.time()
        result = await performance_model_manager.generate_response(
            messages=[{"role": "user", "content": "Hello"}],
            model="test-model"
        )
        end_time = time.time()
        
        processing_time = end_time - start_time
        assert processing_time < 0.1  # åº”è¯¥å¾ˆå¿«
        assert result["content"] == "Fast response"
    
    @pytest.mark.asyncio
    async def test_throughput_performance(self, performance_model_manager):
        """æµ‹è¯•ååé‡æ€§èƒ½"""
        import time
        
        # æµ‹è¯•æ‰¹é‡è¯·æ±‚çš„ååé‡
        num_requests = 100
        start_time = time.time()
        
        tasks = []
        for i in range(num_requests):
            task = performance_model_manager.generate_response(
                messages=[{"role": "user", "content": f"Message {i}"}],
                model="test-model"
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        total_time = end_time - start_time
        throughput = num_requests / total_time
        
        assert len(results) == num_requests
        assert throughput > 50  # æ¯ç§’è‡³å°‘50ä¸ªè¯·æ±‚
    
    def test_memory_usage_during_processing(self):
        """æµ‹è¯•å¤„ç†è¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # åˆ›å»ºå¤§é‡è¯·æ±‚å¯¹è±¡
        requests = []
        for i in range(1000):
            request = ChatRequest(
                messages=[{"role": "user", "content": f"Message {i}"}],
                model="test-model"
            )
            requests.append(request)
        
        peak_memory = process.memory_info().rss
        memory_increase = peak_memory - initial_memory
        
        # å†…å­˜å¢é•¿åº”è¯¥æ˜¯åˆç†çš„
        assert memory_increase < 100 * 1024 * 1024  # å°‘äº100MB
        
        # æ¸…ç†
        del requests
        import gc
        gc.collect()


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
