#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸå§‹è¾“å‡ºç´«èˆ’èŠå¤© - å®Œå…¨ä¸åšä»»ä½•æ¸…ç†å’Œæˆªæ–­
çœ‹çœ‹æ¨¡å‹çš„çœŸå®è¡¨ç°
"""

import torch
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("ğŸŒ¸ åŠ è½½ç´«èˆ’...")
    adapter_path = Path("./output_zishu_default_personality_full/final_model")
    
    with open(adapter_path / "adapter_config.json", 'r') as f:
        adapter_config = json.load(f)
    base_model_path = adapter_config.get("base_model_name_or_path")
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    if not tokenizer.pad_token:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = PeftModel.from_pretrained(base_model, adapter_path)
    print("âœ… å®Œæˆï¼")
    return model, tokenizer

def raw_generate(model, tokenizer, user_input):
    """å®Œå…¨åŸå§‹ç”Ÿæˆ - ä¸åšä»»ä½•å¤„ç†"""
    
    # ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„ChatMLæ ¼å¼
    prompt = f"<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    
    print(f"ğŸ” ä½¿ç”¨æ‰‹åŠ¨ChatMLæ ¼å¼:")
    print(f"   {repr(prompt)}")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=100,  # å¢åŠ é•¿åº¦çœ‹å®Œæ•´è¾“å‡º
            temperature=0.7,    
            top_p=0.8,         
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # å®Œå…¨åŸå§‹è¾“å‡ºï¼Œä¸åšä»»ä½•æ¸…ç†
    response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:], 
        skip_special_tokens=True
    )
    
    print(f"ğŸ“ åŸå§‹è¾“å‡ºé•¿åº¦: {len(response)} å­—ç¬¦")
    print(f"ğŸ“ åŸå§‹è¾“å‡º (repr): {repr(response)}")
    
    # è¿”å›å®Œå…¨æœªå¤„ç†çš„åŸå§‹è¾“å‡º
    return response

def compare_with_without_cleaning(model, tokenizer, user_input):
    """å¯¹æ¯”æœ‰æ— æ¸…ç†çš„æ•ˆæœ"""
    print(f"\nğŸ”¬ åŸå§‹ vs æ¸…ç†å¯¹æ¯”: {user_input}")
    print("=" * 70)
    
    # ç”ŸæˆåŸå§‹è¾“å‡º
    prompt = f"<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=100,
            temperature=0.7,
            top_p=0.8,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    raw_response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:], 
        skip_special_tokens=True
    )
    
    print("ğŸ”¥ å®Œå…¨åŸå§‹è¾“å‡º:")
    print(f"   é•¿åº¦: {len(raw_response)} å­—ç¬¦")
    print(f"   å†…å®¹: {repr(raw_response)}")
    print(f"   æ˜¾ç¤º: {raw_response}")
    print()
    
    # åˆ†æé—®é¢˜ç‚¹
    issues = []
    if 'ç”¨æˆ·ï¼š' in raw_response or 'å­¦ç”Ÿï¼š' in raw_response:
        issues.append("âŒ æ£€æµ‹åˆ°ç»­å†™é—®é¢˜")
    if 'ä»è¿™æ®µå¯¹è¯' in raw_response or 'å¯ä»¥çœ‹å‡º' in raw_response:
        issues.append("âŒ æ£€æµ‹åˆ°åˆ†æè·³æˆ")
    if '<|im_start|>' in raw_response or '<|im_end|>' in raw_response:
        issues.append("âŒ æ£€æµ‹åˆ°ç‰¹æ®Šæ ‡è®°æ®‹ç•™")
    if len(raw_response) > 80:
        issues.append("âš ï¸ è¾“å‡ºè¾ƒé•¿")
    
    if issues:
        print("ğŸš¨ å‘ç°çš„é—®é¢˜:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    
    print("=" * 70)

def interactive_chat():
    """äº¤äº’å¼èŠå¤© - æ˜¾ç¤ºå®Œå…¨åŸå§‹è¾“å‡º"""
    model, tokenizer = load_model()
    
    print("\nğŸ’¬ åŸå§‹è¾“å‡ºç´«èˆ’èŠå¤©")
    print("ğŸ”¥ å®Œå…¨ä¸åšä»»ä½•æ¸…ç†ï¼Œå±•ç¤ºæ¨¡å‹çœŸå®è¾“å‡º")
    print("è¾“å…¥'exit'é€€å‡ºï¼Œ'analyze [é—®é¢˜]'è¿›è¡Œè¯¦ç»†åˆ†æ")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ˜Š ä½ : ").strip()
            
            if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
                break
            
            if user_input.startswith('analyze '):
                test_input = user_input[8:].strip()
                if test_input:
                    compare_with_without_cleaning(model, tokenizer, test_input)
                continue
            
            if not user_input:
                continue
            
            print("ğŸ¤” ç´«èˆ’æ€è€ƒä¸­...")
            response = raw_generate(model, tokenizer, user_input)
            print(f"ğŸŒ¸ ç´«èˆ’ (åŸå§‹): {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

def batch_raw_test():
    """æ‰¹é‡æµ‹è¯•åŸå§‹è¾“å‡º"""
    model, tokenizer = load_model()
    
    test_cases = [
        "ä½ å¥½",
        "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ",
        "ä½ æ˜¯è°ï¼Ÿ", 
        "ä½ æ˜¯ç´«èˆ’å—ï¼Ÿ",
        "èƒ½å¸®æˆ‘è§£å†³ä¸€ä¸ªæ•°å­¦é—®é¢˜å—ï¼Ÿ",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    ]
    
    print("\nğŸ”¥ åŸå§‹è¾“å‡ºæ‰¹é‡æµ‹è¯•")
    print("ğŸ¯ å®Œå…¨ä¸åšä»»ä½•æ¸…ç†ï¼Œå±•ç¤ºçœŸå®è®­ç»ƒæ•ˆæœ")
    print("=" * 50)
    
    for i, test_input in enumerate(test_cases, 1):
        print(f"\næµ‹è¯• {i}: {test_input}")
        print("-" * 30)
        
        response = raw_generate(model, tokenizer, test_input)
        print(f"ç´«èˆ’ (åŸå§‹): {response}")
        
        # ç®€å•æ ‡æ³¨é—®é¢˜
        if 'ç”¨æˆ·ï¼š' in response or 'å­¦ç”Ÿï¼š' in response:
            print("   âš ï¸ å‘ç°ç»­å†™")
        if 'ä»è¿™æ®µå¯¹è¯' in response:
            print("   âš ï¸ å‘ç°åˆ†æè·³æˆ")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            batch_raw_test()
        elif sys.argv[1] == "analyze":
            model, tokenizer = load_model()
            if len(sys.argv) > 2:
                test_input = " ".join(sys.argv[2:])
            else:
                test_input = input("è¾“å…¥æµ‹è¯•å†…å®¹: ")
            compare_with_without_cleaning(model, tokenizer, test_input)
        else:
            print("ç”¨æ³•: python raw_output_chat.py [test|analyze [é—®é¢˜]]")
    else:
        interactive_chat() 