#!/usr/bin/env python3
"""
Bangumi Archive æ•°æ®åˆ†æžå·¥å…·

è¿™ä¸ªè„šæœ¬ç”¨äºŽåˆ†æžå’Œå¤„ç† Bangumi.tv çš„æ•°æ®è½¬å‚¨æ–‡ä»¶
åŒ…å«ï¼šç•ªå‰§ã€è§’è‰²ã€äººç‰©ã€å‰§é›†ç­‰ä¿¡æ¯çš„ç»Ÿè®¡å’Œæå–
"""

import json
import pandas as pd
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

class BangumiAnalyzer:
    def __init__(self, data_dir="./"):
        self.data_dir = data_dir
        self.files = {
            'subjects': 'subject.jsonlines',           # ç•ªå‰§/ä½œå“æ•°æ® (695MB)
            'characters': 'character.jsonlines',       # è§’è‰²æ•°æ® (127MB)  
            'persons': 'person.jsonlines',             # äººç‰©æ•°æ® (54MB)
            'episodes': 'episode.jsonlines',           # å‰§é›†æ•°æ® (279MB)
            'subject_relations': 'subject-relations.jsonlines',    # ä½œå“å…³ç³» (59MB)
            'subject_persons': 'subject-persons.jsonlines',        # ä½œå“-äººç‰©å…³ç³» (82MB)
            'subject_characters': 'subject-characters.jsonlines',  # ä½œå“-è§’è‰²å…³ç³» (21MB)
            'person_characters': 'person-characters.jsonlines'     # äººç‰©-è§’è‰²å…³ç³» (15MB)
        }
    
    def load_jsonlines(self, filename, limit=None):
        """åŠ è½½ JSONLINES æ–‡ä»¶"""
        filepath = os.path.join(self.data_dir, filename)
        data = []
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if limit and i >= limit:
                        break
                    try:
                        data.append(json.loads(line.strip()))
                    except json.JSONDecodeError as e:
                        print(f"JSON è§£æžé”™è¯¯åœ¨è¡Œ {i+1}: {e}")
                        continue
        except FileNotFoundError:
            print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
            return []
        
        return data
    
    def get_file_stats(self):
        """èŽ·å–æ‰€æœ‰æ–‡ä»¶çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for name, filename in self.files.items():
            filepath = os.path.join(self.data_dir, filename)
            if os.path.exists(filepath):
                # è®¡ç®—è¡Œæ•°
                line_count = 0
                file_size = os.path.getsize(filepath)
                
                with open(filepath, 'r', encoding='utf-8') as f:
                    for line in f:
                        line_count += 1
                
                stats[name] = {
                    'filename': filename,
                    'lines': line_count,
                    'size_mb': file_size / 1024 / 1024
                }
            else:
                stats[name] = {'filename': filename, 'lines': 0, 'size_mb': 0, 'status': 'missing'}
        
        return stats
    
    def analyze_subjects(self, limit=10000):
        """åˆ†æžç•ªå‰§/ä½œå“æ•°æ®"""
        print("åˆ†æžä½œå“æ•°æ®...")
        subjects = self.load_jsonlines(self.files['subjects'], limit)
        
        if not subjects:
            return None
        
        # åŸºæœ¬ç»Ÿè®¡
        total_subjects = len(subjects)
        types = Counter([s.get('type') for s in subjects])
        
        # ç±»åž‹æ˜ å°„
        type_mapping = {
            1: 'ä¹¦ç±', 2: 'åŠ¨ç”»', 3: 'éŸ³ä¹', 4: 'æ¸¸æˆ', 6: 'çœŸäºº'
        }
        
        # è¯„åˆ†åˆ†æž
        scores = [s.get('score', 0) for s in subjects if s.get('score', 0) > 0]
        
        # å¹´ä»½åˆ†æž
        years = []
        for s in subjects:
            date = s.get('date', '')
            if date and len(date) >= 4:
                try:
                    year = int(date[:4])
                    if 1950 <= year <= 2025:  # åˆç†å¹´ä»½èŒƒå›´
                        years.append(year)
                except ValueError:
                    continue
        
        # æ ‡ç­¾åˆ†æž
        all_tags = []
        for s in subjects:
            tags = s.get('tags', [])
            for tag in tags:
                all_tags.append(tag.get('name', ''))
        
        tag_counter = Counter(all_tags)
        
        analysis = {
            'total_subjects': total_subjects,
            'types': {type_mapping.get(k, f'ç±»åž‹{k}'): v for k, v in types.items()},
            'score_stats': {
                'count': len(scores),
                'mean': sum(scores) / len(scores) if scores else 0,
                'min': min(scores) if scores else 0,
                'max': max(scores) if scores else 0
            },
            'year_range': {
                'earliest': min(years) if years else None,
                'latest': max(years) if years else None,
                'count': len(years)
            },
            'top_tags': tag_counter.most_common(20)
        }
        
        return analysis
    
    def analyze_characters(self, limit=10000):
        """åˆ†æžè§’è‰²æ•°æ®"""
        print("åˆ†æžè§’è‰²æ•°æ®...")
        characters = self.load_jsonlines(self.files['characters'], limit)
        
        if not characters:
            return None
        
        # åŸºæœ¬ç»Ÿè®¡
        total_characters = len(characters)
        
        # è§’è‰²ç±»åž‹ç»Ÿè®¡ (roleå­—æ®µ)
        roles = Counter([c.get('role') for c in characters])
        
        # è§’è‰²æ€§åˆ«ç»Ÿè®¡ï¼ˆä»Ž infobox è§£æžï¼‰
        genders = []
        for c in characters:
            infobox = c.get('infobox', '')
            if 'æ€§åˆ«= ç”·' in infobox:
                genders.append('ç”·')
            elif 'æ€§åˆ«= å¥³' in infobox:
                genders.append('å¥³')
            else:
                genders.append('æœªçŸ¥')
        
        gender_counter = Counter(genders)
        
        # å—æ¬¢è¿Žç¨‹åº¦ï¼ˆæ”¶è—æ•°ï¼‰
        collects = [c.get('collects', 0) for c in characters if c.get('collects', 0) > 0]
        
        analysis = {
            'total_characters': total_characters,
            'roles': dict(roles),
            'genders': dict(gender_counter),
            'popularity_stats': {
                'count': len(collects),
                'mean': sum(collects) / len(collects) if collects else 0,
                'max': max(collects) if collects else 0
            }
        }
        
        return analysis
    
    def get_anime_for_training(self, min_score=7.0, min_episodes=5, limit=1000):
        """èŽ·å–é€‚åˆè®­ç»ƒçš„åŠ¨ç”»æ•°æ®"""
        print(f"ç­›é€‰é€‚åˆè®­ç»ƒçš„åŠ¨ç”»æ•°æ®ï¼ˆæœ€ä½Žè¯„åˆ†: {min_score}, æœ€å°‘é›†æ•°: {min_episodes}ï¼‰...")
        
        subjects = self.load_jsonlines(self.files['subjects'])
        episodes = self.load_jsonlines(self.files['episodes'])
        
        # æŒ‰ä½œå“IDåˆ†ç»„å‰§é›†
        episode_count = defaultdict(int)
        for ep in episodes:
            episode_count[ep.get('subject_id', 0)] += 1
        
        # ç­›é€‰åŠ¨ç”»
        anime_data = []
        for subject in subjects:
            if (subject.get('type') == 2 and  # åŠ¨ç”»ç±»åž‹
                subject.get('score', 0) >= min_score and
                episode_count[subject.get('id', 0)] >= min_episodes):
                
                anime_info = {
                    'id': subject.get('id'),
                    'name': subject.get('name', ''),
                    'name_cn': subject.get('name_cn', ''),
                    'score': subject.get('score', 0),
                    'date': subject.get('date', ''),
                    'summary': subject.get('summary', ''),
                    'tags': [tag.get('name', '') for tag in subject.get('tags', [])],
                    'episode_count': episode_count[subject.get('id', 0)]
                }
                anime_data.append(anime_info)
                
                if len(anime_data) >= limit:
                    break
        
        return anime_data
    
    def print_summary(self):
        """æ‰“å°æ•°æ®æ€»è§ˆ"""
        print("=" * 60)
        print("Bangumi Archive æ•°æ®æ€»è§ˆ")
        print("=" * 60)
        
        # æ–‡ä»¶ç»Ÿè®¡
        stats = self.get_file_stats()
        print("\nðŸ“ æ–‡ä»¶ç»Ÿè®¡:")
        for name, info in stats.items():
            if 'status' not in info:
                print(f"  {name:20s} | {info['lines']:>8,} æ¡è®°å½• | {info['size_mb']:>6.1f} MB")
            else:
                print(f"  {name:20s} | æ–‡ä»¶ç¼ºå¤±")
        
        # ä½œå“åˆ†æž
        subject_analysis = self.analyze_subjects(limit=50000)
        if subject_analysis:
            print(f"\nðŸ“º ä½œå“ç»Ÿè®¡ (æ ·æœ¬: {subject_analysis['total_subjects']:,} æ¡):")
            for type_name, count in subject_analysis['types'].items():
                print(f"  {type_name:10s}: {count:>6,} ä¸ª")
            
            print(f"\nâ­ è¯„åˆ†ç»Ÿè®¡:")
            score_stats = subject_analysis['score_stats']
            print(f"  æœ‰è¯„åˆ†ä½œå“: {score_stats['count']:,} ä¸ª")
            print(f"  å¹³å‡åˆ†: {score_stats['mean']:.2f}")
            print(f"  åˆ†æ•°èŒƒå›´: {score_stats['min']:.1f} - {score_stats['max']:.1f}")
            
            print(f"\nðŸ—“ï¸  å¹´ä»½èŒƒå›´:")
            if subject_analysis['year_range']['earliest']:
                print(f"  {subject_analysis['year_range']['earliest']} - {subject_analysis['year_range']['latest']}")
                print(f"  æœ‰æ—¥æœŸä¿¡æ¯: {subject_analysis['year_range']['count']:,} ä¸ª")
            
            print(f"\nðŸ·ï¸  çƒ­é—¨æ ‡ç­¾ (Top 10):")
            for tag, count in subject_analysis['top_tags'][:10]:
                print(f"  {tag:15s}: {count:>5,} æ¬¡")
        
        # è§’è‰²åˆ†æž
        character_analysis = self.analyze_characters(limit=30000)
        if character_analysis:
            print(f"\nðŸ‘¤ è§’è‰²ç»Ÿè®¡ (æ ·æœ¬: {character_analysis['total_characters']:,} æ¡):")
            print(f"  æ€§åˆ«åˆ†å¸ƒ:")
            for gender, count in character_analysis['genders'].items():
                print(f"    {gender:6s}: {count:>6,} ä¸ª")
            
            pop_stats = character_analysis['popularity_stats']
            if pop_stats['count'] > 0:
                print(f"  æ”¶è—ç»Ÿè®¡:")
                print(f"    å¹³å‡æ”¶è—: {pop_stats['mean']:.0f}")
                print(f"    æœ€é«˜æ”¶è—: {pop_stats['max']:,}")

def main():
    # åˆ›å»ºåˆ†æžå™¨å®žä¾‹
    analyzer = BangumiAnalyzer()
    
    # æ‰“å°æ•°æ®æ€»è§ˆ
    analyzer.print_summary()
    
    # èŽ·å–é€‚åˆè®­ç»ƒçš„åŠ¨ç”»æ•°æ®ç¤ºä¾‹
    print("\n" + "=" * 60)
    print("èŽ·å–é«˜è´¨é‡åŠ¨ç”»æ•°æ®ç¤ºä¾‹")
    print("=" * 60)
    
    anime_sample = analyzer.get_anime_for_training(min_score=8.0, min_episodes=10, limit=10)
    
    print(f"\næ‰¾åˆ° {len(anime_sample)} éƒ¨é«˜è´¨é‡åŠ¨ç”»:")
    for anime in anime_sample:
        print(f"\nðŸ“º {anime['name_cn'] or anime['name']}")
        print(f"   è¯„åˆ†: {anime['score']:.1f} | é›†æ•°: {anime['episode_count']} | å¹´ä»½: {anime['date'][:4] if anime['date'] else 'æœªçŸ¥'}")
        print(f"   æ ‡ç­¾: {', '.join(anime['tags'][:5])}")
        if anime['summary']:
            summary = anime['summary'][:100] + "..." if len(anime['summary']) > 100 else anime['summary']
            print(f"   ç®€ä»‹: {summary}")

if __name__ == "__main__":
    main() 