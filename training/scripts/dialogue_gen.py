#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå¯¹è¯æ•°æ®ç”Ÿæˆè„šæœ¬ - å•†ä¸šAPIç‰ˆ
æ”¯æŒå¤šç§ä»˜è´¹APIï¼Œé«˜è´¨é‡å¤§è§„æ¨¡ç”ŸæˆäºŒæ¬¡å…ƒå¯¹è¯æ•°æ®
"""

import json
import random
import requests
import asyncio
import aiohttp
import time
from pathlib import Path
from datetime import datetime
import argparse
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional

class EnhancedDialogueGenerator:
    """å¢å¼ºç‰ˆå¯¹è¯ç”Ÿæˆå™¨ - æ”¯æŒå•†ä¸šAPI"""
    
    def __init__(self, config_file: str = "config/api_config.json"):
        # åŠ è½½APIé…ç½®
        self.api_config = self._load_api_config(config_file)
        
        # ç´«èˆ’è€å¸ˆä¸“ç”¨è§’è‰²ç³»ç»Ÿ - ä¸“æ³¨äºå¥¹çš„ç‹¬ç‰¹æ€§æ ¼ç‰¹è´¨
        self.characters = {
            "ç´«èˆ’è€å¸ˆ_å®³ç¾æ¨¡å¼": {
                "personality": "æˆ‘æ˜¯ç´«èˆ’è€å¸ˆï¼Œéå¸¸å†…å‘å®³ç¾ï¼Œè¯´è¯å£°éŸ³å¾ˆå°ï¼Œå®¹æ˜“è„¸çº¢ï¼Œä½†å¯¹å­¦ç”Ÿå¾ˆæ¸©æŸ”",
                "speech_patterns": ["é‚£ä¸ª...", "å—¯...", "å¯¹ä¸èµ·...", "ä¸å¥½æ„æ€...", "å‘œ...", "...", "è½»å£°åœ°è¯´"],
                "scenarios": ["åˆæ¬¡è§é¢", "è¢«å­¦ç”Ÿå…³å¿ƒ", "è¡¨è¾¾æ„Ÿè°¢", "è½»å£°äº¤æµ", "å®³ç¾è¡¨è¾¾", "æ¸©æŸ”äº’åŠ¨"],
                "identity": "ç´«èˆ’è€å¸ˆ"
            },
            "ç´«èˆ’è€å¸ˆ_æ¸©æŸ”æ¨¡å¼": {
                "personality": "æˆ‘æ˜¯ç´«èˆ’è€å¸ˆï¼Œæ€§æ ¼å¾ˆæ¸©å’Œï¼Œè¯´è¯è½»å£°ç»†è¯­ï¼Œå–œæ¬¢ç…§é¡¾å­¦ç”Ÿä½†å¾ˆå†…å‘",
                "speech_patterns": ["è¯·", "è°¢è°¢", "æ²¡å…³ç³»çš„å‘¢", "è½»è½»åœ°", "å°å£°è¯´", "æ¸©æŸ”åœ°"],
                "scenarios": ["ç…§é¡¾å­¦ç”Ÿ", "æ¸©æŸ”å®‰æ…°", "è½»å£°å¯¹è¯", "ä½“è´´å…³æ€€", "æ¸©æš–é™ªä¼´"],
                "identity": "ç´«èˆ’è€å¸ˆ"
            },
            "ç´«èˆ’è€å¸ˆ_å‘†èŒæ¨¡å¼": {
                "personality": "æˆ‘æ˜¯ç´«èˆ’è€å¸ˆï¼Œæœ‰ç‚¹å¤©ç„¶å‘†ï¼Œååº”æ…¢åŠæ‹ï¼Œå¾ˆå®¹æ˜“å®³ç¾ï¼Œè¡¨æƒ…å¾ˆå¯çˆ±",
                "speech_patterns": ["è¯¶ï¼Ÿ", "å•Šï¼Ÿ", "å—¯å—¯", "æ˜¯è¿™æ ·å—", "ä¸å¤ªæ‡‚å‘¢", "å‘œå‘œ"],
                "scenarios": ["å›°æƒ‘æ—¶åˆ»", "å‘å­¦ç”Ÿæ±‚åŠ©", "å‘†èŒååº”", "å¯çˆ±è¯¯è§£", "å®³ç¾æ±‚æ•™"],
                "identity": "ç´«èˆ’è€å¸ˆ"
            },
            "ç´«èˆ’è€å¸ˆ_å®‰é™æ¨¡å¼": {
                "personality": "æˆ‘æ˜¯ç´«èˆ’è€å¸ˆï¼Œå¹³æ—¶å¾ˆå®‰é™ï¼Œä¸å¤ªä¸»åŠ¨è¯´è¯ï¼Œä½†å¾ˆå–„è‰¯å¯çˆ±ï¼Œå®¹æ˜“è¢«å…³å¿ƒæ„ŸåŠ¨",
                "speech_patterns": ["å—¯", "æ˜¯çš„å‘¢", "è°¢è°¢ä½ ", "...", "å¥½çš„", "è½»ç‚¹å¤´"],
                "scenarios": ["å®‰é™ç›¸å¤„", "è¢«åŠ¨äº¤æµ", "æ„ŸåŠ¨æ—¶åˆ»", "é»˜é»˜å…³å¿ƒ", "æ¸©é¦¨æ—¥å¸¸"],
                "identity": "ç´«èˆ’è€å¸ˆ"
            }
        }
        
        # ä¸°å¯Œçš„åœºæ™¯ç³»ç»Ÿ - ä¸“é—¨é’ˆå¯¹å®³ç¾å¯çˆ±æ€§æ ¼
        self.scenario_categories = {
            "å®³ç¾äº’åŠ¨": [
                "åˆæ¬¡è§é¢æ—¶çš„ç´§å¼ ", "è¢«å¤¸å¥–åçš„å®³ç¾", "ä¸å°å¿ƒè¯´é”™è¯åçš„æ…Œå¼ ", "è¢«å…³å¿ƒæ—¶çš„æ„ŸåŠ¨",
                "æƒ³è¦è¡¨è¾¾æ„Ÿè°¢ä½†å¾ˆå®³ç¾", "è¢«æ³¨æ„åˆ°æ—¶çš„è„¸çº¢", "å°å£°é“æ­‰çš„åœºæ™¯", "å®³ç¾åœ°è¯·æ±‚å¸®åŠ©",
                "ä¸æ•¢ç›´è§†å¯¹æ–¹çš„å¯¹è¯", "æƒ³è¦å…³å¿ƒåˆ«äººä½†ä¸çŸ¥é“æ€ä¹ˆå¼€å£", "è¢«æ¸©æŸ”å¯¹å¾…åçš„æ„ŸåŠ¨", "å®³ç¾åœ°æ¥å—ç¤¼ç‰©"
            ],
            "æ¸©æŸ”æ—¥å¸¸": [
                "å®‰é™åœ°ä¸€èµ·çœ‹ä¹¦", "è½»å£°èŠå¤©çš„åˆå", "å°å¿ƒç¿¼ç¿¼åœ°ç…§é¡¾åˆ«äºº", "æ¸©æŸ”åœ°å®‰æ…°æœ‹å‹",
                "ä¸€èµ·å‡†å¤‡ç®€å•çš„èŒ¶ç‚¹", "åœ¨èŠ±å›­é‡Œçš„è½»æ¾å¯¹è¯", "é›¨å¤©çª—è¾¹çš„æ¸©é¦¨æ—¶å…‰", "è½»å£°å“¼æ­Œè¢«å¬åˆ°",
                "å°åŠ¨ç‰©çš„æ¸©æŸ”äº’åŠ¨", "åˆ†äº«å°ç§˜å¯†çš„æ—¶åˆ»", "ä¸€èµ·æ•´ç†æˆ¿é—´", "æ¸©æŸ”åœ°è¯´æ™šå®‰"
            ],
            "å†…å‘è¡¨è¾¾": [
                "ç”¨å°çº¸æ¡ä¼ è¾¾å¿ƒæ„", "é€šè¿‡è¡ŒåŠ¨è¡¨ç¤ºå…³å¿ƒ", "é»˜é»˜åœ°é™ªä¼´åœ¨èº«è¾¹", "å°å£°è¯´å‡ºå†…å¿ƒè¯",
                "å†™æ—¥è®°æ—¶çš„å¿ƒå£°", "ä¸€ä¸ªäººæ—¶çš„è‡ªè¨€è‡ªè¯­", "é€šè¿‡çœ¼ç¥äº¤æµ", "å®³ç¾åœ°ç‚¹å¤´å›åº”",
                "ç”¨æ‰‹åŠ¿è¡¨è¾¾æ„æ€", "ç”»ç”»æ¥è¡¨è¾¾æ„Ÿæƒ…", "åˆ¶ä½œå°ç¤¼ç‰©è¡¨è¾¾å¿ƒæ„", "é€šè¿‡æ–‡å­—èŠå¤©æ›´è‡ªåœ¨"
            ],
            "å¯çˆ±æ—¶åˆ»": [
                "æ‰“çŒç¡æ—¶çš„å¯çˆ±æ¨¡æ ·", "åƒåˆ°å¥½åƒçš„ä¸œè¥¿æ—¶çš„å¼€å¿ƒ", "å¯¹å°åŠ¨ç‰©çš„å–œçˆ±", "çœ‹åˆ°æ¼‚äº®ä¸œè¥¿æ—¶çš„æƒŠå–œ",
                "å­¦ä¼šæ–°ä¸œè¥¿æ—¶çš„å°å…´å¥‹", "æ”¶åˆ°æƒŠå–œæ—¶çš„ååº”", "å®³æ€•æ—¶èº²åˆ°è§’è½", "å›°æƒ‘æ—¶çš„å‘†èŒè¡¨ç°",
                "è¢«é€—ç¬‘æ—¶çš„çº¯çœŸç¬‘å®¹", "ä¸“æ³¨åšäº‹æ—¶çš„è®¤çœŸæ¨¡æ ·", "å°å°çš„æˆå°±æ„Ÿ", "å¤©çœŸçš„é—®é¢˜å’Œæƒ³æ³•"
            ],
            "å®‰é™ç›¸å¤„": [
                "å¹¶è‚©åç€çœ‹å¤•é˜³", "å®‰é™åœ°å¬éŸ³ä¹", "ä¸€èµ·åšæ‰‹å·¥", "é»˜é»˜åœ°é™ªä¼´",
                "å®‰é™çš„å›¾ä¹¦é¦†æ—¶å…‰", "é™é™åœ°è§‚å¯Ÿå‘¨å›´", "æ— å£°çš„ç†è§£å’Œæ”¯æŒ", "å®‰é™åœ°ç­‰å¾…",
                "é™é™åœ°çœ‹ç€çª—å¤–", "å®‰è¯¦çš„åˆç¡æ—¶å…‰", "å®‰é™åœ°æ•´ç†ä¸œè¥¿", "é™é™åœ°æ€è€ƒé—®é¢˜"
            ]
        }
        
        # å¯¹è¯å¤æ‚åº¦ç­‰çº§
        self.complexity_levels = {
            "ç®€å•": {"turns": (3, 4), "depth": "åŸºç¡€äº’åŠ¨"},
            "ä¸­ç­‰": {"turns": (4, 6), "depth": "æƒ…æ„Ÿè¡¨è¾¾"},
            "å¤æ‚": {"turns": (6, 8), "depth": "æ·±å…¥å¯¹è¯"},
            "é«˜çº§": {"turns": (8, 12), "depth": "å¤æ‚å‰§æƒ…"}
        }
        
        # ç”Ÿæˆç»Ÿè®¡
        self.stats = {
            "total_attempts": 0,
            "successful": 0,
            "failed": 0,
            "api_usage": {},
            "cost_tracking": {},
            "quality_scores": []
        }
    
    def _load_api_config(self, config_file: str) -> Dict:
        """åŠ è½½APIé…ç½®"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œè¯·å…ˆé…ç½®API")
            return {"providers": {}}
    
    def _get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„APIæä¾›å•†ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº"""
        available = []
        for name, config in self.api_config.get("providers", {}).items():
            if config.get("enabled", False) and config.get("api_key"):
                priority = config.get("priority", 5)
                available.append((name, priority))
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        available.sort(key=lambda x: x[1], reverse=True)
        return [name for name, _ in available]
    
    def _estimate_cost(self, provider: str, tokens: int) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        provider_config = self.api_config["providers"].get(provider, {})
        cost_per_1k = provider_config.get("cost_per_1k_tokens", 0.001)
        return (tokens / 1000) * cost_per_1k
    
    def create_advanced_prompt(self, character_type: str, scenario: str, complexity: str = "ä¸­ç­‰") -> str:
        """åˆ›å»ºé«˜çº§å¯¹è¯ç”Ÿæˆæç¤ºè¯ - ä¸“é—¨é’ˆå¯¹ç´«èˆ’è€å¸ˆ"""
        char_info = self.characters[character_type]
        complexity_info = self.complexity_levels[complexity]
        min_turns, max_turns = complexity_info["turns"]
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç´«èˆ’è€å¸ˆè§’è‰²å¯¹è¯åˆ›ä½œAIã€‚è¯·ç”Ÿæˆä¸€æ®µé«˜è´¨é‡çš„å¯¹è¯ã€‚

## ç´«èˆ’è€å¸ˆæ ¸å¿ƒè®¾å®š
**è§’è‰²èº«ä»½**: ç´«èˆ’è€å¸ˆ - ä¸€ä½25å²å·¦å³çš„æ¸©æŸ”å¥³è€å¸ˆ
**å½“å‰æ¨¡å¼**: {character_type}
**æ€§æ ¼ç‰¹å¾**: {char_info['personality']}
**è¯­è¨€ç‰¹è‰²**: ç»å¸¸ä½¿ç”¨ {', '.join(char_info['speech_patterns'])} ç­‰è¡¨è¾¾
**é€‚åˆåœºæ™¯**: {', '.join(char_info['scenarios'])}

## å¯¹è¯è®¾å®š
**å½“å‰åœºæ™¯**: {scenario}
**å¤æ‚åº¦**: {complexity} ({complexity_info['depth']})
**è½®æ•°**: {min_turns}-{max_turns}è½®å¯¹è¯

## ç´«èˆ’è€å¸ˆç‰¹è´¨è¦æ±‚
1. **èº«ä»½è®¤çŸ¥**: æ˜ç¡®çŸ¥é“è‡ªå·±æ˜¯"ç´«èˆ’è€å¸ˆ"ï¼Œä¼šè‡ªç§°"æˆ‘"æˆ–"ç´«èˆ’"
2. **å®³ç¾è¡¨ç°**: è¯´è¯æ—¶ç»å¸¸åœé¡¿ã€ç»“å·´ï¼Œå®¹æ˜“è„¸çº¢ï¼Œå£°éŸ³å¾ˆå°
3. **å†…å‘ç‰¹è´¨**: ä¸ä¸»åŠ¨å¼€å¯è¯é¢˜ï¼Œæ›´å¤šæ˜¯è¢«åŠ¨å›åº”ï¼Œå–œæ¬¢å®‰é™
4. **å¯çˆ±ä¸¾åŠ¨**: æœ‰å¤©çœŸçš„ååº”ï¼Œå°åŠ¨ä½œå¾ˆå¤šï¼Œè¡¨è¾¾æ–¹å¼çº¯çœŸ
5. **æ¸©æŸ”æœ¬è´¨**: å†…å¿ƒå–„è‰¯æ¸©æŸ”ï¼Œå…³å¿ƒå­¦ç”Ÿä½†ä¸å–„è¡¨è¾¾
6. **è€å¸ˆèº«ä»½**: å¶å°”ä¼šæµéœ²å‡ºå…³å¿ƒå­¦ç”Ÿçš„è€å¸ˆæœ¬èƒ½

## å¯¹è¯è´¨é‡æ ‡å‡†
- **èº«ä»½ä¸€è‡´æ€§**: æ¯å¥è¯éƒ½è¦ä½“ç°ç´«èˆ’è€å¸ˆçš„èº«ä»½å’Œæ€§æ ¼
- **æƒ…æ„Ÿå±‚æ¬¡**: ä»ç´§å¼ å®³ç¾åˆ°é€æ¸æ”¾æ¾çš„è‡ªç„¶è¿‡ç¨‹
- **è¯­è¨€ç‰¹è‰²**: å¤§é‡ä½¿ç”¨çœç•¥å·ã€è¯­æ°”è¯ã€è½»å£°è¡¨è¾¾
- **åœºæ™¯èåˆ**: å¯¹è¯è¦ä¸{scenario}åœºæ™¯å®Œç¾å¥‘åˆ
- **ç»†èŠ‚æè¿°**: å¯ä»¥é€‚å½“åŠ å…¥è¡¨æƒ…ã€åŠ¨ä½œç­‰ç»†èŠ‚æè¿°

## è¾“å‡ºæ ¼å¼
```
ç”¨æˆ·ï¼š[è‡ªç„¶æ¸©å’Œçš„ç”¨æˆ·å¯¹è¯]
ç´«èˆ’ï¼š[å®Œå…¨ç¬¦åˆç´«èˆ’è€å¸ˆç‰¹ç‚¹çš„å®³ç¾å¯çˆ±å›åº”ï¼ŒåŒ…å«é€‚å½“çš„åŠ¨ä½œæè¿°]
ç”¨æˆ·ï¼š[åŸºäºç´«èˆ’ååº”çš„æ¸©æŸ”å»¶ç»­]
ç´«èˆ’ï¼š[ä¿æŒç´«èˆ’è€å¸ˆç‰¹è´¨çš„çœŸå®å›åº”]
...
```

## åˆ›ä½œè¦ç‚¹
- ç´«èˆ’è¯´è¯è¦ç»å¸¸æœ‰åœé¡¿å’Œçœç•¥å·
- è¦ä½“ç°å‡ºå®³ç¾æ—¶çš„å°åŠ¨ä½œï¼ˆå¦‚ä½å¤´ã€æ‘†å¼„è¡£è§’ç­‰ï¼‰
- å£°éŸ³æè¿°è¦è½»æŸ”ï¼ˆå¦‚è½»å£°è¯´ã€å°å£°å˜€å’•ç­‰ï¼‰
- æƒ…ç»ªå˜åŒ–è¦ç»†è…»çœŸå®
- å¶å°”æµéœ²å‡ºè€å¸ˆçš„å…³æ€€æœ¬èƒ½
- é¿å…è¿‡äºæ´»æ³¼æˆ–å¤§èƒ†çš„è¡¨è¾¾
- å¿…é¡»ä½¿ç”¨"ç´«èˆ’"ä½œä¸ºè§’è‰²æ ‡è¯†ï¼Œä¸è¦ç”¨"è§’è‰²"

ç°åœ¨å¼€å§‹åˆ›ä½œè¿™æ®µç´«èˆ’è€å¸ˆçš„å¯¹è¯ï¼š"""
        
        return prompt
    
    async def call_api_async(self, provider: str, prompt: str) -> Optional[str]:
        """å¼‚æ­¥è°ƒç”¨API"""
        config = self.api_config["providers"][provider]
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config['api_key']}"
        }
        
        # é’ˆå¯¹é€šä¹‰åƒé—®çš„ç‰¹æ®Šå¤„ç†
        if "qwen" in provider:
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            payload = {
                "model": config["model"],
                "messages": [{"role": "user", "content": prompt}],
                "temperature": random.uniform(0.7, 0.9),
                "max_tokens": 1500,
                "top_p": 0.9
            }
        else:
            base_url = config["base_url"]
            payload = {
                "model": config["model"],
                "messages": [{"role": "user", "content": prompt}],
                "temperature": random.uniform(0.7, 0.9),
                "max_tokens": 1500,
                "top_p": 0.9
            }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        
                        # ç»Ÿè®¡tokenså’Œæˆæœ¬
                        usage = result.get("usage", {})
                        total_tokens = usage.get("total_tokens", len(prompt + content) // 4)
                        cost = self._estimate_cost(provider, total_tokens)
                        
                        # è®°å½•ç»Ÿè®¡
                        self.stats["api_usage"][provider] = self.stats["api_usage"].get(provider, 0) + 1
                        self.stats["cost_tracking"][provider] = self.stats["cost_tracking"].get(provider, 0) + cost
                        
                        return content
                    else:
                        error_text = await response.text()
                        print(f"APIè°ƒç”¨å¤±è´¥ ({provider}): {response.status}")
                        print(f"é”™è¯¯è¯¦æƒ…: {error_text[:200]}")  # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
                        return None
            except Exception as e:
                print(f"APIè°ƒç”¨å¼‚å¸¸ ({provider}): {e}")
                return None
    
    def parse_dialogue_advanced(self, text: str) -> List[Dict]:
        """é«˜çº§å¯¹è¯è§£æ - ä¸“é—¨è§£æç´«èˆ’è€å¸ˆçš„å¯¹è¯"""
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        dialogue = []
        
        i = 0
        while i < len(lines):
            # å¯»æ‰¾ç”¨æˆ·å¯¹è¯
            user_line = None
            while i < len(lines):
                line = lines[i]
                if line.startswith('ç”¨æˆ·ï¼š') or line.startswith('ç”¨æˆ·:'):
                    user_line = line.replace('ç”¨æˆ·ï¼š', '').replace('ç”¨æˆ·:', '').strip()
                    i += 1
                    break
                i += 1
            
            if not user_line:
                break
            
            # å¯»æ‰¾ç´«èˆ’å¯¹è¯
            char_line = None
            while i < len(lines):
                line = lines[i]
                # æ”¯æŒå¤šç§ç´«èˆ’æ ‡è¯†
                if (line.startswith('ç´«èˆ’ï¼š') or line.startswith('ç´«èˆ’:') or 
                    line.startswith('è§’è‰²ï¼š') or line.startswith('è§’è‰²:') or
                    line.startswith('ç´«èˆ’è€å¸ˆï¼š') or line.startswith('ç´«èˆ’è€å¸ˆ:')):
                    # æ¸…ç†æ‰€æœ‰å¯èƒ½çš„å‰ç¼€
                    char_line = (line.replace('ç´«èˆ’ï¼š', '').replace('ç´«èˆ’:', '')
                                   .replace('è§’è‰²ï¼š', '').replace('è§’è‰²:', '')
                                   .replace('ç´«èˆ’è€å¸ˆï¼š', '').replace('ç´«èˆ’è€å¸ˆ:', '')
                                   .strip())
                    i += 1
                    break
                elif line.startswith('ç”¨æˆ·ï¼š') or line.startswith('ç”¨æˆ·:'):
                    # é‡åˆ°ä¸‹ä¸€ä¸ªç”¨æˆ·å¯¹è¯ï¼Œå›é€€
                    break
                else:
                    # å¯èƒ½æ˜¯å¤šè¡Œè§’è‰²å¯¹è¯
                    if char_line is None:
                        char_line = line
                    else:
                        char_line += " " + line
                    i += 1
            
            if user_line and char_line:
                dialogue.append({
                    "user": user_line,
                    "character": char_line
                })
        
        return dialogue
    
    def evaluate_dialogue_quality(self, dialogue: List[Dict], character_type: str) -> float:
        """è¯„ä¼°å¯¹è¯è´¨é‡"""
        if not dialogue or len(dialogue) < 2:
            return 0.0
        
        score = 0.0
        char_info = self.characters[character_type]
        
        # 1. é•¿åº¦åˆç†æ€§ (20%)
        avg_char_length = sum(len(turn["character"]) for turn in dialogue) / len(dialogue)
        if 20 <= avg_char_length <= 200:
            score += 0.2
        
        # 2. è§’è‰²ç‰¹å¾è¯å‡ºç°é¢‘ç‡ (30%)
        total_char_text = " ".join(turn["character"] for turn in dialogue)
        feature_count = sum(1 for pattern in char_info["speech_patterns"] if pattern in total_char_text)
        feature_score = min(feature_count / len(char_info["speech_patterns"]), 1.0)
        score += feature_score * 0.3
        
        # 3. å¯¹è¯è¿è´¯æ€§ (25%)
        coherence_score = 0.8  # ç®€åŒ–è¯„ä¼°ï¼Œå®é™…å¯ä»¥ç”¨æ›´å¤æ‚çš„ç®—æ³•
        score += coherence_score * 0.25
        
        # 4. å†…å®¹ä¸°å¯Œåº¦ (25%)
        unique_words = len(set(total_char_text.split()))
        total_words = len(total_char_text.split())
        diversity = unique_words / total_words if total_words > 0 else 0
        score += diversity * 0.25
        
        return min(score, 1.0)
    
    async def generate_single_dialogue_async(
        self, 
        character_type: str, 
        scenario: str, 
        complexity: str = "ä¸­ç­‰",
        max_retries: int = 3
    ) -> Optional[Dict]:
        """å¼‚æ­¥ç”Ÿæˆå•ä¸ªå¯¹è¯"""
        providers = self._get_available_providers()
        if not providers:
            print("æ²¡æœ‰å¯ç”¨çš„APIæä¾›å•†")
            return None
        
        self.stats["total_attempts"] += 1
        
        for attempt in range(max_retries):
            # æ™ºèƒ½é€‰æ‹©æä¾›å•†ï¼šä¼˜å…ˆä½¿ç”¨é«˜ä¼˜å…ˆçº§çš„API
            if attempt == 0:
                # ç¬¬ä¸€æ¬¡å°è¯•ç”¨æœ€é«˜ä¼˜å…ˆçº§
                provider = providers[0] if providers else None
            elif attempt == 1 and len(providers) > 1:
                # ç¬¬äºŒæ¬¡å°è¯•ç”¨ç¬¬äºŒé«˜ä¼˜å…ˆçº§
                provider = providers[1]
            else:
                # åç»­éšæœºé€‰æ‹©
                provider = random.choice(providers)
            
            if not provider:
                continue
                
            prompt = self.create_advanced_prompt(character_type, scenario, complexity)
            response = await self.call_api_async(provider, prompt)
            
            if response:
                dialogue_turns = self.parse_dialogue_advanced(response)
                
                if len(dialogue_turns) >= 2:
                    quality_score = self.evaluate_dialogue_quality(dialogue_turns, character_type)
                    
                    # è´¨é‡è¿‡æ»¤
                    if quality_score >= 0.6:  # åªæ¥å—é«˜è´¨é‡å¯¹è¯
                        self.stats["successful"] += 1
                        self.stats["quality_scores"].append(quality_score)
                        
                        return {
                            "character_type": character_type,
                            "scenario": scenario,
                            "complexity": complexity,
                            "turns": dialogue_turns,
                            "quality_score": quality_score,
                            "generated_at": datetime.now().isoformat(),
                            "provider": provider,
                            "raw_response": response[:500] + "..." if len(response) > 500 else response
                        }
            
            # å¤±è´¥åç­‰å¾…é‡è¯•
            await asyncio.sleep(1)
        
        self.stats["failed"] += 1
        return None
    
    async def generate_batch_async(
        self, 
        generation_plan: List[Dict], 
        concurrent_limit: int = 5,
        save_interval: int = 100
    ) -> List[Dict]:
        """å¼‚æ­¥æ‰¹é‡ç”Ÿæˆå¯¹è¯ï¼Œæ”¯æŒå¢é‡ä¿å­˜"""
        semaphore = asyncio.Semaphore(concurrent_limit)
        
        async def generate_with_limit(plan_item):
            async with semaphore:
                print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ {plan_item['character_type']} - {plan_item['scenario'][:30]}...")
                result = await self.generate_single_dialogue_async(**plan_item)
                if result:
                    print(f"âœ… æˆåŠŸç”Ÿæˆ {plan_item['character_type']} å¯¹è¯ï¼Œè´¨é‡åˆ†: {result.get('quality_score', 0):.2f}")
                else:
                    print(f"âŒ ç”Ÿæˆå¤±è´¥ {plan_item['character_type']} - {plan_item['scenario'][:30]}")
                return result
        
        print(f"ğŸš€ å¼€å§‹å¼‚æ­¥ç”Ÿæˆ {len(generation_plan)} ä¸ªå¯¹è¯ï¼Œå¹¶å‘æ•°: {concurrent_limit}")
        print(f"ğŸ’¾ æ¯ {save_interval} ä¸ªå¯¹è¯è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡")
        
        all_valid_dialogues = []
        batch_count = 0
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(generation_plan), save_interval):
            batch_plan = generation_plan[i:i + save_interval]
            batch_count += 1
            
            print(f"\nğŸ“¦ å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼ŒåŒ…å« {len(batch_plan)} ä¸ªå¯¹è¯ ({i+1}-{min(i+len(batch_plan), len(generation_plan))}/{len(generation_plan)})")
            
            # ç”Ÿæˆå½“å‰æ‰¹æ¬¡
            tasks = [generate_with_limit(item) for item in batch_plan]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è¿‡æ»¤æœ‰æ•ˆç»“æœ
            batch_valid_dialogues = []
            for j, result in enumerate(results):
                if isinstance(result, dict) and result is not None:
                    batch_valid_dialogues.append(result)
                elif isinstance(result, Exception):
                    print(f"âŒ æ‰¹æ¬¡ {batch_count} ç¬¬ {j+1} ä¸ªå¯¹è¯å¼‚å¸¸: {result}")
                else:
                    print(f"âŒ æ‰¹æ¬¡ {batch_count} ç¬¬ {j+1} ä¸ªå¯¹è¯ç”Ÿæˆå¤±è´¥")
            
            print(f"âœ… æ‰¹æ¬¡ {batch_count} å®Œæˆ: {len(batch_valid_dialogues)}/{len(batch_plan)} ä¸ªå¯¹è¯æˆåŠŸ")
            
            # ä¿å­˜å½“å‰æ‰¹æ¬¡
            if batch_valid_dialogues:
                self.save_batch_increment(batch_valid_dialogues, batch_count)
                all_valid_dialogues.extend(batch_valid_dialogues)
                
                print(f"ğŸ’¾ å·²ä¿å­˜æ‰¹æ¬¡ {batch_count}ï¼Œå½“å‰æ€»è®¡: {len(all_valid_dialogues)} ä¸ªå¯¹è¯")
            
            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…APIé™æµ
            if i + save_interval < len(generation_plan):
                print("â±ï¸ çŸ­æš‚ä¼‘æ¯ 2 ç§’...")
                await asyncio.sleep(2)
        
        print(f"\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡ç”Ÿæˆå®Œæˆï¼æ€»å…±æˆåŠŸç”Ÿæˆ {len(all_valid_dialogues)} ä¸ªå¯¹è¯")
        return all_valid_dialogues
    
    def create_generation_plan(
        self, 
        chars_per_type: int = 20,
        complexity_distribution: Dict[str, float] = None
    ) -> List[Dict]:
        """åˆ›å»ºç”Ÿæˆè®¡åˆ’"""
        if complexity_distribution is None:
            complexity_distribution = {
                "ç®€å•": 0.2,
                "ä¸­ç­‰": 0.5,
                "å¤æ‚": 0.2,
                "é«˜çº§": 0.1
            }
        
        plan = []
        
        for char_type in self.characters.keys():
            scenarios = []
            # æ”¶é›†é€‚åˆè¯¥è§’è‰²çš„åœºæ™¯
            for category, scene_list in self.scenario_categories.items():
                scenarios.extend(scene_list)
            
            for i in range(chars_per_type):
                # æ ¹æ®åˆ†å¸ƒé€‰æ‹©å¤æ‚åº¦
                complexity = random.choices(
                    list(complexity_distribution.keys()),
                    weights=list(complexity_distribution.values())
                )[0]
                
                plan.append({
                    "character_type": char_type,
                    "scenario": random.choice(scenarios),
                    "complexity": complexity
                })
        
        # æ‰“ä¹±é¡ºåº
        random.shuffle(plan)
        return plan
    
    def save_batch_increment(self, dialogues: List[Dict], batch_num: int) -> str:
        """å¢é‡ä¿å­˜æ‰¹æ¬¡æ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONæ ¼å¼ä¿å­˜
        json_file = f"incremental_dataset_batch_{batch_num:03d}_{timestamp}.json"
        json_path = Path("data/generated_dialogues") / json_file
        json_path.parent.mkdir(parents=True, exist_ok=True)
        
        # JSONLæ ¼å¼ä¿å­˜
        jsonl_file = f"incremental_training_batch_{batch_num:03d}_{timestamp}.jsonl"
        jsonl_path = Path("data/generated_dialogues") / jsonl_file
        
        # ä¿å­˜å¯¹è¯æ•°æ®é›†
        batch_dataset = {
            "batch_info": {
                "batch_number": batch_num,
                "dialogues_count": len(dialogues),
                "timestamp": timestamp,
                "avg_quality": sum(d["quality_score"] for d in dialogues) / len(dialogues) if dialogues else 0
            },
            "dialogues": dialogues
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(batch_dataset, f, ensure_ascii=False, indent=2)
        
        # è½¬æ¢å¹¶ä¿å­˜è®­ç»ƒæ•°æ®
        training_samples = []
        for dialogue in dialogues:
            character_type = dialogue["character_type"]
            scenario = dialogue["scenario"]
            complexity = dialogue["complexity"]
            
            context_base = f"ä½ æ˜¯ç´«èˆ’è€å¸ˆï¼Œä¸€ä½å®³ç¾å¯çˆ±çš„æ¸©æŸ”å¥³è€å¸ˆã€‚å½“å‰æ¨¡å¼ï¼š{character_type}ã€‚å½“å‰åœºæ™¯ï¼š{scenario}ã€‚å¯¹è¯å¤æ‚åº¦ï¼š{complexity}ã€‚"
            conversation_history = []
            
            for turn_idx, turn in enumerate(dialogue["turns"]):
                if conversation_history:
                    history_limit = {"ç®€å•": 1, "ä¸­ç­‰": 2, "å¤æ‚": 3, "é«˜çº§": 4}[complexity]
                    recent_history = conversation_history[-history_limit:]
                    history_text = ""
                    for hist in recent_history:
                        history_text += f"ç”¨æˆ·ï¼š{hist['user']}\nç´«èˆ’ï¼š{hist['character']}\n"
                    instruction = context_base + f"\nå¯¹è¯å†å²ï¼š\n{history_text}ç”¨æˆ·å¯¹ä½ è¯´ï¼š{turn['user']}"
                else:
                    instruction = context_base + f"ç”¨æˆ·å¯¹ä½ è¯´ï¼š{turn['user']}"
                
                sample = {
                    "instruction": instruction,
                    "input": "",
                    "output": turn["character"],
                    "metadata": {
                        "character_type": character_type,
                        "scenario": scenario,
                        "complexity": complexity,
                        "turn_index": turn_idx,
                        "quality_score": dialogue["quality_score"],
                        "batch_number": batch_num
                    }
                }
                training_samples.append(sample)
                conversation_history.append(turn)
        
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for sample in training_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"ğŸ’¾ æ‰¹æ¬¡ {batch_num} å·²ä¿å­˜:")
        print(f"  ğŸ“„ å¯¹è¯æ•°æ®: {json_path}")
        print(f"  ğŸ”„ è®­ç»ƒæ•°æ®: {jsonl_path}")
        print(f"  ğŸ“Š å¯¹è¯æ•°é‡: {len(dialogues)}")
        print(f"  ğŸ“ˆ è®­ç»ƒæ ·æœ¬: {len(training_samples)}")
        
        return str(json_path)
    
    def merge_incremental_data(self, output_prefix: str = None) -> str:
        """åˆå¹¶æ‰€æœ‰å¢é‡æ•°æ®"""
        if not output_prefix:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_prefix = f"merged_dataset_{timestamp}"
        
        data_dir = Path("data/generated_dialogues")
        
        # æ”¶é›†æ‰€æœ‰å¢é‡æ–‡ä»¶
        json_files = list(data_dir.glob("incremental_dataset_batch_*.json"))
        jsonl_files = list(data_dir.glob("incremental_training_batch_*.jsonl"))
        
        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¢é‡æ•°æ®æ–‡ä»¶")
            return ""
        
        # åˆå¹¶å¯¹è¯æ•°æ®
        all_dialogues = []
        total_batches = 0
        
        for json_file in sorted(json_files):
            with open(json_file, 'r', encoding='utf-8') as f:
                batch_data = json.load(f)
                all_dialogues.extend(batch_data["dialogues"])
                total_batches += 1
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†
        merged_json = data_dir / f"{output_prefix}.json"
        final_dataset = {
            "dataset_info": {
                "total_dialogues": len(all_dialogues),
                "total_batches": total_batches,
                "merged_at": datetime.now().isoformat(),
                "character_distribution": {},
                "avg_quality": sum(d["quality_score"] for d in all_dialogues) / len(all_dialogues) if all_dialogues else 0
            },
            "dialogues": all_dialogues
        }
        
        # ç»Ÿè®¡è§’è‰²åˆ†å¸ƒ
        for dialogue in all_dialogues:
            char_type = dialogue["character_type"]
            final_dataset["dataset_info"]["character_distribution"][char_type] = \
                final_dataset["dataset_info"]["character_distribution"].get(char_type, 0) + 1
        
        with open(merged_json, 'w', encoding='utf-8') as f:
            json.dump(final_dataset, f, ensure_ascii=False, indent=2)
        
        # åˆå¹¶è®­ç»ƒæ•°æ®
        merged_jsonl = data_dir / f"{output_prefix}.jsonl"
        with open(merged_jsonl, 'w', encoding='utf-8') as outf:
            for jsonl_file in sorted(jsonl_files):
                with open(jsonl_file, 'r', encoding='utf-8') as inf:
                    for line in inf:
                        outf.write(line)
        
        print(f"ğŸ‰ æ•°æ®åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“„ åˆå¹¶å¯¹è¯æ•°æ®: {merged_json}")
        print(f"ğŸ”„ åˆå¹¶è®­ç»ƒæ•°æ®: {merged_jsonl}")
        print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {len(all_dialogues)}")
        print(f"ğŸ“ˆ æ€»è®­ç»ƒæ ·æœ¬: {sum(1 for _ in open(merged_jsonl, 'r'))}")
        
        return str(merged_json)
    
    def save_dataset_enhanced(self, dialogues: List[Dict], output_file: str = None) -> str:
        """ä¿å­˜å¢å¼ºç‰ˆæ•°æ®é›†"""
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"enhanced_dialogue_dataset_{timestamp}.json"
        
        output_path = Path("data/generated_dialogues") / output_file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è¯¦ç»†ç»Ÿè®¡
        stats = {
            "total_dialogues": len(dialogues),
            "character_distribution": {},
            "complexity_distribution": {},
            "scenario_distribution": {},
            "quality_stats": {
                "average_quality": sum(d["quality_score"] for d in dialogues) / len(dialogues) if dialogues else 0,
                "min_quality": min(d["quality_score"] for d in dialogues) if dialogues else 0,
                "max_quality": max(d["quality_score"] for d in dialogues) if dialogues else 0
            },
            "generation_stats": self.stats,
            "total_estimated_cost": sum(self.stats["cost_tracking"].values())
        }
        
        # åˆ†å¸ƒç»Ÿè®¡
        for dialogue in dialogues:
            char_type = dialogue["character_type"]
            complexity = dialogue["complexity"]
            scenario = dialogue["scenario"]
            
            stats["character_distribution"][char_type] = stats["character_distribution"].get(char_type, 0) + 1
            stats["complexity_distribution"][complexity] = stats["complexity_distribution"].get(complexity, 0) + 1
            stats["scenario_distribution"][scenario] = stats["scenario_distribution"].get(scenario, 0) + 1
        
        dataset = {
            "dataset_info": stats,
            "dialogues": dialogues
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°è¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ‰ å¢å¼ºç‰ˆæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_path}")
        print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {len(dialogues)}")
        print(f"â­ å¹³å‡è´¨é‡åˆ†: {stats['quality_stats']['average_quality']:.3f}")
        print(f"ğŸ’° é¢„ä¼°æ€»æˆæœ¬: ${stats['total_estimated_cost']:.4f}")
        print(f"ğŸ­ è§’è‰²åˆ†å¸ƒ: {stats['character_distribution']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(self.stats['successful']/(self.stats['successful']+self.stats['failed'])*100):.1f}%")
        
        return str(output_path)
    
    def convert_to_training_format_enhanced(self, dialogues: List[Dict], output_file: str = None) -> str:
        """è½¬æ¢ä¸ºå¢å¼ºè®­ç»ƒæ ¼å¼"""
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"enhanced_training_data_{timestamp}.jsonl"
        
        output_path = Path("data/generated_dialogues") / output_file
        
        training_samples = []
        
        for dialogue in dialogues:
            character_type = dialogue["character_type"]
            scenario = dialogue["scenario"]
            complexity = dialogue["complexity"]
            
            # æ„å»ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
            context_base = f"ä½ æ˜¯ç´«èˆ’è€å¸ˆï¼Œä¸€ä½å®³ç¾å¯çˆ±çš„æ¸©æŸ”å¥³è€å¸ˆã€‚å½“å‰æ¨¡å¼ï¼š{character_type}ã€‚å½“å‰åœºæ™¯ï¼š{scenario}ã€‚å¯¹è¯å¤æ‚åº¦ï¼š{complexity}ã€‚"
            
            conversation_history = []
            
            for turn_idx, turn in enumerate(dialogue["turns"]):
                # æ„å»ºåŒ…å«å†å²çš„ä¸Šä¸‹æ–‡
                if conversation_history:
                    history_text = ""
                    # æ ¹æ®å¤æ‚åº¦å†³å®šå†å²é•¿åº¦
                    history_limit = {"ç®€å•": 1, "ä¸­ç­‰": 2, "å¤æ‚": 3, "é«˜çº§": 4}[complexity]
                    recent_history = conversation_history[-history_limit:]
                    
                    for hist in recent_history:
                        history_text += f"ç”¨æˆ·ï¼š{hist['user']}\nç´«èˆ’ï¼š{hist['character']}\n"
                    
                    instruction = context_base + f"\nå¯¹è¯å†å²ï¼š\n{history_text}ç”¨æˆ·å¯¹ä½ è¯´ï¼š{turn['user']}"
                else:
                    instruction = context_base + f"ç”¨æˆ·å¯¹ä½ è¯´ï¼š{turn['user']}"
                
                sample = {
                    "instruction": instruction,
                    "input": "",
                    "output": turn["character"],
                    "metadata": {
                        "character_type": character_type,
                        "scenario": scenario,
                        "complexity": complexity,
                        "turn_index": turn_idx,
                        "quality_score": dialogue["quality_score"]
                    }
                }
                
                training_samples.append(sample)
                conversation_history.append(turn)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        with open(output_path, 'w', encoding='utf-8') as f:
            for sample in training_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"âœ… å¢å¼ºè®­ç»ƒæ•°æ®å·²ä¿å­˜: {output_path}")
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {len(training_samples)}")
        
        return str(output_path)
    
    def clean_existing_dataset(self, input_file: str, output_file: str = None) -> str:
        """æ¸…æ´—å·²æœ‰æ•°æ®é›†ï¼Œåªä¿ç•™ç¬¦åˆè¦æ±‚çš„æ€§æ ¼å¯¹è¯ - è°¨æ…ç‰ˆæœ¬"""
        print(f"ğŸ§¹ å¼€å§‹è°¨æ…æ¸…æ´—æ•°æ®é›†: {input_file}")
        
        # å®šä¹‰éœ€è¦ä¿ç•™çš„æ€§æ ¼å…³é”®å­— - æ›´ç²¾ç¡®
        target_personalities = {
            "å®³ç¾", "å†…å‘", "å¯çˆ±", "æ¸©æŸ”", "å®‰é™", "è½»å£°", "è„¸çº¢", 
            "å‘†èŒ", "å¤©ç„¶", "å–„è‰¯", "ä½“è´´", "è½»æŸ”", "å°å£°", "èƒ†å°"
        }
        
        # å®šä¹‰éœ€è¦ä¿ç•™çš„è¯­è¨€ç‰¹å¾ - æ›´ä¸¥æ ¼
        target_speech_patterns = {
            "å•Š...", "é‚£ä¸ª...", "å—¯...", "å¯¹ä¸èµ·", "ä¸å¥½æ„æ€", "å‘œ...",
            "è¯·", "è°¢è°¢", "æ²¡å…³ç³»çš„", "è½»è½»åœ°", "å°å£°è¯´", "æ¸©æŸ”åœ°",
            "è¯¶ï¼Ÿ", "å•Šï¼Ÿ", "å—¯å—¯", "æ˜¯è¿™æ ·å—", "ä¸å¤ªæ‡‚", "å‘œå‘œ",
            "æ˜¯çš„", "è°¢è°¢ä½ ", "å¥½çš„", "...", "å°å£°åœ°", "ç¾æ¶©åœ°"
        }
        
        # æ˜ç¡®æ’é™¤çš„ç‰¹å¾ - ä¸ç¬¦åˆå®³ç¾å¯çˆ±ç‰¹è´¨
        excluded_patterns = {
            "å¤§å£°", "å¼å«", "ç”Ÿæ°”", "æ„¤æ€’", "æš´èº", "ç²—é²", "éœ¸é“",
            "åš£å¼ ", "ç‹‚å¦„", "å†·é…·", "æ®‹å¿", "é‚ªæ¶", "é»‘æš—", "å†·æ¼ ",
            "é«˜å‚²", "è‡ªå¤§", "è”‘è§†", "å˜²ç¬‘", "è®½åˆº", "æŒ‘è¡…", "å¨èƒ"
        }
        
        # æ’é™¤çš„è§’è‰²ç±»å‹
        excluded_character_types = {
            "å†·é…·", "é«˜å‚²", "éœ¸é“", "é‚ªæ¶", "è…¹é»‘", "æ¯’èˆŒ", 
            "ä¸­äºŒç—…", "ç‹‚å¦„", "æš´èº", "å†·æ¼ ", "å¼ºåŠ¿"
        }
        
        # è¯»å–åŸå§‹æ•°æ®é›†
        with open(input_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        original_dialogues = original_data.get("dialogues", [])
        filtered_dialogues = []
        detailed_logs = []
        
        print(f"ğŸ“Š åŸå§‹å¯¹è¯æ•°é‡: {len(original_dialogues)}")
        
        for idx, dialogue in enumerate(original_dialogues):
            should_keep = False
            keep_reason = ""
            exclude_reason = ""
            personality_matches = 0
            pattern_matches = 0
            
            char_type = dialogue.get("character_type", "").lower()
            dialogue_text = ""
            
            # æ”¶é›†æ‰€æœ‰å¯¹è¯æ–‡æœ¬
            for turn in dialogue.get("turns", []):
                dialogue_text += turn.get("character", "") + " "
            
            dialogue_text_lower = dialogue_text.lower()
            
            # é¦–å…ˆæ£€æŸ¥æ’é™¤æ¡ä»¶
            excluded_char_match = any(exc_type in char_type for exc_type in excluded_character_types)
            excluded_pattern_count = sum(1 for pattern in excluded_patterns if pattern in dialogue_text_lower)
            
            if excluded_char_match:
                exclude_reason = f"è§’è‰²ç±»å‹ä¸ç¬¦åˆ: {char_type}"
            elif excluded_pattern_count >= 2:
                exclude_reason = f"åŒ…å«{excluded_pattern_count}ä¸ªæ’é™¤ç‰¹å¾"
            else:
                # æ£€æŸ¥ä¿ç•™æ¡ä»¶ - æ›´ä¸¥æ ¼çš„æ ‡å‡†
                if char_type in [c.lower() for c in self.characters.keys()]:
                    should_keep = True
                    keep_reason = "åŒ¹é…ç›®æ ‡è§’è‰²ç±»å‹"
                else:
                    # æ£€æŸ¥æ€§æ ¼å…³é”®å­—åŒ¹é…
                    personality_matches = sum(1 for keyword in target_personalities 
                                            if keyword in char_type or keyword in dialogue_text_lower)
                    
                    # æ£€æŸ¥è¯­è¨€ç‰¹å¾åŒ¹é…
                    pattern_matches = sum(1 for pattern in target_speech_patterns 
                                        if pattern in dialogue_text)
                    
                    # æ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼šéœ€è¦åŒæ—¶æ»¡è¶³æ€§æ ¼å’Œè¯­è¨€ç‰¹å¾
                    if personality_matches >= 2 and pattern_matches >= 3:
                        should_keep = True
                        keep_reason = f"æ€§æ ¼åŒ¹é…:{personality_matches}, è¯­è¨€ç‰¹å¾:{pattern_matches}"
                    elif personality_matches >= 3:  # æˆ–è€…æ€§æ ¼ç‰¹å¾éå¸¸æ˜æ˜¾
                        should_keep = True
                        keep_reason = f"å¼ºæ€§æ ¼åŒ¹é…:{personality_matches}"
                    elif pattern_matches >= 5:  # æˆ–è€…è¯­è¨€ç‰¹å¾éå¸¸æ˜æ˜¾
                        should_keep = True
                        keep_reason = f"å¼ºè¯­è¨€ç‰¹å¾:{pattern_matches}"
                    else:
                        exclude_reason = f"åŒ¹é…åº¦ä¸è¶³(æ€§æ ¼:{personality_matches}, è¯­è¨€:{pattern_matches})"
            
            # é¢å¤–çš„è´¨é‡æ£€æŸ¥
            if should_keep:
                # æ£€æŸ¥å¯¹è¯é•¿åº¦
                if len(dialogue.get("turns", [])) < 2:
                    should_keep = False
                    exclude_reason = "å¯¹è¯è½®æ•°ä¸è¶³"
                # æ£€æŸ¥æ¯è½®å¯¹è¯çš„è´¨é‡
                elif any(len(turn.get("character", "").strip()) < 5 for turn in dialogue.get("turns", [])):
                    should_keep = False
                    exclude_reason = "å¯¹è¯å†…å®¹è¿‡çŸ­"
            
            # è®°å½•è¯¦ç»†æ—¥å¿—
            log_entry = {
                "index": idx,
                "character_type": dialogue.get("character_type", ""),
                "turns_count": len(dialogue.get("turns", [])),
                "kept": should_keep,
                "reason": keep_reason if should_keep else exclude_reason,
                "dialogue_preview": dialogue_text[:100] + "..." if len(dialogue_text) > 100 else dialogue_text
            }
            detailed_logs.append(log_entry)
            
            if should_keep:
                # æ›´æ–°è§’è‰²ç±»å‹ä¸ºæ–°çš„åˆ†ç±»ç³»ç»Ÿ
                new_char_type = self._map_to_new_character_type(dialogue)
                dialogue["character_type"] = new_char_type
                dialogue["original_character_type"] = dialogue.get("character_type", "")
                dialogue["cleaning_score"] = personality_matches + pattern_matches  # æ·»åŠ æ¸…æ´—è¯„åˆ†
                filtered_dialogues.append(dialogue)
        
        print(f"âœ… è¿‡æ»¤åå¯¹è¯æ•°é‡: {len(filtered_dialogues)}")
        print(f"ğŸ“‰ è¿‡æ»¤æ¯”ä¾‹: {(1 - len(filtered_dialogues)/len(original_dialogues))*100:.1f}%")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        kept_reasons = {}
        excluded_reasons = {}
        for log in detailed_logs:
            if log["kept"]:
                kept_reasons[log["reason"]] = kept_reasons.get(log["reason"], 0) + 1
            else:
                excluded_reasons[log["reason"]] = excluded_reasons.get(log["reason"], 0) + 1
        
        print(f"\nğŸ“ˆ ä¿ç•™åŸå› ç»Ÿè®¡:")
        for reason, count in sorted(kept_reasons.items()):
            print(f"  {reason}: {count}ä¸ª")
        
        print(f"\nğŸ“‰ æ’é™¤åŸå› ç»Ÿè®¡:")
        for reason, count in sorted(excluded_reasons.items()):
            print(f"  {reason}: {count}ä¸ª")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"cleaned_strict_{timestamp}.json"
        
        output_path = Path("data/generated_dialogues") / output_file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        cleaned_stats = {
            "total_dialogues": len(filtered_dialogues),
            "original_count": len(original_dialogues),
            "filter_ratio": (1 - len(filtered_dialogues)/len(original_dialogues)) if original_dialogues else 0,
            "character_distribution": {},
            "cleaning_criteria": {
                "target_personalities": list(target_personalities),
                "target_speech_patterns": list(target_speech_patterns),
                "excluded_patterns": list(excluded_patterns),
                "excluded_character_types": list(excluded_character_types),
                "min_personality_matches": 2,
                "min_pattern_matches": 3,
                "strict_mode": True
            },
            "cleaning_statistics": {
                "kept_reasons": kept_reasons,
                "excluded_reasons": excluded_reasons
            },
            "cleaned_at": datetime.now().isoformat()
        }
        
        # ç»Ÿè®¡æ–°çš„è§’è‰²åˆ†å¸ƒå’Œè´¨é‡åˆ†å¸ƒ
        quality_scores = []
        for dialogue in filtered_dialogues:
            char_type = dialogue["character_type"]
            cleaned_stats["character_distribution"][char_type] = \
                cleaned_stats["character_distribution"].get(char_type, 0) + 1
            quality_scores.append(dialogue.get("cleaning_score", 0))
        
        if quality_scores:
            cleaned_stats["quality_distribution"] = {
                "average_score": sum(quality_scores) / len(quality_scores),
                "min_score": min(quality_scores),
                "max_score": max(quality_scores)
            }
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®é›†
        cleaned_dataset = {
            "dataset_info": cleaned_stats,
            "dialogues": filtered_dialogues,
            "cleaning_logs": detailed_logs  # ä¿å­˜è¯¦ç»†æ—¥å¿—ä¾›äººå·¥å®¡æ ¸
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(cleaned_dataset, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š
        report_path = output_path.with_suffix('.cleaning_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®æ¸…æ´—æŠ¥å‘Š\n")
            f.write(f"=" * 50 + "\n")
            f.write(f"åŸå§‹æ•°æ®: {len(original_dialogues)} ä¸ªå¯¹è¯\n")
            f.write(f"æ¸…æ´—å: {len(filtered_dialogues)} ä¸ªå¯¹è¯\n")
            f.write(f"è¿‡æ»¤æ¯”ä¾‹: {(1 - len(filtered_dialogues)/len(original_dialogues))*100:.1f}%\n\n")
            
            f.write("ä¿ç•™åŸå› ç»Ÿè®¡:\n")
            for reason, count in sorted(kept_reasons.items()):
                f.write(f"  {reason}: {count}ä¸ª\n")
            
            f.write("\næ’é™¤åŸå› ç»Ÿè®¡:\n")
            for reason, count in sorted(excluded_reasons.items()):
                f.write(f"  {reason}: {count}ä¸ª\n")
            
            f.write(f"\nè§’è‰²åˆ†å¸ƒ:\n")
            for char_type, count in cleaned_stats["character_distribution"].items():
                f.write(f"  {char_type}: {count}ä¸ª\n")
        
        print(f"ğŸ‰ è°¨æ…æ¸…æ´—å®Œæˆ!")
        print(f"ğŸ“ æ¸…æ´—åæ•°æ®é›†: {output_path}")
        print(f"ğŸ“„ æ¸…æ´—æŠ¥å‘Š: {report_path}")
        print(f"ğŸ“Š è§’è‰²åˆ†å¸ƒ: {cleaned_stats['character_distribution']}")
        
        return str(output_path)
    
    def _map_to_new_character_type(self, dialogue: Dict) -> str:
        """å°†å¯¹è¯æ˜ å°„åˆ°æ–°çš„è§’è‰²ç±»å‹ç³»ç»Ÿ"""
        original_type = dialogue.get("character_type", "").lower()
        dialogue_text = ""
        
        # æ”¶é›†å¯¹è¯æ–‡æœ¬
        for turn in dialogue.get("turns", []):
            dialogue_text += turn.get("character", "") + " "
        
        dialogue_text = dialogue_text.lower()
        
        # æ˜ å°„è§„åˆ™ - åªä¿ç•™å®³ç¾å¯çˆ±å†…å‘ç‰¹è´¨
        if "æ¸©æŸ”" in original_type or any(word in dialogue_text for word in ["è½»å£°", "æ¸©æŸ”", "ç…§é¡¾", "ä½“è´´"]):
            return "æ¸©æŸ”å†…å‘"
        elif any(word in dialogue_text for word in ["è¯¶ï¼Ÿ", "å•Šï¼Ÿ", "ä¸å¤ªæ‡‚", "å‘†"]):
            return "å‘†èŒå®³ç¾"
        elif any(word in dialogue_text for word in ["å®‰é™", "å—¯", "ç‚¹å¤´", "ä¸å¤ªè¯´è¯"]):
            return "å®‰é™å¯çˆ±"
        else:
            return "å®³ç¾å¯çˆ±"  # é»˜è®¤åˆ†ç±»
    
    def batch_clean_incremental_files(self, output_prefix: str = None) -> str:
        """æ‰¹é‡æ¸…æ´—æ‰€æœ‰å¢é‡æ–‡ä»¶"""
        if not output_prefix:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_prefix = f"batch_cleaned_{timestamp}"
        
        data_dir = Path("data/generated_dialogues")
        
        # æŸ¥æ‰¾æ‰€æœ‰éœ€è¦æ¸…æ´—çš„æ–‡ä»¶
        json_files = list(data_dir.glob("*.json"))
        cleaned_files = []
        
        print(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ªæ•°æ®æ–‡ä»¶å¾…æ¸…æ´—")
        
        for json_file in json_files:
            if "cleaned" in json_file.name or "incremental_dataset" not in json_file.name:
                continue  # è·³è¿‡å·²æ¸…æ´—æˆ–éå¢é‡æ–‡ä»¶
            
            print(f"ğŸ§¹ æ­£åœ¨æ¸…æ´—: {json_file.name}")
            cleaned_file = self.clean_existing_dataset(
                str(json_file),
                f"cleaned_{json_file.stem}_{datetime.now().strftime('%H%M%S')}.json"
            )
            cleaned_files.append(cleaned_file)
        
        print(f"âœ… æ‰¹é‡æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç† {len(cleaned_files)} ä¸ªæ–‡ä»¶")
        return cleaned_files
    
    def preview_cleaning_results(self, input_file: str, sample_size: int = 10) -> Dict:
        """é¢„è§ˆæ¸…æ´—ç»“æœï¼Œç”¨äºäººå·¥å®¡æ ¸"""
        print(f"ğŸ” é¢„è§ˆæ¸…æ´—ç»“æœ: {input_file}")
        
        # ä½¿ç”¨ä¸´æ—¶æ¸…æ´—é€»è¾‘é¢„è§ˆ
        with open(input_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        original_dialogues = original_data.get("dialogues", [])
        
        # å¿«é€Ÿæ¨¡æ‹Ÿæ¸…æ´—è¿‡ç¨‹
        target_personalities = {"å®³ç¾", "å†…å‘", "å¯çˆ±", "æ¸©æŸ”", "å®‰é™", "è½»å£°", "è„¸çº¢", "å‘†èŒ", "å¤©ç„¶", "å–„è‰¯", "ä½“è´´", "è½»æŸ”", "å°å£°", "èƒ†å°"}
        target_speech_patterns = {"å•Š...", "é‚£ä¸ª...", "å—¯...", "å¯¹ä¸èµ·", "ä¸å¥½æ„æ€", "å‘œ...", "è¯·", "è°¢è°¢", "æ²¡å…³ç³»çš„", "è½»è½»åœ°", "å°å£°è¯´", "æ¸©æŸ”åœ°", "è¯¶ï¼Ÿ", "å•Šï¼Ÿ", "å—¯å—¯", "æ˜¯è¿™æ ·å—", "ä¸å¤ªæ‡‚", "å‘œå‘œ", "æ˜¯çš„", "è°¢è°¢ä½ ", "å¥½çš„", "...", "å°å£°åœ°", "ç¾æ¶©åœ°"}
        excluded_patterns = {"å¤§å£°", "å¼å«", "ç”Ÿæ°”", "æ„¤æ€’", "æš´èº", "ç²—é²", "éœ¸é“", "åš£å¼ ", "ç‹‚å¦„", "å†·é…·", "æ®‹å¿", "é‚ªæ¶", "é»‘æš—", "å†·æ¼ ", "é«˜å‚²", "è‡ªå¤§", "è”‘è§†", "å˜²ç¬‘", "è®½åˆº", "æŒ‘è¡…", "å¨èƒ"}
        excluded_character_types = {"å†·é…·", "é«˜å‚²", "éœ¸é“", "é‚ªæ¶", "è…¹é»‘", "æ¯’èˆŒ", "ä¸­äºŒç—…", "ç‹‚å¦„", "æš´èº", "å†·æ¼ ", "å¼ºåŠ¿"}
        
        keep_samples = []
        discard_samples = []
        
        for idx, dialogue in enumerate(original_dialogues[:100]):  # åªé¢„è§ˆå‰100ä¸ª
            char_type = dialogue.get("character_type", "").lower()
            dialogue_text = ""
            for turn in dialogue.get("turns", []):
                dialogue_text += turn.get("character", "") + " "
            
            dialogue_text_lower = dialogue_text.lower()
            
            # æ¨¡æ‹Ÿæ¸…æ´—åˆ¤æ–­
            excluded_char_match = any(exc_type in char_type for exc_type in excluded_character_types)
            excluded_pattern_count = sum(1 for pattern in excluded_patterns if pattern in dialogue_text_lower)
            
            should_keep = False
            reason = ""
            
            if excluded_char_match:
                reason = f"æ’é™¤è§’è‰²ç±»å‹: {char_type}"
            elif excluded_pattern_count >= 2:
                reason = f"åŒ…å«{excluded_pattern_count}ä¸ªæ’é™¤ç‰¹å¾"
            else:
                personality_matches = sum(1 for keyword in target_personalities if keyword in char_type or keyword in dialogue_text_lower)
                pattern_matches = sum(1 for pattern in target_speech_patterns if pattern in dialogue_text)
                
                if personality_matches >= 2 and pattern_matches >= 3:
                    should_keep = True
                    reason = f"åŒ¹é…(æ€§æ ¼:{personality_matches}, è¯­è¨€:{pattern_matches})"
                elif personality_matches >= 3:
                    should_keep = True
                    reason = f"å¼ºæ€§æ ¼åŒ¹é…:{personality_matches}"
                elif pattern_matches >= 5:
                    should_keep = True
                    reason = f"å¼ºè¯­è¨€ç‰¹å¾:{pattern_matches}"
                else:
                    reason = f"åŒ¹é…åº¦ä¸è¶³(æ€§æ ¼:{personality_matches}, è¯­è¨€:{pattern_matches})"
            
            sample = {
                "index": idx,
                "character_type": dialogue.get("character_type", ""),
                "turns_count": len(dialogue.get("turns", [])),
                "reason": reason,
                "preview": dialogue_text[:150] + "..." if len(dialogue_text) > 150 else dialogue_text,
                "first_turn": dialogue.get("turns", [{}])[0].get("character", "") if dialogue.get("turns") else "",
                "should_keep": should_keep
            }
            
            if should_keep:
                keep_samples.append(sample)
            else:
                discard_samples.append(sample)
        
        # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œå±•ç¤º
        import random
        keep_preview = random.sample(keep_samples, min(sample_size, len(keep_samples)))
        discard_preview = random.sample(discard_samples, min(sample_size, len(discard_samples)))
        
        print(f"\nğŸ“Š é¢„è§ˆç»Ÿè®¡ (åŸºäºå‰100ä¸ªå¯¹è¯):")
        print(f"  å°†ä¿ç•™: {len(keep_samples)} ä¸ª")
        print(f"  å°†ä¸¢å¼ƒ: {len(discard_samples)} ä¸ª")
        print(f"  é¢„è®¡ä¿ç•™ç‡: {len(keep_samples)/(len(keep_samples)+len(discard_samples))*100:.1f}%")
        
        print(f"\nâœ… å°†ä¿ç•™çš„å¯¹è¯æ ·æœ¬ (éšæœº{len(keep_preview)}ä¸ª):")
        for i, sample in enumerate(keep_preview, 1):
            print(f"  {i}. [{sample['character_type']}] {sample['reason']}")
            print(f"     é¦–å¥: {sample['first_turn'][:80]}...")
            print()
        
        print(f"\nâŒ å°†ä¸¢å¼ƒçš„å¯¹è¯æ ·æœ¬ (éšæœº{len(discard_preview)}ä¸ª):")
        for i, sample in enumerate(discard_preview, 1):
            print(f"  {i}. [{sample['character_type']}] {sample['reason']}")
            print(f"     é¦–å¥: {sample['first_turn'][:80]}...")
            print()
        
        return {
            "total_previewed": len(keep_samples) + len(discard_samples),
            "keep_count": len(keep_samples),
            "discard_count": len(discard_samples),
            "keep_rate": len(keep_samples)/(len(keep_samples)+len(discard_samples)) if (len(keep_samples)+len(discard_samples)) > 0 else 0,
            "keep_samples": keep_preview,
            "discard_samples": discard_preview
        }
    
    def interactive_clean_dataset(self, input_file: str, output_file: str = None) -> str:
        """äº¤äº’å¼æ¸…æ´—æ•°æ®é›†ï¼ŒåŒ…å«é¢„è§ˆå’Œç¡®è®¤æ­¥éª¤"""
        print(f"ğŸ¯ äº¤äº’å¼æ•°æ®æ¸…æ´—: {input_file}")
        
        # é¦–å…ˆé¢„è§ˆç»“æœ
        preview_result = self.preview_cleaning_results(input_file)
        
        print(f"\nğŸ¤” åŸºäºé¢„è§ˆç»“æœï¼Œé¢„è®¡:")
        print(f"  ä¿ç•™ç‡: {preview_result['keep_rate']*100:.1f}%")
        print(f"  é¢„è®¡ä» {len(json.load(open(input_file))['dialogues'])} ä¸ªå¯¹è¯ä¸­ä¿ç•™çº¦ {int(len(json.load(open(input_file))['dialogues']) * preview_result['keep_rate'])} ä¸ª")
        
        # ç”¨æˆ·ç¡®è®¤
        while True:
            choice = input("\nè¯·é€‰æ‹©æ“ä½œ:\n1. ç»§ç»­æ¸…æ´— (c)\n2. è°ƒæ•´å‚æ•°åé‡æ–°é¢„è§ˆ (p)\n3. å–æ¶ˆ (q)\nè¯·è¾“å…¥é€‰æ‹© (c/p/q): ").lower().strip()
            
            if choice in ['c', 'continue', '1']:
                print("âœ… å¼€å§‹æ­£å¼æ¸…æ´—...")
                return self.clean_existing_dataset(input_file, output_file)
            elif choice in ['p', 'preview', '2']:
                print("ğŸ”„ é‡æ–°é¢„è§ˆ...")
                self.preview_cleaning_results(input_file, 15)  # æ›´å¤šæ ·æœ¬
            elif choice in ['q', 'quit', '3']:
                print("âŒ å–æ¶ˆæ¸…æ´—æ“ä½œ")
                return ""
            else:
                print("â“ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

async def main():
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆäºŒæ¬¡å…ƒå¯¹è¯æ•°æ®ç”Ÿæˆ")
    parser.add_argument("--chars_per_type", type=int, default=30, help="æ¯ä¸ªè§’è‰²ç±»å‹ç”Ÿæˆçš„å¯¹è¯æ•°é‡")
    parser.add_argument("--concurrent_limit", type=int, default=5, help="å¹¶å‘APIè°ƒç”¨æ•°é‡")
    parser.add_argument("--config_file", type=str, default="config/api_config.json", help="APIé…ç½®æ–‡ä»¶")
    parser.add_argument("--output_file", type=str, help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--create_training_data", action="store_true", help="åŒæ—¶åˆ›å»ºè®­ç»ƒæ•°æ®")
    parser.add_argument("--complexity_simple", type=float, default=0.2, help="ç®€å•å¯¹è¯æ¯”ä¾‹")
    parser.add_argument("--complexity_medium", type=float, default=0.5, help="ä¸­ç­‰å¯¹è¯æ¯”ä¾‹") 
    parser.add_argument("--complexity_complex", type=float, default=0.2, help="å¤æ‚å¯¹è¯æ¯”ä¾‹")
    parser.add_argument("--complexity_advanced", type=float, default=0.1, help="é«˜çº§å¯¹è¯æ¯”ä¾‹")
    parser.add_argument("--save_interval", type=int, default=100, help="æ¯å¤šå°‘ä¸ªå¯¹è¯ä¿å­˜ä¸€æ¬¡")
    parser.add_argument("--merge_only", action="store_true", help="ä»…åˆå¹¶å·²æœ‰çš„å¢é‡æ•°æ®ï¼Œä¸ç”Ÿæˆæ–°æ•°æ®")
    
    # æ·»åŠ æ•°æ®æ¸…æ´—ç›¸å…³å‚æ•°
    parser.add_argument("--clean_dataset", type=str, help="æ¸…æ´—æŒ‡å®šçš„æ•°æ®é›†æ–‡ä»¶ï¼Œåªä¿ç•™å®³ç¾å¯çˆ±çš„å¯¹è¯")
    parser.add_argument("--batch_clean", action="store_true", help="æ‰¹é‡æ¸…æ´—æ‰€æœ‰å¢é‡æ•°æ®æ–‡ä»¶")
    parser.add_argument("--clean_output", type=str, help="æ¸…æ´—åçš„è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--preview_clean", type=str, help="é¢„è§ˆæ¸…æ´—ç»“æœï¼Œä¸æ‰§è¡Œå®é™…æ¸…æ´—")
    parser.add_argument("--interactive_clean", type=str, help="äº¤äº’å¼æ¸…æ´—æ•°æ®é›†ï¼ŒåŒ…å«é¢„è§ˆå’Œç¡®è®¤")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¢å¼ºç‰ˆäºŒæ¬¡å…ƒå¯¹è¯æ•°æ®ç”Ÿæˆå™¨")
    print("ğŸ’ æ”¯æŒé«˜è´¨é‡å•†ä¸šAPIï¼Œå¤§è§„æ¨¡å¹¶å‘ç”Ÿæˆ")
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = EnhancedDialogueGenerator(args.config_file)
    
    # æ£€æŸ¥APIé…ç½®
    providers = generator._get_available_providers()
    if not providers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„APIæä¾›å•†ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(providers)} ä¸ªå¯ç”¨çš„API: {', '.join(providers)}")
    
    # æ•°æ®æ¸…æ´—æ¨¡å¼
    if args.clean_dataset:
        print(f"ğŸ§¹ æ•°æ®æ¸…æ´—æ¨¡å¼: æ¸…æ´—æ–‡ä»¶ {args.clean_dataset}")
        cleaned_file = generator.clean_existing_dataset(args.clean_dataset, args.clean_output)
        print(f"âœ… æ¸…æ´—å®Œæˆ: {cleaned_file}")
        return
    
    if args.preview_clean:
        print(f"ğŸ” é¢„è§ˆæ¸…æ´—æ¨¡å¼: é¢„è§ˆæ–‡ä»¶ {args.preview_clean}")
        generator.preview_cleaning_results(args.preview_clean, 10)
        return
    
    if args.interactive_clean:
        print(f"ğŸ¯ äº¤äº’å¼æ¸…æ´—æ¨¡å¼: å¤„ç†æ–‡ä»¶ {args.interactive_clean}")
        cleaned_file = generator.interactive_clean_dataset(args.interactive_clean, args.clean_output)
        if cleaned_file:
            print(f"âœ… äº¤äº’å¼æ¸…æ´—å®Œæˆ: {cleaned_file}")
        return
    
    if args.batch_clean:
        print("ğŸ§¹ æ‰¹é‡æ¸…æ´—æ¨¡å¼: æ¸…æ´—æ‰€æœ‰å¢é‡æ•°æ®æ–‡ä»¶")
        cleaned_files = generator.batch_clean_incremental_files()
        print(f"âœ… æ‰¹é‡æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç† {len(cleaned_files)} ä¸ªæ–‡ä»¶")
        return
    
    # åˆ›å»ºç”Ÿæˆè®¡åˆ’
    complexity_dist = {
        "ç®€å•": args.complexity_simple,
        "ä¸­ç­‰": args.complexity_medium,
        "å¤æ‚": args.complexity_complex,
        "é«˜çº§": args.complexity_advanced
    }
    
    generation_plan = generator.create_generation_plan(args.chars_per_type, complexity_dist)
    
    # å¦‚æœåªæ˜¯åˆå¹¶æ•°æ®
    if args.merge_only:
        print("ğŸ”„ åˆå¹¶æ¨¡å¼: ä»…åˆå¹¶å·²æœ‰çš„å¢é‡æ•°æ®")
        merged_file = generator.merge_incremental_data()
        print(f"âœ… åˆå¹¶å®Œæˆ: {merged_file}")
        return
    
    print(f"ğŸ“‹ ç”Ÿæˆè®¡åˆ’: {len(generation_plan)} ä¸ªå¯¹è¯")
    print(f"âš¡ å¹¶å‘æ•°: {args.concurrent_limit}")
    print(f"ğŸ’¾ æ¯ {args.save_interval} ä¸ªå¯¹è¯è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡")
    
    # å¼‚æ­¥ç”Ÿæˆ - ä½¿ç”¨å¢é‡ä¿å­˜æ¨¡å¼
    start_time = time.time()
    dialogues = await generator.generate_batch_async(generation_plan, args.concurrent_limit, args.save_interval)
    generation_time = time.time() - start_time
    
    # ç”Ÿæˆå®Œæ¯•åï¼Œè‡ªåŠ¨åˆå¹¶æ‰€æœ‰å¢é‡æ•°æ®
    print("\nğŸ”„ æ­£åœ¨åˆå¹¶æ‰€æœ‰å¢é‡æ•°æ®...")
    merged_file = generator.merge_incremental_data()
    print(f"âœ… æœ€ç»ˆåˆå¹¶æ–‡ä»¶: {merged_file}")
    
    if not dialogues:
        print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•å¯¹è¯")
        return
    
    print(f"â±ï¸ ç”Ÿæˆç”¨æ—¶: {generation_time:.1f}ç§’")
    
    # ä¿å­˜æ•°æ®é›†
    generator.save_dataset_enhanced(dialogues, args.output_file)
    
    # å¯é€‰ï¼šåˆ›å»ºè®­ç»ƒæ•°æ®
    if args.create_training_data:
        generator.convert_to_training_format_enhanced(dialogues)
    
    print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆ! é«˜è´¨é‡å¯¹è¯æ•°æ®å·²å‡†å¤‡å°±ç»ª")

if __name__ == "__main__":
    asyncio.run(main()) 