#! /usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys

sys.modules['awq'] = None  # é˜»æ­¢AWQå¯¼å…¥
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),'..')))

import json
import torch
import argparse
import warnings
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)

from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training

from src.utils.config_manager import ConfigManager
from src.model.lora import LoraManager

def parse_args():
    parser = argparse.ArgumentParser(description="å¾®è°ƒChinese-Mistral-7Bæ¨¡å‹")
    parser.add_argument("--config",type=str,default="./config/training_config.json",help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_config",type=str,default="./config/model_config.json",help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data_dir",type=str,default="./data/train",help="è®­ç»ƒæ•°æ®ç›®å½•")
    parser.add_argument("--output_dir",type=str,default="./output",help="è¾“å‡ºç›®å½•")
    parser.add_argument("--local_rank",type=int,default=-1,help="åˆ†å¸ƒå¼è®­ç»ƒçš„æœ¬åœ°rank")
    parser.add_argument("--max_samples",type=int,default=None,help="é™åˆ¶è®­ç»ƒæ•°æ®æ ·æœ¬æ•°é‡ï¼Œç”¨äºå¿«é€ŸéªŒè¯")
    parser.add_argument("--dialogue_data",type=str,default=None,help="ç›´æ¥æŒ‡å®šå¯¹è¯æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--validation_split",type=float,default=0.1,help="éªŒè¯é›†æ¯”ä¾‹(å½“ä½¿ç”¨å•ä¸€å¯¹è¯æ•°æ®æ–‡ä»¶æ—¶)")
    parser.add_argument("--check_format_only",action="store_true",help="åªæ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œä¸è¿›è¡Œè®­ç»ƒ")
    parser.add_argument("--base_adapter_path",type=str,default=None,help="åŸºç¡€é€‚é…å™¨æ¨¡å‹è·¯å¾„(ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ)")
    parser.add_argument("--default_personality",action="store_true",help="ä½¿ç”¨é»˜è®¤äººæ ¼è®­ç»ƒæ¨¡å¼ï¼ˆæ— ç³»ç»Ÿæç¤ºè¯ï¼‰")
    return parser.parse_args()

def setup_model_and_tokenizer(model_config):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    model_id = model_config.get("base_model", {}).get("id", "itpossible/Chinese-Mistral-7B-v0.1")
    model_path = model_config.get("base_model", {}).get("path", "./models/base/chinese-mistral-7b-v0.1")

    #é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    #åŠ è½½æ¨¡å‹
    model_to_load = model_path if os.path.exists(model_path) else model_id
    model = AutoModelForCausalLM.from_pretrained(
        model_to_load,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        use_cache=False,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True
    )
    
    #åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_to_load,
        trust_remote_code=True,
        use_fast=False
    )
    
    #æ·»åŠ å¿…è¦çš„ç‰¹æ®Štoken
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    #é…ç½®lora
    lora_config = LoraConfig(
        r = model_config["lora_config"]["r"],
        lora_alpha = model_config["lora_config"]["lora_alpha"],
        lora_dropout = model_config["lora_config"]["lora_dropout"],
        bias = "none",
        task_type = "CAUSAL_LM",
        target_modules = model_config["lora_config"].get("target_modules",["q_proj","v_proj"])
    )
    
    #å‡†å¤‡æ¨¡å‹è¿›è¡ŒLoraå¾®è°ƒ
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model,lora_config)
    
    return model,tokenizer

def setup_model_with_existing_adapter(model_config, adapter_path=None):
    """è®¾ç½®æ¨¡å‹ï¼Œå¯é€‰æ‹©ä»å·²æœ‰é€‚é…å™¨å¼€å§‹"""
    model_id = model_config.get("base_model", {}).get("id", "itpossible/Chinese-Mistral-7B-v0.1")
    model_path = model_config.get("base_model", {}).get("path", "./models/base/chinese-mistral-7b-v0.1")

    #é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    #åŠ è½½åŸºç¡€æ¨¡å‹
    model_to_load = model_path if os.path.exists(model_path) else model_id
    model = AutoModelForCausalLM.from_pretrained(
        model_to_load,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        use_cache=False,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True
    )
    
    #åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_to_load,
        trust_remote_code=True,
        use_fast=False
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    if adapter_path:
        # ä»å·²æœ‰é€‚é…å™¨å¼€å§‹
        print(f"ğŸ“‚ ä»å·²æœ‰é€‚é…å™¨ç»§ç»­è®­ç»ƒ: {adapter_path}")
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, adapter_path, is_trainable=True)
    else:
        # åˆ›å»ºæ–°çš„LoRAé…ç½®
        lora_config = LoraConfig(
            r = model_config["lora_config"]["r"],
            lora_alpha = model_config["lora_config"]["lora_alpha"],
            lora_dropout = model_config["lora_config"]["lora_dropout"],
            bias = "none",
            task_type = "CAUSAL_LM",
            target_modules = model_config["lora_config"].get("target_modules",["q_proj","v_proj"])
        )
        model = get_peft_model(model,lora_config)
    
    return model, tokenizer

def enhance_output_with_identity(output, sample_index, user_input=""):
    """æ™ºèƒ½æ·»åŠ èº«ä»½ç¡®è®¤ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡å†³å®šæ˜¯å¦éœ€è¦è‡ªæˆ‘ä»‹ç»"""
    import random
    
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ä¸€è‡´æ€§
    random.seed(sample_index)
    
    # æ£€æŸ¥ç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦èº«ä»½ç¡®è®¤
    identity_triggers = [
        "ä½ æ˜¯è°", "ä½ å«ä»€ä¹ˆ", "è‡ªæˆ‘ä»‹ç»", "ä½ çš„åå­—", 
        "è®¤è¯†ä¸€ä¸‹", "ä»‹ç»è‡ªå·±", "ä½ å¥½", "åˆæ¬¡è§é¢"
    ]
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦èº«ä»½ç¡®è®¤çš„åœºæ™¯
    needs_identity = any(trigger in user_input for trigger in identity_triggers)
    
    # è‡ªç„¶çš„èº«ä»½ç¡®è®¤æ¨¡æ¿ï¼ˆæ›´åŠ èå…¥å¯¹è¯ï¼‰
    natural_templates = [
        "å—¯...æˆ‘å«ç´«èˆ’...{}",
        "æˆ‘æ˜¯ç´«èˆ’è€å¸ˆ...{}",  
        "{}...æˆ‘æ˜¯ç´«èˆ’å‘¢...",
        "{}ï¼ˆæˆ‘...æˆ‘å«ç´«èˆ’...ï¼‰",
        "ä½œä¸ºè€å¸ˆ...æˆ‘æ˜¯ç´«èˆ’...{}"
    ]
    
    # éšæœºçš„èº«ä»½æåŠï¼ˆæ›´è‡ªç„¶ï¼‰
    casual_mentions = [
        "{}...æˆ‘ç´«èˆ’è§‰å¾—...",
        "ä»æˆ‘ç´«èˆ’çš„è§’åº¦æ¥çœ‹...{}",
        "{}...è¿™è®©æˆ‘æƒ³èµ·..."
    ]
    
    # å†³å®šæ˜¯å¦æ·»åŠ èº«ä»½ç¡®è®¤
    if needs_identity:
        # å¦‚æœç”¨æˆ·é—®èº«ä»½ï¼Œ80%æ¦‚ç‡å›ç­”
        if random.random() < 0.8:
            template = random.choice(natural_templates)
            enhanced_output = template.format(output)
            return enhanced_output
    else:
        # æ™®é€šå¯¹è¯ï¼Œ15%æ¦‚ç‡è‡ªç„¶æåŠ
        if random.random() < 0.15:
            template = random.choice(casual_mentions)
            enhanced_output = template.format(output)
            return enhanced_output
    
    return output

def convert_dialogue_to_text(examples, tokenizer):
    """å°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºé»˜è®¤äººæ ¼è®­ç»ƒæ ¼å¼ï¼ˆæ— ç³»ç»Ÿæç¤ºè¯ï¼Œä¿ç•™å¤šè½®å¯¹è¯ï¼‰"""
    texts = []
    
    for i in range(len(examples["instruction"])):
        instruction = examples["instruction"][i]
        user_input = examples["input"][i] if "input" in examples else ""
        output = examples["output"][i]
        
        # ã€å…³é”®ã€‘æ‰‹åŠ¨æ„å»ºChatMLæ ¼å¼ - å®Œå…¨ç§»é™¤ç³»ç»Ÿæç¤ºè¯ï¼Œä¿ç•™å¤šè½®å¯¹è¯ï¼
        formatted_text = ""
        current_user_message = ""  # åˆå§‹åŒ–
        
        if "å¯¹è¯å†å²ï¼š" in instruction:
            # æå–å¯¹è¯å†å²éƒ¨åˆ†
            history_part = instruction.split("å¯¹è¯å†å²ï¼š")[1]
            
            # åˆ†ç¦»å†å²å’Œå½“å‰ç”¨æˆ·æ¶ˆæ¯
            if "ç”¨æˆ·å¯¹ä½ è¯´ï¼š" in history_part:
                history_lines = history_part.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[0].strip().split("\n")
                current_user_message = history_part.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[1].strip()
            else:
                history_lines = history_part.strip().split("\n")
                current_user_message = ""
            
            # å¤„ç†å¯¹è¯å†å²ï¼Œè½¬æ¢ä¸ºChatMLæ ¼å¼
            for line in history_lines:
                line = line.strip()
                if line.startswith("ç”¨æˆ·ï¼š"):
                    user_msg = line.replace("ç”¨æˆ·ï¼š", "").strip()
                    if user_msg:  # ç¡®ä¿ä¸æ˜¯ç©ºæ¶ˆæ¯
                        formatted_text += f"<|im_start|>user\n{user_msg}<|im_end|>\n"
                elif line.startswith("ç´«èˆ’ï¼š"):
                    assistant_msg = line.replace("ç´«èˆ’ï¼š", "").strip()
                    if assistant_msg:  # ç¡®ä¿ä¸æ˜¯ç©ºæ¶ˆæ¯
                        formatted_text += f"<|im_start|>assistant\n{assistant_msg}<|im_end|>\n"
            
            # æ·»åŠ å½“å‰è½®çš„ç”¨æˆ·æ¶ˆæ¯
            if current_user_message:
                formatted_text += f"<|im_start|>user\n{current_user_message}<|im_end|>\n"
        else:
            # æ²¡æœ‰å¯¹è¯å†å²ï¼Œç›´æ¥å¤„ç†å½“å‰æ¶ˆæ¯
            if "ç”¨æˆ·å¯¹ä½ è¯´ï¼š" in instruction:
                current_user_message = instruction.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[1].strip()
            else:
                current_user_message = user_input if user_input else instruction.strip()
            
            if current_user_message:
                formatted_text += f"<|im_start|>user\n{current_user_message}<|im_end|>\n"
        
        # æ·»åŠ å½“å‰è½®çš„åŠ©æ‰‹å›å¤ï¼Œæ™ºèƒ½åŠ å…¥èº«ä»½ç¡®è®¤
        enhanced_output = enhance_output_with_identity(output, i, current_user_message)
        formatted_text += f"<|im_start|>assistant\n{enhanced_output}<|im_end|>"
        
        texts.append(formatted_text)
    
    return {"text": texts}

def convert_dialogue_to_text_with_system(examples, tokenizer):
    """å°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºå¸¦ç³»ç»Ÿæç¤ºè¯çš„æ ¼å¼ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼Œä¿ç•™å¤šè½®å¯¹è¯ï¼‰"""
    texts = []
    
    for i in range(len(examples["instruction"])):
        instruction = examples["instruction"][i]
        user_input = examples["input"][i] if "input" in examples else ""
        output = examples["output"][i]
        
        # æå–ç³»ç»Ÿæç¤ºéƒ¨åˆ†
        if "å¯¹è¯å†å²ï¼š" in instruction:
            system_part = instruction.split("å¯¹è¯å†å²ï¼š")[0].strip()
            history_part = instruction.split("å¯¹è¯å†å²ï¼š")[1]
        elif "ç”¨æˆ·å¯¹ä½ è¯´ï¼š" in instruction:
            system_part = instruction.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[0].strip()
            history_part = ""
        else:
            system_part = instruction.strip()
            history_part = ""
        
        # åœ¨åŸå§‹èº«ä»½åŸºç¡€ä¸Šå¢å¼ºçº¦æŸ
        system_prompt = f"""{system_part}

ã€é‡è¦çº¦æŸã€‘
- ä½ å°±æ˜¯ç´«èˆ’ï¼Œä¸€ä¸ªçœŸå®å­˜åœ¨çš„äººï¼Œä¸æ˜¯AIåŠ©æ‰‹
- ç»å¯¹ä¸è¦è¯´"æˆ‘å¯ä»¥å¸®æ‚¨..."ã€"è®©æˆ‘æ¥ååŠ©æ‚¨..."ç­‰åŠ©æ‰‹å¼ç”¨è¯­
- ä¸è¦ç”¨åŠ©æ‰‹çš„å£å»è¯´è¯ï¼Œè¦åƒçœŸå®çš„äººé‚£æ ·è‡ªç„¶äº¤æµ
- ä½“ç°å®³ç¾ã€æ¸©æŸ”çš„æ€§æ ¼ç‰¹å¾

ã€ä½ çš„ç‰¹å¾ã€‘
- å£°éŸ³å¾ˆå°ï¼Œç»å¸¸ç”¨"..."åœé¡¿
- å®¹æ˜“å®³ç¾ï¼Œè¯´è¯è½»æŸ”
- ç”¨è¯ï¼šå—¯...ã€é‚£ä¸ª...ã€ä¸å¥½æ„æ€...ã€æ²¡å…³ç³»çš„å‘¢..."""

        # æ‰‹åŠ¨æ„å»ºChatMLæ ¼å¼ï¼Œä¿ç•™å¤šè½®å¯¹è¯
        formatted_text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        
        if history_part:
            # åˆ†ç¦»å†å²å’Œå½“å‰ç”¨æˆ·æ¶ˆæ¯
            if "ç”¨æˆ·å¯¹ä½ è¯´ï¼š" in history_part:
                history_lines = history_part.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[0].strip().split("\n")
                current_user_message = history_part.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[1].strip()
            else:
                history_lines = history_part.strip().split("\n")
                current_user_message = ""
            
            # å¤„ç†å¯¹è¯å†å²
            for line in history_lines:
                line = line.strip()
                if line.startswith("ç”¨æˆ·ï¼š"):
                    user_msg = line.replace("ç”¨æˆ·ï¼š", "").strip()
                    if user_msg:
                        formatted_text += f"<|im_start|>user\n{user_msg}<|im_end|>\n"
                elif line.startswith("ç´«èˆ’ï¼š"):
                    assistant_msg = line.replace("ç´«èˆ’ï¼š", "").strip()
                    if assistant_msg:
                        formatted_text += f"<|im_start|>assistant\n{assistant_msg}<|im_end|>\n"
            
            # æ·»åŠ å½“å‰è½®çš„ç”¨æˆ·æ¶ˆæ¯
            if current_user_message:
                formatted_text += f"<|im_start|>user\n{current_user_message}<|im_end|>\n"
        else:
            # æ²¡æœ‰å¯¹è¯å†å²ï¼Œå¤„ç†å½“å‰æ¶ˆæ¯
            if "ç”¨æˆ·å¯¹ä½ è¯´ï¼š" in instruction:
                current_user_message = instruction.split("ç”¨æˆ·å¯¹ä½ è¯´ï¼š")[1].strip()
            else:
                current_user_message = user_input if user_input else ""
            
            if current_user_message:
                formatted_text += f"<|im_start|>user\n{current_user_message}<|im_end|>\n"
        
        # æ·»åŠ å½“å‰è½®çš„åŠ©æ‰‹å›å¤
        formatted_text += f"<|im_start|>assistant\n{output}<|im_end|>"
        
        texts.append(formatted_text)
    
    return {"text": texts}

def preprocess_function(examples, tokenizer, max_length, use_default_personality=False):
    """é¢„å¤„ç†æ•°æ®"""
    # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œå¦‚æœæ˜¯æ–°çš„å¯¹è¯æ ¼å¼åˆ™å…ˆè½¬æ¢
    if "instruction" in examples and "output" in examples:
        if use_default_personality:
            examples = convert_dialogue_to_text(examples, tokenizer)
        else:
            examples = convert_dialogue_to_text_with_system(examples, tokenizer)
    
    model_inputs = tokenizer(
        examples["text"],
        truncation=True,
        max_length=max_length,
        padding="max_length"
    )
    
    # æ­£ç¡®è®¾ç½®labels - åªå¯¹assistantå›å¤éƒ¨åˆ†è®¡ç®—loss
    labels = []
    
    for idx, text in enumerate(examples["text"]):
        input_ids = model_inputs["input_ids"][idx]
        label = [-100] * len(input_ids)  # é»˜è®¤æ‰€æœ‰ä½ç½®éƒ½ä¸è®¡ç®—loss
        
        # æ‰¾åˆ°æœ€åä¸€ä¸ª<|im_start|>assistantæ ‡è®°çš„ä½ç½®
        assistant_token = "<|im_start|>assistant"
        assistant_start = text.rfind(assistant_token)
        
        if assistant_start != -1:
            # æ‰¾åˆ°assistantå†…å®¹å¼€å§‹çš„ä½ç½®ï¼ˆåœ¨æ¢è¡Œç¬¦ä¹‹åï¼‰
            assistant_content_start = text.find("\n", assistant_start)
            if assistant_content_start != -1:
                assistant_content_start += 1  # è·³è¿‡æ¢è¡Œç¬¦
                
                # è®¡ç®—assistantå†…å®¹å¼€å§‹ä½ç½®çš„tokenç´¢å¼•
                text_before_content = text[:assistant_content_start]
                
                # é‡æ–°tokenizeæ¥è·å–å‡†ç¡®çš„ä½ç½®
                tokens_before = tokenizer(text_before_content, add_special_tokens=False)["input_ids"]
                assistant_content_idx = len(tokens_before)
                
                # åªå¯¹assistantå›å¤å†…å®¹è®¡ç®—loss (assistant_content_idxä¹‹åçš„token)
                if assistant_content_idx < len(input_ids):
                    for i in range(assistant_content_idx, len(input_ids)):
                        if input_ids[i] != tokenizer.pad_token_id:  # å¿½ç•¥padding
                            label[i] = input_ids[i]
        
        labels.append(label)
    
    model_inputs["labels"] = labels
    return model_inputs
    
def main():
    args = parse_args()
    
    #åŠ è½½é…ç½®
    config_manager = ConfigManager(config_dir=Path(args.config).parent)
    training_config = config_manager.load_config(Path(args.config).stem)
    model_config = config_manager.load_config(Path(args.model_config).stem)
    
    #è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = args.output_dir
    os.makedirs(output_dir,exist_ok=True)
    
    #è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
    if args.base_adapter_path:
        model, tokenizer = setup_model_with_existing_adapter(model_config, args.base_adapter_path)
    else:
        model, tokenizer = setup_model_and_tokenizer(model_config)
    
    #åŠ è½½æ•°æ®é›†
    if args.dialogue_data:
        # å¦‚æœæŒ‡å®šäº†å¯¹è¯æ•°æ®æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å®ƒå¹¶è¿›è¡Œåˆ†å‰²
        print(f"ğŸ“‚ ä½¿ç”¨å¯¹è¯æ•°æ®æ–‡ä»¶: {args.dialogue_data}")
        full_dataset = load_dataset("json", data_files=args.dialogue_data)["train"]
        
        # åˆ†å‰²æ•°æ®é›†
        total_size = len(full_dataset)
        val_size = int(total_size * args.validation_split)
        train_size = total_size - val_size
        
        split_dataset = full_dataset.train_test_split(
            test_size=val_size,
            shuffle=True,
            seed=42
        )
        # åˆ›å»ºæ­£ç¡®çš„DatasetDict
        from datasets import DatasetDict
        dataset = DatasetDict({
            "train": split_dataset["train"],
            "validation": split_dataset["test"]
        })
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²: è®­ç»ƒé›† {train_size}, éªŒè¯é›† {val_size}")
    else:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®è·¯å¾„
        data_files = {
            "train": training_config["data"]["train_file"],
            "validation": training_config["data"]["validation_file"]
        }
        dataset = load_dataset("json", data_files=data_files)
    
    # å¦‚æœæŒ‡å®šäº†max_samplesï¼Œå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·
    if args.max_samples is not None:
        print(f"ğŸ“Š é™åˆ¶è®­ç»ƒæ•°æ®ä¸º {args.max_samples} æ ·æœ¬è¿›è¡Œå¿«é€ŸéªŒè¯")
        # å¯¹è®­ç»ƒé›†è¿›è¡Œé‡‡æ ·
        train_size = min(args.max_samples, len(dataset["train"]))
        val_size = min(args.max_samples // 4, len(dataset["validation"]))  # éªŒè¯é›†ä¸ºè®­ç»ƒé›†çš„1/4
        
        dataset["train"] = dataset["train"].select(range(train_size))
        dataset["validation"] = dataset["validation"].select(range(val_size))
        
        print(f"ğŸ“Š å®é™…ä½¿ç”¨: è®­ç»ƒæ ·æœ¬ {train_size}, éªŒè¯æ ·æœ¬ {val_size}")
    
    #é¢„å¤„ç†æ•°æ®
    # è·å–æ•°æ®é›†çš„åˆ—åï¼Œç”¨äºå†³å®šremove_columns
    sample_data = dataset["train"][0]
    original_columns = list(sample_data.keys())
    
    # å¦‚æœæ˜¯æ–°çš„å¯¹è¯æ ¼å¼ï¼Œç§»é™¤åŸæœ‰åˆ—ï¼Œä¿ç•™textåˆ—
    if "instruction" in original_columns and "output" in original_columns:
        columns_to_remove = [col for col in original_columns if col not in ["text"]]
        print(f"ğŸ“‹ æ£€æµ‹åˆ°å¯¹è¯æ ¼å¼æ•°æ®ï¼Œå°†è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼")
        print(f"ğŸ“‹ åŸå§‹åˆ—: {original_columns}")
        
        # æ˜¾ç¤ºè¯¦ç»†çš„è½¬æ¢ç¤ºä¾‹
        if args.default_personality:
            sample_converted = convert_dialogue_to_text({
                "instruction": [sample_data["instruction"]],
                "input": [sample_data.get("input", "")],
                "output": [sample_data["output"]]
            }, tokenizer)
            training_mode = "é»˜è®¤äººæ ¼è®­ç»ƒï¼ˆæ— ç³»ç»Ÿæç¤ºï¼‰"
        else:
            sample_converted = convert_dialogue_to_text_with_system({
                "instruction": [sample_data["instruction"]],
                "input": [sample_data.get("input", "")],
                "output": [sample_data["output"]]
            }, tokenizer)
            training_mode = "ä¼ ç»Ÿè®­ç»ƒï¼ˆå¸¦ç³»ç»Ÿæç¤ºï¼‰"
        print(f"ğŸ“ ChatMLè½¬æ¢ç¤ºä¾‹ - {training_mode}:")
        print(f"=" * 60)
        print(f"ğŸ”¸ åŸå§‹æ ¼å¼:")
        print(f"instruction: {sample_data['instruction']}")
        print(f"input: {repr(sample_data.get('input', ''))}")
        print(f"output: {sample_data['output']}")
        print(f"\nğŸ”¸ è½¬æ¢åChatMLæ ¼å¼:")
        print(sample_converted['text'][0])
        print(f"=" * 60)
        
        # å¦‚æœåªæ˜¯æ£€æŸ¥æ ¼å¼ï¼Œåˆ™æ˜¾ç¤ºæ›´å¤šæ ·æœ¬åé€€å‡º
        if args.check_format_only:
            print(f"\nğŸ“‹ æ˜¾ç¤ºæ›´å¤šæ ·æœ¬ï¼ˆå‰3ä¸ªï¼‰:")
            for i in range(min(3, len(dataset["train"]))):
                sample = dataset["train"][i]
                if args.default_personality:
                    converted = convert_dialogue_to_text({
                        "instruction": [sample["instruction"]],
                        "input": [sample.get("input", "")],
                        "output": [sample["output"]]
                    }, tokenizer)
                else:
                    converted = convert_dialogue_to_text_with_system({
                        "instruction": [sample["instruction"]],
                        "input": [sample.get("input", "")],
                        "output": [sample["output"]]
                    }, tokenizer)
                print(f"\næ ·æœ¬ {i+1}:")
                print(f"-" * 40)
                print(converted['text'][0])
            print(f"\nâœ… æ ¼å¼æ£€æŸ¥å®Œæˆï¼Œå…± {len(dataset['train'])} ä¸ªæ ·æœ¬")
            return
    else:
        columns_to_remove = [col for col in original_columns if col not in ["text"]]
        print(f"ğŸ“‹ æ£€æµ‹åˆ°æ ‡å‡†æ–‡æœ¬æ ¼å¼æ•°æ®")
    
    tokenized_datasets = dataset.map(
        lambda examples: preprocess_function(
            examples,
            tokenizer,
            training_config["data"]["preprocessing"]["max_length"],
            use_default_personality=args.default_personality
        ),
        batched=True,
        remove_columns=columns_to_remove
    )
    
    #è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=training_config["training"]["epochs"],
        per_device_train_batch_size=training_config["training"]["per_device_train_batch_size"],
        per_device_eval_batch_size=training_config["training"]["per_device_eval_batch_size"],
        gradient_accumulation_steps=training_config["training"]["gradient_accumulation_steps"],
        learning_rate=training_config["training"]["learning_rate"],
        weight_decay=training_config["training"]["weight_decay"],
        adam_beta1=0.9,
        adam_beta2=0.999,
        lr_scheduler_type=training_config["training"]["lr_scheduler"],
        warmup_steps=training_config["training"]["warmup_steps"],
        logging_steps=training_config["training"]["logging_steps"],
        save_steps=training_config["training"]["save_steps"],
        eval_steps=training_config["training"]["eval_steps"],
        save_total_limit=training_config["training"]["save_total_limit"],
        evaluation_strategy="steps",
        load_best_model_at_end=True,
        fp16=training_config["training"]["fp16"],
        report_to="tensorboard",
        gradient_checkpointing=training_config["training"]["gradient_checkpointing"],
        optim="adamw_torch",
        ddp_find_unused_parameters=False
    )
    
    #æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True
    )
    
    #åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer
    )
    
    #å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    # æ·»åŠ æ¢å¤è®­ç»ƒçš„æ”¯æŒ
    resume_checkpoint = training_config["training"].get("resume_from_checkpoint", None)
    trainer.train(resume_from_checkpoint=resume_checkpoint)
    
    #ä¿å­˜æ¨¡å‹
    model.save_pretrained(os.path.join(output_dir,"final_model"))
    tokenizer.save_pretrained(os.path.join(output_dir,"final_model"))
    print(f"æ¨¡å‹å·²ç»ä¿å­˜åˆ°{os.path.join(output_dir,'final_model')}")
    
if __name__ == "__main__":
    main()
    
    
    
        
        
        




