import asyncio
import json
import time
import uuid
from typing import AsyncGenerator, List, Dict, Any, Optional, Union, Tuple
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, Request, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, ValidationError, field_validator
import logging
from logging import Logger

from zishu.api.dependencies import (
    get_logger,
    get_performance_monitor,
    get_thread_factory_from_deps,
    get_character_config,
    submit_task,
    get_task_result,
    get_adapter_manager,
)
from zishu.api.schemas.chat import (
    Message,
    MessageRole,
    MessageType,
    EmotionType,
    CharacterConfig,
    ChatModel,
)
from zishu.api.schemas.request import ChatCompletionRequest
# from zishu.training.train.inference import get_inference_engine  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶å¤±è´¥
from zishu.utils.cache_manager import ModelResponseCache


# è¯·æ±‚/å“åº”æ¨¡å‹
class ChatRequest(BaseModel):
    """å¯¹è¯å®Œæˆè¯·æ±‚(å…¼å®¹OpenAI)"""

    messages: List[Dict[str, str]] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    model: Optional[str] = Field(None, description="æ¨¡å‹ID")
    adapter: Optional[str] = Field(None, description="é€‚é…å™¨ID")
    character_id: Optional[str] = Field(None, description="è§’è‰²ID")
    system_prompt: Optional[str] = Field(None, description="ç³»ç»Ÿæç¤ºè¯ï¼ˆè§’è‰²è®¾å®šï¼‰")
    max_tokens: Optional[int] = Field(None, ge=1, le=8192, description="æœ€å¤§tokenæ•°é‡")
    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(0.9, ge=0.0, le=1.0, description="top-pé‡‡æ ·å‚æ•°")
    stop: Optional[Union[str, List[str]]] = Field(None, description="åœæ­¢ç”Ÿæˆæ¡ä»¶")
    stream: bool = Field(False, description="æ˜¯å¦æµå¼ç”Ÿæˆ")
    user: Optional[str] = Field(None, description="ç”¨æˆ·ID")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")

    @field_validator("messages")
    @classmethod
    def validate_messages(cls, v):
        """éªŒè¯æ¶ˆæ¯æ ¼å¼å’Œè§’è‰²"""
        valid_roles = {"user", "assistant", "system", "function"}
        for msg in v:
            if not isinstance(msg, dict):
                raise ValueError("æ¶ˆæ¯å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            if "role" not in msg:
                raise ValueError("æ¶ˆæ¯å¿…é¡»åŒ…å«roleå­—æ®µ")
            if "content" not in msg:
                raise ValueError("æ¶ˆæ¯å¿…é¡»åŒ…å«contentå­—æ®µ")
            if msg["role"] not in valid_roles:
                raise ValueError(f"æ— æ•ˆçš„æ¶ˆæ¯è§’è‰²: {msg['role']}, æœ‰æ•ˆè§’è‰²: {valid_roles}")
        return v


class StreamChatRequest(BaseModel):
    """æµå¼èŠå¤©è¯·æ±‚"""

    messages: str = Field(..., min_length=1, max_length=10000, description="å¯¹è¯æ¶ˆæ¯")
    model: Optional[str] = Field(None, description="æ¨¡å‹ID")
    adapter: Optional[str] = Field(None, description="é€‚é…å™¨ID")
    character_id: Optional[str] = Field(None, description="è§’è‰²ID")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    context: Optional[str] = Field(None, description="ä¸Šä¸‹æ–‡")


class EmotionChatRequest(BaseModel):
    """æƒ…ç»ªå¯¹è¯è¯·æ±‚"""

    messages: str = Field(..., min_length=1, max_length=10000, description="å¯¹è¯æ¶ˆæ¯")
    current_emotion: Optional[EmotionType] = Field(None, description="å½“å‰æƒ…ç»ª")
    character_id: Optional[str] = Field(None, description="è§’è‰²ID")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    emotion_intensity: Optional[float] = Field(0.5, ge=0.0, le=1.0, description="æƒ…ç»ªå¼ºåº¦")
    analyze_emotion: bool = Field(True, description="æ˜¯å¦åˆ†ææƒ…ç»ª")


class ChatChoice(BaseModel):
    """å¯¹è¯é€‰æ‹©"""

    index: int = Field(..., description="é€‰æ‹©ç´¢å¼•")
    message: Message = Field(..., description="æ¶ˆæ¯å†…å®¹")
    finish_reason: Optional[str] = Field(None, description="å®ŒæˆåŸå› ")


class ChatUsage(BaseModel):
    """tokenä½¿ç”¨ç»Ÿè®¡"""

    prompt_tokens: int = Field(..., description="æç¤ºtokenæ•°é‡")
    completion_tokens: int = Field(..., description="å®Œæˆtokenæ•°é‡")
    total_tokens: int = Field(..., description="æ€»tokenæ•°é‡")


class ChatCompletionResponse(BaseModel):
    """å¯¹è¯å®Œæˆå“åº”"""

    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex[:24]}")
    object: str = Field(default="chat.completion")
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str = Field(..., description="æ¨¡å‹ID")
    choices: List[ChatChoice] = Field(..., description="å“åº”é€‰æ‹©")
    usage: ChatUsage = Field(..., description="tokenä½¿ç”¨ç»Ÿè®¡")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")


class StreamChunk(BaseModel):
    """æµå¼å“åº”å—"""

    id: str = Field(...)
    object: str = Field(default="chat.completion.chunk")
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str = Field(...)
    choices: List[Dict[str, Any]] = Field(...)


class EmotionResponse(BaseModel):
    """æƒ…ç»ªå“åº”"""

    message: Message = Field(..., description="å›å¤æ¶ˆæ¯")
    user_emotion: Optional[EmotionType] = Field(None, description="åˆ†æçš„ç”¨æˆ·æƒ…ç»ª")
    response_emotion: Optional[EmotionType] = Field(None, description="å›å¤çš„æƒ…ç»ª")
    emotion_confidence: Optional[float] = Field(
        ..., ge=0.0, le=1.0, description="æƒ…ç»ªç½®ä¿¡åº¦"
    )
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")


class HistoryResponse(BaseModel):
    """å¯¹è¯å†å²å“åº”"""

    session_id: Optional[str] = Field(..., description="ä¼šè¯ID")
    messages: List[Message] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    total_count: int = Field(..., description="æ€»æ¶ˆæ¯æ•°é‡")
    character_id: Optional[CharacterConfig] = Field(None, description="è§’è‰²é…ç½®")


# è¾…åŠ©ç±»
class ChatSessionManager:
    """å¯¹è¯ä¼šè¯ç®¡ç†å™¨"""

    def __init__(self):
        self.sessions: Dict[str, List[Message]] = {}
        self.session_configs: Dict[str, Dict[str, Any]] = {}

    async def get_or_create_session(self, session_id: Optional[str] = None) -> str:
        """è·å–æˆ–åˆ›å»ºå¯¹è¯ä¼šè¯"""
        if not session_id:
            session_id = f"session_{uuid.uuid4().hex[:16]}"

        if session_id not in self.sessions:
            self.sessions[session_id] = []
            self.session_configs[session_id] = {
                "created_at": datetime.now(),
                "last_activity": datetime.now(),
            }

        return session_id

    async def add_message(self, session_id: str, message: Message) -> None:
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯ä¼šè¯"""
        if session_id not in self.sessions:
            await self.get_or_create_session(session_id)

        self.sessions[session_id].append(message)
        self.session_configs[session_id]["last_activity"] = datetime.now()

        # é™åˆ¶ä¼šè¯å†å²é•¿åº¦
        if len(self.sessions[session_id]) > 100:
            self.sessions[session_id] = self.sessions[session_id][-50:]

    async def get_messages(
        self, session_id: str, limit: Optional[int] = None
    ) -> List[Message]:
        """è·å–å¯¹è¯ä¼šè¯æ¶ˆæ¯"""
        if session_id not in self.sessions:
            return []

        messages = self.sessions[session_id]
        if limit:
            return messages[-limit:]
        return messages

    async def get_context_messages(
        self, session_id: str, max_context_length: int = 10
    ) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯ä¼šè¯ä¸Šä¸‹æ–‡æ¶ˆæ¯"""
        messages = await self.get_messages(session_id, max_context_length)
        return [{"role": msg.role.value, "content": msg.content} for msg in messages]


class EmotionAnalyzer:
    """æƒ…ç»ªåˆ†æå™¨"""

    # TODOï¼š ç»“åˆè®°å¿†é€‚é…å™¨æ¡†æ¶ï¼Œç»“åˆè®°å¿†è¿›è¡Œæƒ…ç»ªåˆ†æ
    def __init__(self):
        self.emotion_keywords = {
            EmotionType.HAPPY: ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "æ„‰å¿«", "å…´å¥‹"],
            EmotionType.SAD: ["éš¾è¿‡", "ä¼¤å¿ƒ", "æ²®ä¸§", "å¤±è½", "æŠ‘éƒ"],
            EmotionType.ANGRY: ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "çƒ¦èº"],
            EmotionType.SURPRISED: ["æƒŠè®¶", "éœ‡æƒŠ", "æ„å¤–"],
            EmotionType.FEARFUL: ["å®³æ€•", "ææƒ§", "æ‹…å¿ƒ", "ç„¦è™‘"],
            EmotionType.NEUTRAL: ["ä¸­æ€§", "å¹³é™", "å†·æ¼ ", "æ— åŠ¨äºè¡·", "æ— æ„Ÿ"],
            # å¯ä»¥æ‰©å±•æ›´å¤šæƒ…ç»ªå…³é”®è¯
        }

    async def analyze_emotion(self, text: str) -> Tuple[EmotionType, float]:
        """åˆ†ææƒ…ç»ª"""

        # TODO: å¯æ›¿æ¢ä¸ºæ›´å¤æ‚çš„æƒ…ç»ªåˆ†ææ¨¡å‹,ç»“åˆè§’è‰²é…ç½®å’Œç”¨æˆ·æƒ…ç»ªï¼Œç”Ÿæˆå›å¤æƒ…ç»ª
        emotion_scores = {}

        for emotion, keywords in self.emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                emotion_scores[emotion] = score / len(keywords)

        if emotion_scores:
            best_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
            return best_emotion[0], min(best_emotion[1], 1.0)

        return EmotionType.NEUTRAL, 0.5

    async def generate_response_emotion(
        self,
        user_emotion: EmotionType,
        character_config: Optional[CharacterConfig] = None,
    ) -> EmotionType:
        emotion_map = {
            EmotionType.HAPPY: EmotionType.HAPPY,
            EmotionType.SAD: EmotionType.CALM,
            EmotionType.ANGRY: EmotionType.CALM,
            EmotionType.FEARFUL: EmotionType.CALM,
            EmotionType.SURPRISED: EmotionType.CURIOUS,
            EmotionType.NEUTRAL: EmotionType.CALM,
        }

        return emotion_map.get(user_emotion, EmotionType.NEUTRAL)


# ä¾èµ–å‡½æ•°
async def get_inference_engine_dep():
    """è·å–æ¨ç†å¼•æ“ä¾èµ–"""
    try:
        from zishu.training.train.inference import get_inference_engine
        return get_inference_engine()
    except Exception as e:
        # æ¨ç†å¼•æ“ä¸å¯ç”¨æ—¶è¿”å›Noneï¼Œå…è®¸ä½¿ç”¨ç¬¬ä¸‰æ–¹é€‚é…å™¨
        import logging
        logging.getLogger(__name__).warning(f"æ¨ç†å¼•æ“ä¸å¯ç”¨: {e}")
        return None


async def get_session_manager() -> ChatSessionManager:
    """è·å–å¯¹è¯ä¼šè¯ç®¡ç†å™¨"""
    # è¿™é‡Œå¯ä»¥ä½¿ç”¨ä¾èµ–æ³¨å…¥æˆ–å•ä¾‹æ¨¡å¼
    if not hasattr(get_session_manager, "_instance"):
        get_session_manager._instance = ChatSessionManager()
    return get_session_manager._instance


async def get_emotion_analyzer() -> EmotionAnalyzer:
    """è·å–æƒ…ç»ªåˆ†æå™¨"""
    if not hasattr(get_emotion_analyzer, "_instance"):
        get_emotion_analyzer._instance = EmotionAnalyzer()
    return get_emotion_analyzer._instance


async def get_response_cache() -> ModelResponseCache:
    """è·å–å“åº”ç¼“å­˜"""
    if not hasattr(get_response_cache, "_instance"):
        get_response_cache._instance = ModelResponseCache(max_size=1000, ttl=3600)
    return get_response_cache._instance


# è·¯ç”±å®šä¹‰
router = APIRouter(
    prefix="/chat",
    tags=["chat"],
)


@router.post("/completions", response_model=ChatCompletionResponse)
async def chat_completions(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    inference_engine=Depends(get_inference_engine_dep),
    session_manager: ChatSessionManager = Depends(get_session_manager),
    response_cache: ModelResponseCache = Depends(get_response_cache),
    logger: Logger = Depends(get_logger),
    adapter_manager=Depends(get_adapter_manager),
):
    """
    å¯¹è¯å®Œæˆæ¥å£(å…¼å®¹OpenAI)
    æ”¯æŒå¤šè½®å¯¹è¯ã€è§’è‰²é…ç½®ã€é€‚é…å™¨åˆ‡æ¢
    """
    try:
        # ä¼šè¯ç®¡ç†
        session_id = await session_manager.get_or_create_session(request.session_id)

        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å², TODO: å¯ä»è®°å¿†é€‚é…å™¨è·å–æ¶ˆæ¯
        if request.messages:
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            messages = []
            for msg in request.messages:
                if isinstance(msg, dict):
                    messages.append(msg)
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå‡è®¾æ˜¯ç”¨æˆ·æ¶ˆæ¯
                    messages.append({"role": "user", "content": str(msg)})

        else:
            # ä»ä¼šè¯å†å²è·å–æ¶ˆæ¯
            context_messages = await session_manager.get_context_messages(session_id)
            messages = context_messages
        
        # ğŸ¯ å¦‚æœæœ‰è§’è‰²IDï¼Œæ·»åŠ è§’è‰²çš„system prompt
        if request.character_id:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯
            has_system_msg = any(msg.get("role") == "system" for msg in messages)
            
            if not has_system_msg and request.system_prompt:
                # åœ¨æ¶ˆæ¯åˆ—è¡¨å¼€å¤´æ’å…¥system prompt
                messages.insert(0, {
                    "role": "system",
                    "content": request.system_prompt
                })
                logger.info(f"å·²æ·»åŠ è§’è‰²system prompt: {request.system_prompt[:50]}...")
            elif not has_system_msg:
                logger.warning(f"è§’è‰² {request.character_id} æ²¡æœ‰æä¾›system_prompt")

        # cacheæ£€æŸ¥
        cache_key = f"chat:{hash(str(messages))}:{request.model}:{request.character_id}"
        try:
            cached_response = response_cache.get(cache_key)
            if cached_response and not request.stream:
                logger.info(f"Return cached response:session_id={session_id}")
                return json.loads(cached_response)
        except Exception as cache_error:
            logger.warning(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {cache_error}")

        # ç”Ÿæˆå“åº”
        if request.stream:
            return StreamingResponse(
                _stream_chat_response(
                    messages,
                    request,
                    session_id,
                    inference_engine,
                    logger,
                    session_manager,
                ),
                media_type="text/plain",
            )

        # éæµå¼å“åº”
        start_time = time.time()
        response_text = await _generate_chat_response(
            messages, request, inference_engine, logger, adapter_manager
        )

        # æ„å»ºå“åº”æ¶ˆæ¯
        assistant_message = Message(
            role=MessageRole.ASSISTANT,
            content=response_text,
            session_id=session_id,
            processing_time=time.time() - start_time,
        )

        # ä¿å­˜åˆ°ä¼šè¯å†å²
        if messages:
            user_message = Message(
                role=MessageRole.USER,
                content=messages[-1].get("content", ""),
                session_id=session_id,
            )
            await session_manager.add_message(session_id, user_message)

        await session_manager.add_message(session_id, assistant_message)

        # æ„å»ºå®Œæ•´å“åº”
        response = ChatCompletionResponse(
            model=request.model or "default",
            choices=[
                ChatChoice(index=0, message=assistant_message, finish_reason="stop")
            ],
            usage=ChatUsage(
                prompt_tokens=len(
                    " ".join([msg.get("content", "") for msg in messages])
                ),
                completion_tokens=len(response_text),
                total_tokens=len("".join(msg.get("content", "") for msg in messages))
                + len(response_text),
            ),
            session_id=session_id,
        )

        # cacheå“åº”
        try:
            background_tasks.add_task(
                response_cache.set, cache_key, response.model_dump_json()
            )
        except Exception as cache_error:
            logger.warning(f"ç¼“å­˜è®¾ç½®å¤±è´¥: {cache_error}")

        logger.info(
            f"Chat completion response:session_id={session_id} time={time.time() - start_time:.2f}s"
        )
        return response

    except ValidationError as e:
        logger.error(f"Request verification failed: {str(e)}")
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"Dialog processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Dialog processing failed")


@router.post("/stream", response_class=StreamingResponse)
async def stream_chat(
    request: StreamChatRequest,
    inference_engine=Depends(get_inference_engine_dep),
    session_manager: ChatSessionManager = Depends(get_session_manager),
    logger: logging.Logger = Depends(get_logger),
):
    """
    æµå¼å¯¹è¯æ¥å£
    å®æ—¶è¿”å›ç”Ÿæˆå†…å®¹
    """
    try:
        session_id = await session_manager.get_or_create_session(request.session_id)

        # æ„å»ºæ¶ˆæ¯å†å²
        context_messages = await session_manager.get_context_messages(session_id, 10)
        messages = context_messages + [{"role": "user", "content": request.messages}]

        return StreamingResponse(
            _stream_chat_response_simple(
                messages, request, session_id, inference_engine, session_manager, logger
            ),
            media_type="text/event-stream",
        )

    except Exception as e:
        logger.error(f"Stream chat failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Stream chat failed")


@router.post("/emotion", response_model=EmotionResponse)
async def emotion_chat(
    request: EmotionChatRequest,
    inference_engine=Depends(get_inference_engine_dep),
    session_manager: ChatSessionManager = Depends(get_session_manager),
    emotion_analyzer: EmotionAnalyzer = Depends(get_emotion_analyzer),
    character_config: CharacterConfig = Depends(get_character_config),
    logger: logging.Logger = Depends(get_logger),
):
    """
    å¸¦æƒ…ç»ªçš„å¯¹è¯æ¥å£
    æ ¹æ®ç”¨æˆ·æƒ…ç»ªç”Ÿæˆå›å¤æƒ…ç»ª
    """
    start_time = time.time()

    try:
        session_id = await session_manager.get_or_create_session(request.session_id)

        # åˆ†æç”¨æˆ·æƒ…ç»ª
        user_emotion = request.current_emotion
        emotion_confidence = 1.0

        if request.analyze_emotion:
            user_emotion, emotion_confidence = await emotion_analyzer.analyze_emotion(
                request.messages
            )

        # ç”Ÿæˆå›åº”æƒ…ç»ª
        response_emotion = await emotion_analyzer.generate_response_emotion(
            user_emotion, character_config
        )

        # æ„å»ºå¸¦æƒ…ç»ªçš„æ¶ˆæ¯å†å²
        context_messages = await session_manager.get_context_messages(session_id, 5)
        messages = context_messages + [{"role": "user", "content": request.messages}]

        # ç”Ÿæˆå›åº”(å¯ä»¥å°†æƒ…ç»ªæ¶ˆæ¯åŠ å…¥åˆ°ç”Ÿæˆå‚æ•°ä¸­)
        response_text = await _generate_emotion_response(
            messages, request, user_emotion, response_emotion, inference_engine, logger
        )

        # æ„å»ºå“åº”æ¶ˆæ¯
        assistant_message = Message(
            role=MessageRole.ASSISTANT,
            content=response_text,
            session_id=session_id,
            emotion=response_emotion,
            emotion_intensity=request.emotion_intensity,
            processing_time=time.time() - start_time,
        )

        # ä¿å­˜åˆ°ä¼šè¯å†å²
        user_message = Message(
            role=MessageRole.USER,
            content=request.messages,
            session_id=session_id,
            emotion=user_emotion,
            emotion_intensity=emotion_confidence,
            processing_time=time.time() - start_time,
        )

        await session_manager.add_message(session_id, user_message)
        await session_manager.add_message(session_id, assistant_message)

        # æ„å»ºå“åº”
        response = EmotionResponse(
            message=assistant_message,
            user_emotion=user_emotion,
            response_emotion=response_emotion,
            emotion_confidence=emotion_confidence,
            session_id=session_id,
        )
        logger.info(
            f"Emotion chat response:session_id={session_id} user_emotion={user_emotion} response_emotion={response_emotion}"
        )
        return response

    except Exception as e:
        logger.error(f"Emotion chat failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Emotion chat failed")


@router.get("/history/{session_id}", response_model=HistoryResponse)
async def get_chat_history(
    session_id: str,
    limit: Optional[int] = None,
    session_manager: ChatSessionManager = Depends(get_session_manager),
    character_config: CharacterConfig = Depends(get_character_config),
    logger: logging.Logger = Depends(get_logger),
):
    """
    è·å–å¯¹è¯å†å²è®°å½•
    æ”¯æŒåˆ†é¡µå’Œæ•°é‡é™åˆ¶
    """
    try:
        messages = await session_manager.get_messages(session_id, limit)

        response = HistoryResponse(
            session_id=session_id,
            messages=messages,
            total_count=len(messages),
            character_config=character_config,
        )
        logger.info(
            f"Get chat history:session_id={session_id} total_count={len(messages)}"
        )
        return response

    except Exception as e:
        logger.error(f"Failed to get chat history: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get chat history")


@router.delete("/history/{session_id}")
async def clear_chat_history(
    session_id: str,
    session_manager: ChatSessionManager = Depends(get_session_manager),
    logger: logging.Logger = Depends(get_logger),
):
    """
    æ¸…é™¤æŒ‡å®šå¯¹è¯å†å²è®°å½•
    """
    try:
        if session_id in session_manager.sessions:
            del session_manager.sessions[session_id]
            del session_manager.session_configs[session_id]

        logger.info(f"Clear chat history:session_id={session_id}")
        return {
            "message": "Chat history cleared successfully",
            "session_id": session_id,
        }

    except Exception as e:
        logger.error(f"Failed to clear chat history: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to clear chat history")


# è¾…åŠ©å‡½æ•°
async def _generate_chat_response(
    messages: List[Dict[str, str]],
    request: ChatCompletionRequest,
    inference_engine,
    logger: Logger,
    adapter_manager=None,
) -> str:
    """ç”Ÿæˆæ™®é€šå¯¹è¯å“åº”"""
    try:
        # å¦‚æœæŒ‡å®šäº†é€‚é…å™¨ï¼Œä½¿ç”¨é€‚é…å™¨è¿›è¡Œå“åº”
        if request.adapter and adapter_manager:
            try:
                adapter = adapter_manager._adapters.get(request.adapter)
                
                # å¦‚æœé€‚é…å™¨ä¸åœ¨è¿è¡Œåˆ—è¡¨ä¸­ï¼Œå°è¯•å¯åŠ¨å®ƒ
                if not adapter:
                    logger.info(f"é€‚é…å™¨ {request.adapter} æœªåœ¨è¿è¡Œåˆ—è¡¨ä¸­ï¼Œå°è¯•å¯åŠ¨...")
                    try:
                        start_success = await adapter_manager.start_adapter(request.adapter)
                        if start_success:
                            adapter = adapter_manager._adapters.get(request.adapter)
                            logger.info(f"é€‚é…å™¨ {request.adapter} å¯åŠ¨æˆåŠŸ")
                        else:
                            logger.warning(f"é€‚é…å™¨ {request.adapter} å¯åŠ¨å¤±è´¥")
                    except Exception as start_error:
                        logger.error(f"å¯åŠ¨é€‚é…å™¨å¤±è´¥: {start_error}")
                
                if adapter:
                    logger.info(f"ä½¿ç”¨é€‚é…å™¨ç”Ÿæˆå“åº”: {request.adapter}")
                    from zishu.adapters.base import ExecutionContext
                    from zishu.adapters.soft import SoftAdapterRequest, SoftAdapterMode
                    
                    # æ„å»ºè½¯é€‚é…å™¨è¯·æ±‚
                    soft_request = SoftAdapterRequest(
                        query="",  # queryåœ¨messagesä¸­
                        mode=SoftAdapterMode.CONVERSATION,
                        context={"messages": messages},
                        temperature=request.temperature,
                        max_tokens=request.max_tokens,
                    )
                    
                    exec_context = ExecutionContext(
                        request_id=f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        user_id=request.user or "default_user",
                    )
                    
                    result = await adapter.process(soft_request, exec_context)
                    
                    # æ£€æŸ¥æ‰§è¡ŒçŠ¶æ€
                    if hasattr(result, 'status') and result.status != 'success':
                        error_msg = result.error or "æœªçŸ¥é”™è¯¯"
                        logger.error(f"é€‚é…å™¨æ‰§è¡Œå¤±è´¥: {error_msg}")
                        raise Exception(f"é€‚é…å™¨æ‰§è¡Œå¤±è´¥: {error_msg}")
                    
                    # ExecutionResult ä½¿ç”¨ output å±æ€§ï¼Œè€Œä¸æ˜¯ content
                    if hasattr(result, 'output') and result.output:
                        output = result.output
                        
                        # å¦‚æœ output æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
                        if isinstance(output, str):
                            return output.strip()
                        
                        # å¦‚æœ output æœ‰ content å±æ€§ï¼ˆå¦‚ SoftAdapterResponseï¼‰
                        if hasattr(output, 'content'):
                            return str(output.content).strip()
                        
                        # å¦‚æœ output æ˜¯å­—å…¸ï¼Œå°è¯•è·å– content
                        if isinstance(output, dict):
                            content = output.get('content', output.get('text', ''))
                            if content:
                                return str(content).strip()
                            return str(output).strip()
                        
                        # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        logger.warning(f"æœªçŸ¥çš„ output ç±»å‹: {type(output)}, å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                        return str(output).strip()
                    
                    # å…¼å®¹æ—§ç‰ˆæœ¬çš„ content å±æ€§
                    elif hasattr(result, 'content'):
                        return result.content.strip()
                    else:
                        logger.error(f"é€‚é…å™¨è¿”å›ç»“æœæ ¼å¼å¼‚å¸¸: {type(result)}, status={getattr(result, 'status', 'unknown')}")
                        return "æŠ±æ­‰ï¼Œé€‚é…å™¨è¿”å›ç»“æœæ ¼å¼å¼‚å¸¸"
                else:
                    logger.warning(f"é€‚é…å™¨æœªæ‰¾åˆ°æˆ–æ— æ³•å¯åŠ¨: {request.adapter}ï¼Œä½¿ç”¨é»˜è®¤æ¨ç†å¼•æ“")
            except Exception as e:
                logger.error(f"é€‚é…å™¨è°ƒç”¨å¤±è´¥: {e}ï¼Œå›é€€åˆ°é»˜è®¤æ¨ç†å¼•æ“")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # ä½¿ç”¨é»˜è®¤æ¨ç†å¼•æ“
        if inference_engine is None:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¨ç†å¼•æ“ä¸”æœªæŒ‡å®šé€‚é…å™¨")
            return "æŠ±æ­‰ï¼Œå½“å‰æœªé…ç½®æ¨ç†å¼•æ“ã€‚è¯·åœ¨è§’è‰²æ¨¡æ¿ä¸­é…ç½® API é€‚é…å™¨ï¼ˆå¦‚ OpenAIã€Claude ç­‰ï¼‰æ¥ä½¿ç”¨èŠå¤©åŠŸèƒ½ã€‚"
        
        response = inference_engine.chat_generate(
            messages=messages,
            model_id=request.model,
            character_id=request.character_id,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stop=request.stop,
        )

        return response.strip()

    except Exception as e:
        logger.error(f"Failed to generate chat response: {e}")
        raise


async def _generate_emotion_response(
    messages: List[Dict[str, str]],
    request: EmotionChatRequest,
    user_emotion: EmotionType,
    response_emotion: EmotionType,
    inference_engine,
    logger: Logger,
) -> str:
    """ç”Ÿæˆå¸¦æƒ…ç»ªçš„å¯¹è¯å“åº”"""
    try:
        # åœ¨ç³»ç»Ÿæç¤ºä¸­åŠ å…¥æƒ…ç»ªæ¶ˆæ¯
        emotion_prompt = f"ç”¨æˆ·å½“å‰æƒ…ç»ª: {user_emotion}, å›å¤æƒ…ç»ª: {response_emotion}"

        # ä¿®æ”¹æ¶ˆæ¯å†å²ï¼ŒåŠ å…¥æƒ…ç»ªæ¶ˆæ¯
        enhanced_messages = messages.copy()
        if enhanced_messages and enhanced_messages[0]["role"] == "system":
            enhanced_messages[0]["content"] += f"\n{emotion_prompt}"

        else:
            enhanced_messages.insert(0, {"role": "system", "content": emotion_prompt})

        response = inference_engine.chat_generate(
            messages=enhanced_messages,
            character_id=request.character_id,
            temperature=0.8,  # æƒ…ç»ªå¯¹è¯ä½¿ç”¨è¾ƒé«˜æ¸©åº¦
        )
        return response.strip()

    except Exception as e:
        logger.error(f"Failed to generate emotion response: {e}")
        raise


async def _stream_chat_response_simple(
    messages: List[Dict[str, str]],
    request: StreamChatRequest,
    session_id: str,
    inference_engine,
    session_manager: ChatSessionManager,
    logger: Logger,
) -> AsyncGenerator[str, None]:
    """æµå¼å¯¹è¯å“åº”ç”Ÿæˆå™¨"""
    try:
        # TODO: ç›®å‰ç›®å‰çš„æ¨ç†å¼•æ“ä¸æ”¯æŒæµå¼ï¼Œè¿™é‡Œæä¾›æ¡†æ¶ç»“æ„ï¼Œéœ€è¦å®ç°æµå¼ç”Ÿæˆé€»è¾‘

        chunk_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"

        # æ¨¡æ‹Ÿæµå¼å“åº”(å®é™…åº”è¯¥è°ƒç”¨æ¨ç†å¼•æ“çš„æµå¼æ¥å£)
        # åˆ›å»ºä¸´æ—¶requestå¯¹è±¡ç”¨äºè°ƒç”¨_generate_chat_response
        temp_request = ChatCompletionRequest(
            messages=[
                {"role": msg["role"], "content": msg["content"]} for msg in messages
            ],
            model=request.model,
            character_id=request.character_id,
        )
        response_text = await _generate_chat_response(
            messages, temp_request, inference_engine, logger
        )

        for i, char in enumerate(response_text):
            chunk_data = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": request.model or "default",
                "choices": [
                    {
                        "index": 0,
                        "delta": {"content": char},
                        "finish_reason": None if i < len(response_text) - 1 else "stop",
                    }
                ],
            }
            yield f"data: {json.dumps(chunk_data)}\n\n"
            await asyncio.sleep(0.01)

        # å‘é€ç»“æŸå—
        final_chunk_data = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": request.model or "default",
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
        }
        yield f"data: {json.dumps(final_chunk_data)}\n\n"
    except Exception as e:
        logger.error(f"Failed to stream chat response: {e}")
        error_chunk = {
            "error": {
                "message": str(e),
                "type": "stream_error",
            }
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"


async def _stream_chat_response(
    messages: List[Dict[str, str]],
    request: ChatCompletionRequest,
    session_id: str,
    inference_engine,
    logger: Logger,
    session_manager: ChatSessionManager,
) -> AsyncGenerator[str, None]:
    """ç®€åŒ–çš„æµå¼å¯¹è¯å“åº”ç”Ÿæˆå™¨"""
    try:
        response_text = inference_engine.chat_generate(
            messages=messages,
            model_id=request.model,
            character_id=request.character_id,
        )
        # é€å­—ç¬¦æµå¼è¾“å‡º
        for char in response_text:
            yield f"data: {json.dumps({'delta': {'content': char}})}\n\n"
            await asyncio.sleep(0.01)

        yield f"data: {json.dumps({'done': True})}\n\n"

    except Exception as e:
        logger.error(f"Failed to simplified stream chat response: {e}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"
