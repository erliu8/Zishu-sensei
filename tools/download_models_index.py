#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½Index-1.9B Characteræ¨¡å‹ï¼ˆæ”¯æŒModelScopeå’ŒHuggingFaceï¼‰

ä½¿ç”¨æ–¹æ³•:
    # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆä¾èµ–åœ¨ /data/disk/zishu-sensei/venvï¼‰
    source /data/disk/zishu-sensei/venv/bin/activate
    python tools/download_models_index.py
    
    æˆ–è€…ç›´æ¥ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒçš„Python:
    /data/disk/zishu-sensei/venv/bin/python tools/download_models_index.py
"""
import os
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# è®¾ç½®HuggingFaceé•œåƒï¼ˆå¦‚æœç½‘ç»œæ— æ³•è®¿é—®å®˜æ–¹ç«™ç‚¹ï¼‰
if "HF_ENDPOINT" not in os.environ:
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    print("â„¹ï¸  ä½¿ç”¨HuggingFaceé•œåƒ: https://hf-mirror.com")

# æ¨¡å‹é…ç½® - å°è¯•å¤šä¸ªå¯èƒ½çš„æ¨¡å‹ID
# æ­£ç¡®çš„æ¨¡å‹ID: IndexTeam/Index-1.9B-Character (æ¥è‡ª https://huggingface.co/IndexTeam/Index-1.9B-Character)
modelscope_model_id = "IndexTeam/Index-1.9B-Character"
huggingface_model_ids = [
    "IndexTeam/Index-1.9B-Character",  # æ­£ç¡®çš„æ¨¡å‹ID
    "bilibili/Index-1.9B-character",   # å¤‡ç”¨IDï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
    "Index-1.9B-character",            # å¤‡ç”¨IDï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
    "Index-1.9B",                      # å¤‡ç”¨IDï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
]
local_model_path = "/data/models/Index-1.9B-character"

# åˆ›å»ºç›®å½•
os.makedirs(local_model_path, exist_ok=True)

print(f"æ­£åœ¨ä¸‹è½½Index-1.9B Characteræ¨¡å‹åˆ° {local_model_path}")

# é¦–å…ˆå°è¯•ä»ModelScopeä¸‹è½½
model_dir = None
try:
    from modelscope import snapshot_download
    print(f"å°è¯•ä»ModelScopeä¸‹è½½ï¼Œæ¨¡å‹ID: {modelscope_model_id}")
    model_dir = snapshot_download(
        model_id=modelscope_model_id,
        cache_dir=local_model_path,
        revision="master"
    )
    print(f"âœ… ä»ModelScopeä¸‹è½½æˆåŠŸ: {model_dir}")
except ImportError:
    print("âš ï¸  ModelScopeæœªå®‰è£…ï¼Œå°†å°è¯•ä»HuggingFaceä¸‹è½½")
except Exception as e:
    print(f"âš ï¸  ModelScopeä¸‹è½½å¤±è´¥: {e}")
    print("å°†å°è¯•ä»HuggingFaceä¸‹è½½...")

# å¦‚æœModelScopeå¤±è´¥ï¼Œå°è¯•ä»HuggingFaceä¸‹è½½
if model_dir is None or not os.path.exists(model_dir):
    print("\nå°è¯•ä»HuggingFaceä¸‹è½½...")
    from huggingface_hub import snapshot_download as hf_snapshot_download
    from huggingface_hub import HfApi
    
    # è·å–é•œåƒç«¯ç‚¹
    hf_endpoint = os.environ.get("HF_ENDPOINT", "https://huggingface.co")
    print(f"ä½¿ç”¨ç«¯ç‚¹: {hf_endpoint}")
    
    # å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…ï¼‰
    api = HfApi(endpoint=hf_endpoint)
    valid_model_id = None
    
    print("æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨...")
    for hf_model_id in huggingface_model_ids:
        try:
            print(f"  æ£€æŸ¥æ¨¡å‹ID: {hf_model_id}")
            model_info = api.model_info(hf_model_id, timeout=15)
            print(f"  âœ… æ‰¾åˆ°æ¨¡å‹: {hf_model_id}")
            print(f"     æ–‡ä»¶æ•°é‡: {len(model_info.siblings)}")
            valid_model_id = hf_model_id
            break
        except Exception as check_e:
            error_msg = str(check_e)
            if "404" in error_msg or "not found" in error_msg.lower() or "does not exist" in error_msg.lower():
                print(f"  âš ï¸  æ¨¡å‹ID {hf_model_id} ä¸å­˜åœ¨")
            elif "Network" in error_msg or "unreachable" in error_msg or "Connection" in error_msg:
                print(f"  âš ï¸  ç½‘ç»œè¿æ¥å¤±è´¥: {error_msg[:100]}")
                print(f"  ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨ä»£ç†")
            else:
                print(f"  âš ï¸  æ£€æŸ¥å¤±è´¥: {error_msg[:100]}")
            continue
    
    # å¦‚æœæ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹ï¼Œå¼€å§‹ä¸‹è½½
    if valid_model_id:
        try:
            print(f"\nå¼€å§‹ä¸‹è½½æ¨¡å‹: {valid_model_id}")
            print("   è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            # æ¸…ç†ä¹‹å‰å¤±è´¥çš„ä¸‹è½½
            import shutil
            if os.path.exists(local_model_path) and os.path.isdir(local_model_path):
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…æ–‡ä»¶ï¼ˆæ’é™¤é”æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶ï¼‰
                files = [f for f in os.listdir(local_model_path) 
                        if not f.startswith('.') and os.path.isfile(os.path.join(local_model_path, f))]
                if not files:
                    print("   æ¸…ç†ä¹‹å‰å¤±è´¥çš„ä¸‹è½½...")
                    try:
                        shutil.rmtree(local_model_path)
                        os.makedirs(local_model_path, exist_ok=True)
                    except:
                        pass
            
            # ä½¿ç”¨huggingface_hubä¸‹è½½å®Œæ•´æ¨¡å‹
            model_dir = hf_snapshot_download(
                repo_id=valid_model_id,
                local_dir=local_model_path,
                local_dir_use_symlinks=False,
                ignore_patterns=["*.safetensors.index.json", "*.bin.index.json"],  # å¿½ç•¥ç´¢å¼•æ–‡ä»¶
                resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
                endpoint=hf_endpoint  # ä½¿ç”¨é•œåƒç«¯ç‚¹
            )
            
            # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
            if os.path.exists(model_dir):
                files = [f for f in os.listdir(model_dir) 
                        if not f.startswith('.') and os.path.isfile(os.path.join(model_dir, f))]
                if files:
                    print(f"âœ… ä»HuggingFaceä¸‹è½½æˆåŠŸ: {model_dir}")
                    print(f"   ä¸‹è½½äº† {len(files)} ä¸ªæ–‡ä»¶")
                else:
                    print(f"âš ï¸  ä¸‹è½½ç›®å½•å­˜åœ¨ä½†æ— æ–‡ä»¶: {model_dir}")
                    model_dir = None
            else:
                print(f"âš ï¸  ä¸‹è½½å¤±è´¥: ç›®å½•ä¸å­˜åœ¨")
                model_dir = None
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
            raise
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸  ä¸‹è½½å¤±è´¥: {error_msg[:200]}")
            if "Network" in error_msg or "unreachable" in error_msg or "Connection" in error_msg:
                print("ğŸ’¡ æç¤º: ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
                print("   1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                print("   2. æ˜¯å¦éœ€è¦ä½¿ç”¨ä»£ç†")
                print("   3. å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡: export HF_ENDPOINT=https://hf-mirror.com")
            model_dir = None
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹ID")
        model_dir = None
    
    if model_dir is None or not os.path.exists(model_dir):
        print("\nâŒ æ‰€æœ‰ä¸‹è½½æ–¹å¼éƒ½å¤±è´¥äº†ã€‚")
        print("ğŸ’¡ æç¤º:")
        print("   1. æ£€æŸ¥æ¨¡å‹IDæ˜¯å¦æ­£ç¡®")
        print("   2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   3. å¦‚æœæ¨¡å‹åœ¨HuggingFaceä¸Šï¼Œå¯èƒ½éœ€è¦ç™»å½•: huggingface-cli login")
        print("   4. å¦‚æœæ¨¡å‹åœ¨ModelScopeä¸Šï¼Œè¯·ç¡®è®¤æ¨¡å‹IDæ˜¯å¦æ­£ç¡®")
        print("   5. æ¨¡å‹å¯èƒ½å·²è¢«åˆ é™¤æˆ–é‡å‘½åï¼Œè¯·æ£€æŸ¥å®˜æ–¹æ–‡æ¡£")
        print(f"\nå°è¯•è¿‡çš„æ¨¡å‹ID:")
        print(f"   - ModelScope: {modelscope_model_id}")
        for hf_id in huggingface_model_ids:
            print(f"   - HuggingFace: {hf_id}")
        raise RuntimeError("æ— æ³•ä¸‹è½½æ¨¡å‹")

# ç¡®ä¿model_diræ˜¯æœ¬åœ°è·¯å¾„
if not os.path.isabs(model_dir) or not os.path.exists(model_dir):
    # å¦‚æœmodel_diræ˜¯æ¨¡å‹IDï¼Œä½¿ç”¨æœ¬åœ°è·¯å¾„
    if model_dir == local_model_path or os.path.exists(local_model_path):
        model_dir = local_model_path
    else:
        # å¦‚æœæœ¬åœ°è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨¡å‹IDï¼ˆè®©transformersè‡ªåŠ¨ä¸‹è½½ï¼‰
        print(f"âš ï¸  æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨æ¨¡å‹ID: {model_dir}")

# æµ‹è¯•æ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼Œå¦‚æœå†…å­˜è¶³å¤Ÿï¼‰
print("\nå¼€å§‹æµ‹è¯•æ¨¡å‹åŠ è½½...")
print(f"ä½¿ç”¨æ¨¡å‹è·¯å¾„: {model_dir}")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        model_dir, 
        trust_remote_code=True
    )
    print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
    
    # å°è¯•åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨4-bité‡åŒ–ä»¥èŠ‚çœå†…å­˜ï¼‰
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨4-bité‡åŒ–ï¼‰...")
    from transformers import BitsAndBytesConfig
    
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        quantization_config=quantization_config,
        device_map="auto",
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    print("âœ… Index-1.9B Characteræ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    # æµ‹è¯•è§’è‰²æ‰®æ¼”å¯¹è¯
    print("\nå¼€å§‹æµ‹è¯•è§’è‰²æ‰®æ¼”å¯¹è¯...")
    test_prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        **inputs, 
        max_new_tokens=100, 
        temperature=0.7, 
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("æ¨¡å‹å›å¤ï¼š")
    print(response)
    print("\nâœ… æ¨¡å‹æµ‹è¯•å®Œæˆ!")
    
except Exception as e:
    print(f"âš ï¸  æ¨¡å‹åŠ è½½/æµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯å†…å­˜ä¸è¶³ï¼‰: {e}")
    print("ğŸ’¡ æç¤º: æ¨¡å‹æ–‡ä»¶å·²æˆåŠŸä¸‹è½½ï¼Œå¯ä»¥åœ¨åç»­ä½¿ç”¨æ—¶å†åŠ è½½")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_dir}")

