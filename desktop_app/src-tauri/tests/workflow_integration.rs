//! ğŸ”„ å·¥ä½œæµæ¨¡å—é›†æˆæµ‹è¯•
//! 
//! æµ‹è¯•å·¥ä½œæµå¼•æ“ã€è°ƒåº¦å™¨ã€è§¦å‘å™¨ç­‰é›†æˆåŠŸèƒ½

use std::collections::HashMap;
use std::time::Duration;
use serde_json::Value;

#[cfg(test)]
mod workflow_engine_tests {
    use super::*;
    
    /// æµ‹è¯•å·¥ä½œæµå®Œæ•´æ‰§è¡Œç”Ÿå‘½å‘¨æœŸ
    #[tokio::test]
    async fn test_workflow_complete_execution_lifecycle() {
        // Arrange
        let engine = create_test_workflow_engine().await;
        let workflow_def = create_simple_workflow_definition();
        
        // Act - åˆ›å»ºå¹¶å¯åŠ¨å·¥ä½œæµ
        let workflow_id = engine.create_workflow(workflow_def).await.unwrap();
        let execution_result = engine.execute_workflow(&workflow_id, create_test_input()).await;
        
        // Assert
        assert!(execution_result.is_ok());
        let execution = execution_result.unwrap();
        assert_eq!(execution.status, WorkflowStatus::Completed);
        assert!(execution.output.is_some());
        
        // éªŒè¯æ‰§è¡Œå†å²
        let history = engine.get_execution_history(&workflow_id).await.unwrap();
        assert_eq!(history.len(), 1);
    }
    
    /// æµ‹è¯•å·¥ä½œæµæ­¥éª¤æ‰§è¡Œé¡ºåº
    #[tokio::test]
    async fn test_workflow_step_execution_order() {
        let engine = create_test_workflow_engine().await;
        let workflow_def = create_sequential_workflow_definition();
        
        let workflow_id = engine.create_workflow(workflow_def).await.unwrap();
        let execution = engine.execute_workflow(&workflow_id, create_test_input()).await.unwrap();
        
        // éªŒè¯æ­¥éª¤æ‰§è¡Œé¡ºåº
        assert_eq!(execution.steps.len(), 3);
        for (i, step) in execution.steps.iter().enumerate() {
            assert_eq!(step.order, i);
            assert_eq!(step.status, StepStatus::Completed);
        }
    }
    
    /// æµ‹è¯•å·¥ä½œæµå¹¶è¡Œæ­¥éª¤æ‰§è¡Œ
    #[tokio::test]
    async fn test_workflow_parallel_step_execution() {
        let engine = create_test_workflow_engine().await;
        let workflow_def = create_parallel_workflow_definition();
        
        let start_time = std::time::Instant::now();
        let workflow_id = engine.create_workflow(workflow_def).await.unwrap();
        let execution = engine.execute_workflow(&workflow_id, create_test_input()).await.unwrap();
        let execution_time = start_time.elapsed();
        
        // éªŒè¯å¹¶è¡Œæ‰§è¡Œæ•ˆç‡ï¼ˆå¹¶è¡Œæ‰§è¡Œåº”è¯¥æ¯”ä¸²è¡Œå¿«ï¼‰
        assert!(execution_time.as_millis() < 2000); // å‡è®¾å¹¶è¡Œæ­¥éª¤å„éœ€1ç§’
        assert_eq!(execution.status, WorkflowStatus::Completed);
        
        // éªŒè¯æ‰€æœ‰å¹¶è¡Œæ­¥éª¤éƒ½å·²å®Œæˆ
        let parallel_steps: Vec<_> = execution.steps.iter()
            .filter(|step| step.step_type == StepType::Parallel)
            .collect();
        assert_eq!(parallel_steps.len(), 2);
        for step in parallel_steps {
            assert_eq!(step.status, StepStatus::Completed);
        }
    }
    
    /// æµ‹è¯•å·¥ä½œæµé”™è¯¯å¤„ç†å’Œå›æ»š
    #[tokio::test]
    async fn test_workflow_error_handling_and_rollback() {
        let engine = create_test_workflow_engine().await;
        let workflow_def = create_workflow_with_error_step();
        
        let workflow_id = engine.create_workflow(workflow_def).await.unwrap();
        let execution_result = engine.execute_workflow(&workflow_id, create_test_input()).await;
        
        // å·¥ä½œæµåº”è¯¥å¤±è´¥
        assert!(execution_result.is_ok()); // æ‰§è¡Œæœ¬èº«ä¸æŠ¥é”™
        let execution = execution_result.unwrap();
        assert_eq!(execution.status, WorkflowStatus::Failed);
        
        // éªŒè¯å›æ»šæ“ä½œå·²æ‰§è¡Œ
        let rollback_steps: Vec<_> = execution.steps.iter()
            .filter(|step| step.step_type == StepType::Rollback)
            .collect();
        assert!(!rollback_steps.is_empty());
        
        for rollback_step in rollback_steps {
            assert_eq!(rollback_step.status, StepStatus::Completed);
        }
    }
}

#[cfg(test)]
mod workflow_scheduler_tests {
    use super::*;
    use std::time::Duration;
    
    /// æµ‹è¯•å·¥ä½œæµå®šæ—¶è°ƒåº¦
    #[tokio::test]
    async fn test_workflow_scheduled_execution() {
        let scheduler = create_test_workflow_scheduler().await;
        let workflow_def = create_simple_workflow_definition();
        
        // åˆ›å»ºå®šæ—¶ä»»åŠ¡ï¼ˆæ¯100msæ‰§è¡Œä¸€æ¬¡ï¼‰
        let schedule = CronSchedule::from_str("*/100 * * * * *").unwrap();
        let job_id = scheduler.schedule_workflow(workflow_def, schedule).await.unwrap();
        
        // ç­‰å¾…æ‰§è¡Œå‡ æ¬¡
        tokio::time::sleep(Duration::from_millis(350)).await;
        
        // æ£€æŸ¥æ‰§è¡Œå†å²
        let executions = scheduler.get_job_executions(&job_id).await.unwrap();
        assert!(executions.len() >= 3); // è‡³å°‘æ‰§è¡Œ3æ¬¡
        
        // åœæ­¢è°ƒåº¦ä»»åŠ¡
        scheduler.cancel_job(&job_id).await.unwrap();
    }
    
    /// æµ‹è¯•å·¥ä½œæµè§¦å‘å™¨æ‰§è¡Œ
    #[tokio::test]
    async fn test_workflow_trigger_execution() {
        let scheduler = create_test_workflow_scheduler().await;
        let workflow_def = create_simple_workflow_definition();
        
        // åˆ›å»ºäº‹ä»¶è§¦å‘å™¨
        let trigger = EventTrigger {
            event_type: "test_event".to_string(),
            condition: serde_json::json!({"key": "value"}),
        };
        
        let job_id = scheduler.create_triggered_workflow(workflow_def, trigger).await.unwrap();
        
        // è§¦å‘äº‹ä»¶
        scheduler.emit_event("test_event", serde_json::json!({"key": "value"})).await.unwrap();
        
        // ç­‰å¾…æ‰§è¡Œå®Œæˆ
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        // æ£€æŸ¥æ˜¯å¦æ‰§è¡Œ
        let executions = scheduler.get_job_executions(&job_id).await.unwrap();
        assert_eq!(executions.len(), 1);
        assert_eq!(executions[0].status, WorkflowStatus::Completed);
    }
    
    /// æµ‹è¯•å¹¶å‘å·¥ä½œæµæ‰§è¡Œé™åˆ¶
    #[tokio::test]
    async fn test_concurrent_workflow_execution_limits() {
        let scheduler = create_test_workflow_scheduler().await;
        scheduler.set_max_concurrent_executions(2).await;
        
        let workflow_def = create_long_running_workflow_definition();
        
        // åŒæ—¶å¯åŠ¨5ä¸ªå·¥ä½œæµ
        let mut job_ids = Vec::new();
        for _i in 0..5 {
            let job_id = scheduler.execute_workflow_immediately(workflow_def.clone()).await.unwrap();
            job_ids.push(job_id);
        }
        
        // çŸ­æš‚ç­‰å¾…
        tokio::time::sleep(Duration::from_millis(50)).await;
        
        // æ£€æŸ¥å¹¶å‘æ‰§è¡Œæ•°é‡
        let active_count = scheduler.get_active_execution_count().await;
        assert_eq!(active_count, 2); // åº”è¯¥åªæœ‰2ä¸ªåœ¨æ‰§è¡Œ
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        tokio::time::sleep(Duration::from_millis(500)).await;
        
        // æ£€æŸ¥æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ
        for job_id in job_ids {
            let executions = scheduler.get_job_executions(&job_id).await.unwrap();
            assert_eq!(executions.len(), 1);
            assert_eq!(executions[0].status, WorkflowStatus::Completed);
        }
    }
}

#[cfg(test)]
mod workflow_persistence_tests {
    use super::*;
    
    /// æµ‹è¯•å·¥ä½œæµå®šä¹‰æŒä¹…åŒ–
    #[tokio::test]
    async fn test_workflow_definition_persistence() {
        let engine = create_test_workflow_engine().await;
        let workflow_def = create_simple_workflow_definition();
        
        // ä¿å­˜å·¥ä½œæµå®šä¹‰
        let workflow_id = engine.create_workflow(workflow_def.clone()).await.unwrap();
        
        // é‡æ–°åŠ è½½å·¥ä½œæµå®šä¹‰
        let loaded_def = engine.get_workflow_definition(&workflow_id).await.unwrap();
        
        // éªŒè¯å®šä¹‰ä¸€è‡´
        assert_eq!(loaded_def.name, workflow_def.name);
        assert_eq!(loaded_def.steps.len(), workflow_def.steps.len());
    }
    
    /// æµ‹è¯•å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€æ¢å¤
    #[tokio::test]
    async fn test_workflow_execution_state_recovery() {
        let engine = create_test_workflow_engine().await;
        let workflow_def = create_interruptible_workflow_definition();
        
        let workflow_id = engine.create_workflow(workflow_def).await.unwrap();
        
        // å¯åŠ¨å·¥ä½œæµæ‰§è¡Œ
        let execution_handle = engine.start_workflow_async(&workflow_id, create_test_input()).await.unwrap();
        
        // æ¨¡æ‹Ÿä¸­æ–­ï¼ˆä¿å­˜çŠ¶æ€ï¼‰
        tokio::time::sleep(Duration::from_millis(50)).await;
        engine.pause_execution(&execution_handle.execution_id).await.unwrap();
        
        // æ¢å¤æ‰§è¡Œ
        let _resumed_execution = engine.resume_execution(&execution_handle.execution_id).await.unwrap();
        
        // ç­‰å¾…å®Œæˆ
        let final_result = engine.wait_for_completion(&execution_handle.execution_id).await.unwrap();
        assert_eq!(final_result.status, WorkflowStatus::Completed);
    }
}

// æµ‹è¯•è¾…åŠ©å‡½æ•°å’Œç±»å‹å®šä¹‰

async fn create_test_workflow_engine() -> WorkflowEngine {
    WorkflowEngine::new()
}

async fn create_test_workflow_scheduler() -> WorkflowScheduler {
    WorkflowScheduler::new()
}

fn create_simple_workflow_definition() -> WorkflowDefinition {
    WorkflowDefinition {
        id: "simple-workflow".to_string(),
        name: "ç®€å•å·¥ä½œæµ".to_string(),
        description: Some("æµ‹è¯•ç”¨ç®€å•å·¥ä½œæµ".to_string()),
        steps: vec![
            WorkflowStep {
                id: "step1".to_string(),
                name: "æ­¥éª¤1".to_string(),
                step_type: StepType::Action,
                action: "test_action".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            }
        ],
        variables: HashMap::new(),
    }
}

fn create_sequential_workflow_definition() -> WorkflowDefinition {
    WorkflowDefinition {
        id: "sequential-workflow".to_string(),
        name: "é¡ºåºå·¥ä½œæµ".to_string(),
        description: Some("æµ‹è¯•é¡ºåºæ‰§è¡Œçš„å·¥ä½œæµ".to_string()),
        steps: vec![
            WorkflowStep {
                id: "step1".to_string(),
                name: "æ­¥éª¤1".to_string(),
                step_type: StepType::Action,
                action: "action1".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            },
            WorkflowStep {
                id: "step2".to_string(),
                name: "æ­¥éª¤2".to_string(),
                step_type: StepType::Action,
                action: "action2".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec!["step1".to_string()],
            },
            WorkflowStep {
                id: "step3".to_string(),
                name: "æ­¥éª¤3".to_string(),
                step_type: StepType::Action,
                action: "action3".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec!["step2".to_string()],
            },
        ],
        variables: HashMap::new(),
    }
}

fn create_parallel_workflow_definition() -> WorkflowDefinition {
    WorkflowDefinition {
        id: "parallel-workflow".to_string(),
        name: "å¹¶è¡Œå·¥ä½œæµ".to_string(),
        description: Some("æµ‹è¯•å¹¶è¡Œæ‰§è¡Œçš„å·¥ä½œæµ".to_string()),
        steps: vec![
            WorkflowStep {
                id: "parallel1".to_string(),
                name: "å¹¶è¡Œæ­¥éª¤1".to_string(),
                step_type: StepType::Parallel,
                action: "parallel_action1".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            },
            WorkflowStep {
                id: "parallel2".to_string(),
                name: "å¹¶è¡Œæ­¥éª¤2".to_string(),
                step_type: StepType::Parallel,
                action: "parallel_action2".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            },
        ],
        variables: HashMap::new(),
    }
}

fn create_workflow_with_error_step() -> WorkflowDefinition {
    WorkflowDefinition {
        id: "error-workflow".to_string(),
        name: "é”™è¯¯å·¥ä½œæµ".to_string(),
        description: Some("åŒ…å«é”™è¯¯æ­¥éª¤çš„æµ‹è¯•å·¥ä½œæµ".to_string()),
        steps: vec![
            WorkflowStep {
                id: "normal_step".to_string(),
                name: "æ­£å¸¸æ­¥éª¤".to_string(),
                step_type: StepType::Action,
                action: "normal_action".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            },
            WorkflowStep {
                id: "error_step".to_string(),
                name: "é”™è¯¯æ­¥éª¤".to_string(),
                step_type: StepType::Action,
                action: "error_action".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec!["normal_step".to_string()],
            },
            WorkflowStep {
                id: "rollback_step".to_string(),
                name: "å›æ»šæ­¥éª¤".to_string(),
                step_type: StepType::Rollback,
                action: "rollback_action".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            },
        ],
        variables: HashMap::new(),
    }
}

fn create_long_running_workflow_definition() -> WorkflowDefinition {
    WorkflowDefinition {
        id: "long-running-workflow".to_string(),
        name: "é•¿æ—¶é—´è¿è¡Œå·¥ä½œæµ".to_string(),
        description: Some("æµ‹è¯•é•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œæµ".to_string()),
        steps: vec![
            WorkflowStep {
                id: "long_step".to_string(),
                name: "é•¿æ—¶é—´æ­¥éª¤".to_string(),
                step_type: StepType::Action,
                action: "sleep_action".to_string(),
                parameters: serde_json::json!({"duration": 200}), // 200ms
                dependencies: vec![],
            }
        ],
        variables: HashMap::new(),
    }
}

fn create_interruptible_workflow_definition() -> WorkflowDefinition {
    WorkflowDefinition {
        id: "interruptible-workflow".to_string(),
        name: "å¯ä¸­æ–­å·¥ä½œæµ".to_string(),
        description: Some("æµ‹è¯•å¯ä¸­æ–­å’Œæ¢å¤çš„å·¥ä½œæµ".to_string()),
        steps: vec![
            WorkflowStep {
                id: "checkpoint_step".to_string(),
                name: "æ£€æŸ¥ç‚¹æ­¥éª¤".to_string(),
                step_type: StepType::Action,
                action: "checkpoint_action".to_string(),
                parameters: serde_json::json!({}),
                dependencies: vec![],
            }
        ],
        variables: HashMap::new(),
    }
}

fn create_test_input() -> Value {
    serde_json::json!({"test": "input"})
}

// å ä½ç±»å‹å®šä¹‰

#[derive(Debug, Clone)]
struct WorkflowDefinition {
    id: String,
    name: String,
    description: Option<String>,
    steps: Vec<WorkflowStep>,
    variables: HashMap<String, Value>,
}

#[derive(Debug, Clone)]
struct WorkflowStep {
    id: String,
    name: String,
    step_type: StepType,
    action: String,
    parameters: Value,
    dependencies: Vec<String>,
}

#[derive(Debug, Clone, PartialEq)]
enum StepType {
    Action,
    Parallel,
    Rollback,
}

#[derive(Debug, Clone)]
struct WorkflowExecution {
    id: String,
    workflow_id: String,
    status: WorkflowStatus,
    steps: Vec<StepExecution>,
    output: Option<Value>,
}

#[derive(Debug, Clone)]
struct StepExecution {
    step_id: String,
    status: StepStatus,
    order: usize,
    step_type: StepType,
}

#[derive(Debug, Clone, PartialEq)]
enum WorkflowStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Paused,
}

#[derive(Debug, Clone, PartialEq)]
enum StepStatus {
    Pending,
    Running,
    Completed,
    Failed,
}

#[derive(Debug, Clone)]
struct ExecutionHandle {
    execution_id: String,
}

#[derive(Debug, Clone)]
struct EventTrigger {
    event_type: String,
    condition: Value,
}

#[derive(Debug)]
struct CronSchedule;

impl CronSchedule {
    fn from_str(_s: &str) -> Result<Self, String> {
        Ok(Self)
    }
}

// æ¨¡æ‹Ÿå®ç°
struct WorkflowEngine;

impl WorkflowEngine {
    fn new() -> Self {
        Self
    }
    
    async fn create_workflow(&self, _def: WorkflowDefinition) -> Result<String, WorkflowError> {
        Ok("workflow-123".to_string())
    }
    
    async fn execute_workflow(&self, _id: &str, _input: Value) -> Result<WorkflowExecution, WorkflowError> {
        Ok(WorkflowExecution {
            id: "execution-123".to_string(),
            workflow_id: "workflow-123".to_string(),
            status: WorkflowStatus::Completed,
            steps: vec![],
            output: Some(serde_json::json!({"result": "success"})),
        })
    }
    
    async fn get_execution_history(&self, _id: &str) -> Result<Vec<WorkflowExecution>, WorkflowError> {
        Ok(vec![])
    }
    
    async fn get_workflow_definition(&self, _id: &str) -> Result<WorkflowDefinition, WorkflowError> {
        Ok(create_simple_workflow_definition())
    }
    
    async fn start_workflow_async(&self, _id: &str, _input: Value) -> Result<ExecutionHandle, WorkflowError> {
        Ok(ExecutionHandle {
            execution_id: "async-execution-123".to_string(),
        })
    }
    
    async fn pause_execution(&self, _id: &str) -> Result<(), WorkflowError> {
        Ok(())
    }
    
    async fn resume_execution(&self, _id: &str) -> Result<WorkflowExecution, WorkflowError> {
        Ok(WorkflowExecution {
            id: "resumed-execution-123".to_string(),
            workflow_id: "workflow-123".to_string(),
            status: WorkflowStatus::Running,
            steps: vec![],
            output: None,
        })
    }
    
    async fn wait_for_completion(&self, _id: &str) -> Result<WorkflowExecution, WorkflowError> {
        Ok(WorkflowExecution {
            id: "completed-execution-123".to_string(),
            workflow_id: "workflow-123".to_string(),
            status: WorkflowStatus::Completed,
            steps: vec![],
            output: Some(serde_json::json!({"result": "success"})),
        })
    }
}

struct WorkflowScheduler;

impl WorkflowScheduler {
    fn new() -> Self {
        Self
    }
    
    async fn schedule_workflow(&self, _def: WorkflowDefinition, _schedule: CronSchedule) -> Result<String, WorkflowError> {
        Ok("job-123".to_string())
    }
    
    async fn get_job_executions(&self, _job_id: &str) -> Result<Vec<WorkflowExecution>, WorkflowError> {
        Ok(vec![
            WorkflowExecution {
                id: "exec-1".to_string(),
                workflow_id: "workflow-123".to_string(),
                status: WorkflowStatus::Completed,
                steps: vec![],
                output: None,
            };
            3
        ])
    }
    
    async fn cancel_job(&self, _job_id: &str) -> Result<(), WorkflowError> {
        Ok(())
    }
    
    async fn create_triggered_workflow(&self, _def: WorkflowDefinition, _trigger: EventTrigger) -> Result<String, WorkflowError> {
        Ok("triggered-job-123".to_string())
    }
    
    async fn emit_event(&self, _event_type: &str, _data: Value) -> Result<(), WorkflowError> {
        Ok(())
    }
    
    async fn set_max_concurrent_executions(&self, _max: usize) {
        // è®¾ç½®æœ€å¤§å¹¶å‘æ•°
    }
    
    async fn execute_workflow_immediately(&self, _def: WorkflowDefinition) -> Result<String, WorkflowError> {
        Ok("immediate-job-123".to_string())
    }
    
    async fn get_active_execution_count(&self) -> usize {
        2 // æ¨¡æ‹Ÿè¿”å›2ä¸ªæ´»è·ƒæ‰§è¡Œ
    }
}

#[derive(Debug)]
enum WorkflowError {
    NotFound,
    ExecutionFailed,
    InvalidDefinition,
}
