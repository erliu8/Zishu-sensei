/**
 * VoiceInput è¯­éŸ³è¾“å…¥ç»„ä»¶
 * 
 * é«˜çº§è¯­éŸ³è¾“å…¥ç»„ä»¶ï¼Œæ”¯æŒï¼š
 * - Web Speech API è¯­éŸ³è¯†åˆ«
 * - å®æ—¶éŸ³é¢‘å¯è§†åŒ–
 * - å¤šè¯­è¨€æ”¯æŒ
 * - é”™è¯¯å¤„ç†å’Œé‡è¯•
 * - æš‚åœ/ç»§ç»­å½•éŸ³
 * - ä¸­é—´ç»“æœæ˜¾ç¤º
 * - æ— éšœç¢æ”¯æŒ
 * - å®Œå–„çš„çŠ¶æ€ç®¡ç†
 * 
 * @module VoiceInput
 */

import React, { useState, useEffect, useRef, useCallback, useMemo } from 'react'
import styles from './VoiceInput.module.css'

// ==================== ç±»å‹å®šä¹‰ ====================

/**
 * è¯­éŸ³è¯†åˆ«çŠ¶æ€
 */
export enum VoiceStatus {
  /** ç©ºé—²çŠ¶æ€ */
  IDLE = 'idle',
  /** æ­£åœ¨ç›‘å¬ */
  LISTENING = 'listening',
  /** æš‚åœä¸­ */
  PAUSED = 'paused',
  /** å¤„ç†ä¸­ */
  PROCESSING = 'processing',
  /** é”™è¯¯çŠ¶æ€ */
  ERROR = 'error',
}

/**
 * è¯­éŸ³è¯†åˆ«é”™è¯¯ç±»å‹
 */
export enum VoiceErrorType {
  /** ä¸æ”¯æŒè¯­éŸ³è¯†åˆ« */
  NOT_SUPPORTED = 'not-supported',
  /** æœªæˆäºˆéº¦å…‹é£æƒé™ */
  NO_PERMISSION = 'no-permission',
  /** æœªæ£€æµ‹åˆ°è¯­éŸ³ */
  NO_SPEECH = 'no-speech',
  /** ç½‘ç»œé”™è¯¯ */
  NETWORK = 'network',
  /** éŸ³é¢‘æ•è·é”™è¯¯ */
  AUDIO_CAPTURE = 'audio-capture',
  /** æœåŠ¡ä¸å¯ç”¨ */
  SERVICE_NOT_ALLOWED = 'service-not-allowed',
  /** æœªçŸ¥é”™è¯¯ */
  UNKNOWN = 'unknown',
}

/**
 * è¯­éŸ³è¯†åˆ«ç»“æœ
 */
export interface VoiceResult {
  /** è¯†åˆ«æ–‡æœ¬ */
  text: string
  /** æ˜¯å¦ä¸ºæœ€ç»ˆç»“æœ */
  isFinal: boolean
  /** ç½®ä¿¡åº¦ (0-1) */
  confidence?: number
  /** æ—¶é—´æˆ³ */
  timestamp: number
}

/**
 * è¯­è¨€é…ç½®
 */
export interface LanguageOption {
  /** è¯­è¨€ä»£ç  */
  code: string
  /** è¯­è¨€åç§° */
  name: string
  /** åŒºåŸŸæ ‡è¯† */
  region?: string
}

/**
 * VoiceInput ç»„ä»¶å±æ€§
 */
export interface VoiceInputProps {
  /** æ˜¯å¦è‡ªåŠ¨å¼€å§‹å½•éŸ³ */
  autoStart?: boolean
  /** é»˜è®¤è¯­è¨€ */
  defaultLanguage?: string
  /** æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ */
  supportedLanguages?: LanguageOption[]
  /** æ˜¯å¦æ˜¾ç¤ºä¸­é—´ç»“æœ */
  showInterimResults?: boolean
  /** æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œ0 è¡¨ç¤ºæ— é™åˆ¶ */
  maxDuration?: number
  /** æ˜¯å¦è¿ç»­è¯†åˆ« */
  continuous?: boolean
  /** æ˜¯å¦æ˜¾ç¤ºéŸ³é¢‘å¯è§†åŒ– */
  showVisualizer?: boolean
  /** æ˜¯å¦ç¦ç”¨ */
  disabled?: boolean
  /** è‡ªå®šä¹‰ç±»å */
  className?: string
  /** è‡ªå®šä¹‰æ ·å¼ */
  style?: React.CSSProperties
  /** è‡ªå®šä¹‰å›¾æ ‡ */
  icons?: {
    idle?: React.ReactNode
    listening?: React.ReactNode
    processing?: React.ReactNode
    error?: React.ReactNode
  }
  
  // äº‹ä»¶å›è°ƒ
  /** å¼€å§‹å½•éŸ³æ—¶è§¦å‘ */
  onStart?: () => void
  /** åœæ­¢å½•éŸ³æ—¶è§¦å‘ */
  onStop?: () => void
  /** æ¥æ”¶åˆ°ç»“æœæ—¶è§¦å‘ */
  onResult?: (result: VoiceResult) => void
  /** æœ€ç»ˆç»“æœæ—¶è§¦å‘ */
  onFinalResult?: (text: string) => void
  /** å‘ç”Ÿé”™è¯¯æ—¶è§¦å‘ */
  onError?: (error: VoiceErrorType, message: string) => void
  /** çŠ¶æ€å˜åŒ–æ—¶è§¦å‘ */
  onStatusChange?: (status: VoiceStatus) => void
  
  // ARIA å’Œæ— éšœç¢
  'aria-label'?: string
}

// ==================== å¸¸é‡å®šä¹‰ ====================

/** é»˜è®¤æ”¯æŒçš„è¯­è¨€ */
const DEFAULT_LANGUAGES: LanguageOption[] = [
  { code: 'zh-CN', name: 'ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰', region: 'ä¸­å›½' },
  { code: 'zh-TW', name: 'ä¸­æ–‡ï¼ˆç¹é«”ï¼‰', region: 'å°ç£' },
  { code: 'en-US', name: 'English', region: 'United States' },
  { code: 'ja-JP', name: 'æ—¥æœ¬èª', region: 'æ—¥æœ¬' },
  { code: 'ko-KR', name: 'í•œêµ­ì–´', region: 'ëŒ€í•œë¯¼êµ­' },
]

/** é”™è¯¯æ¶ˆæ¯æ˜ å°„ */
const ERROR_MESSAGES: Record<VoiceErrorType, string> = {
  [VoiceErrorType.NOT_SUPPORTED]: 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½',
  [VoiceErrorType.NO_PERMISSION]: 'éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½ä½¿ç”¨è¯­éŸ³è¾“å…¥',
  [VoiceErrorType.NO_SPEECH]: 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•',
  [VoiceErrorType.NETWORK]: 'ç½‘ç»œé”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥',
  [VoiceErrorType.AUDIO_CAPTURE]: 'æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥è®¾å¤‡',
  [VoiceErrorType.SERVICE_NOT_ALLOWED]: 'è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨',
  [VoiceErrorType.UNKNOWN]: 'å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·é‡è¯•',
}

/** ARIA æ ‡ç­¾ */
const ARIA_LABELS = {
  button: 'è¯­éŸ³è¾“å…¥æŒ‰é’®',
  listening: 'æ­£åœ¨ç›‘å¬',
  processing: 'æ­£åœ¨å¤„ç†',
  languageSelect: 'é€‰æ‹©è¯­è¨€',
  stopButton: 'åœæ­¢å½•éŸ³',
}

// ==================== å·¥å…·å‡½æ•° ====================

/**
 * æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦æ”¯æŒè¯­éŸ³è¯†åˆ«
 */
const checkSpeechRecognitionSupport = (): boolean => {
  if (typeof window === 'undefined') return false
  
  const SpeechRecognition = 
    (window as any).SpeechRecognition || 
    (window as any).webkitSpeechRecognition
  
  return !!SpeechRecognition
}

/**
 * è·å– SpeechRecognition æ„é€ å‡½æ•°
 */
const getSpeechRecognition = (): any => {
  if (typeof window === 'undefined') return null
  
  return (window as any).SpeechRecognition || 
         (window as any).webkitSpeechRecognition
}

/**
 * æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º
 */
const formatDuration = (ms: number): string => {
  const seconds = Math.floor(ms / 1000)
  const minutes = Math.floor(seconds / 60)
  const remainingSeconds = seconds % 60
  
  if (minutes > 0) {
    return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`
  }
  return `${seconds}s`
}

// ==================== ä¸»ç»„ä»¶ ====================

/**
 * VoiceInput ç»„ä»¶
 */
export const VoiceInput: React.FC<VoiceInputProps> = ({
  autoStart = false,
  defaultLanguage = 'zh-CN',
  supportedLanguages = DEFAULT_LANGUAGES,
  showInterimResults = true,
  maxDuration = 60000, // é»˜è®¤ 60 ç§’
  continuous = true,
  showVisualizer = true,
  disabled = false,
  className,
  style,
  icons = {},
  onStart,
  onStop,
  onResult,
  onFinalResult,
  onError,
  onStatusChange,
  'aria-label': ariaLabel = ARIA_LABELS.button,
}) => {
  // ==================== çŠ¶æ€ç®¡ç† ====================
  
  const [status, setStatus] = useState<VoiceStatus>(VoiceStatus.IDLE)
  const [currentLanguage, setCurrentLanguage] = useState(defaultLanguage)
  const [interimText, setInterimText] = useState('')
  const [finalText, setFinalText] = useState('')
  const [errorMessage, setErrorMessage] = useState('')
  const [duration, setDuration] = useState(0)
  const [volume, setVolume] = useState(0)
  const [isSupported, setIsSupported] = useState(true)
  
  // ==================== Refs ====================
  
  const recognitionRef = useRef<any>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const startTimeRef = useRef<number>(0)
  const durationTimerRef = useRef<number | null>(null)
  const maxDurationTimerRef = useRef<number | null>(null)
  
  // ==================== æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ ====================
  
  useEffect(() => {
    const supported = checkSpeechRecognitionSupport()
    setIsSupported(supported)
    
    if (!supported) {
      handleError(VoiceErrorType.NOT_SUPPORTED)
    }
  }, [])
  
  // ==================== æ›´æ–°çŠ¶æ€ ====================
  
  const updateStatus = useCallback((newStatus: VoiceStatus) => {
    setStatus(newStatus)
    onStatusChange?.(newStatus)
  }, [onStatusChange])
  
  // ==================== é”™è¯¯å¤„ç† ====================
  
  const handleError = useCallback((errorType: VoiceErrorType, customMessage?: string) => {
    const message = customMessage || ERROR_MESSAGES[errorType]
    setErrorMessage(message)
    updateStatus(VoiceStatus.ERROR)
    onError?.(errorType, message)
    
    // 5 ç§’åæ¸…é™¤é”™è¯¯çŠ¶æ€
    setTimeout(() => {
      if (status === VoiceStatus.ERROR) {
        setErrorMessage('')
        updateStatus(VoiceStatus.IDLE)
      }
    }, 5000)
  }, [status, updateStatus, onError])
  
  // ==================== éŸ³é¢‘å¯è§†åŒ– ====================
  
  const setupAudioVisualizer = useCallback(async () => {
    if (!showVisualizer) return
    
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      
      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      const AudioContext = (window as any).AudioContext || (window as any).webkitAudioContext
      const audioContext = new AudioContext()
      audioContextRef.current = audioContext
      
      // åˆ›å»ºåˆ†æå™¨
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 256
      analyser.smoothingTimeConstant = 0.8
      analyserRef.current = analyser
      
      // è¿æ¥éº¦å…‹é£åˆ°åˆ†æå™¨
      const microphone = audioContext.createMediaStreamSource(stream)
      microphoneRef.current = microphone
      microphone.connect(analyser)
      
      // å¼€å§‹å¯è§†åŒ–
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      
      const updateVolume = () => {
        if (!analyserRef.current) return
        
        analyserRef.current.getByteFrequencyData(dataArray)
        
        // è®¡ç®—å¹³å‡éŸ³é‡
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length
        const normalizedVolume = Math.min(average / 128, 1)
        setVolume(normalizedVolume)
        
        animationFrameRef.current = requestAnimationFrame(updateVolume)
      }
      
      updateVolume()
    } catch (error) {
      console.error('Failed to setup audio visualizer:', error)
      handleError(VoiceErrorType.AUDIO_CAPTURE)
    }
  }, [showVisualizer, handleError])
  
  // ==================== æ¸…ç†éŸ³é¢‘èµ„æº ====================
  
  const cleanupAudioResources = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }
    
    if (microphoneRef.current) {
      microphoneRef.current.disconnect()
      microphoneRef.current = null
    }
    
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    
    analyserRef.current = null
    setVolume(0)
  }, [])
  
  // ==================== åˆå§‹åŒ–è¯­éŸ³è¯†åˆ« ====================
  
  const initializeSpeechRecognition = useCallback(() => {
    if (!isSupported) return null
    
    const SpeechRecognition = getSpeechRecognition()
    if (!SpeechRecognition) return null
    
    const recognition = new SpeechRecognition()
    
    // é…ç½®è¯†åˆ«å™¨
    recognition.lang = currentLanguage
    recognition.continuous = continuous
    recognition.interimResults = showInterimResults
    recognition.maxAlternatives = 1
    
    // ç»“æœå¤„ç†
    recognition.onresult = (event: any) => {
      let interim = ''
      let final = ''
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i]
        const transcript = result[0].transcript
        
        if (result.isFinal) {
          final += transcript
        } else {
          interim += transcript
        }
      }
      
      if (interim) {
        setInterimText(interim)
        onResult?.({
          text: interim,
          isFinal: false,
          confidence: event.results[event.resultIndex]?.[0]?.confidence,
          timestamp: Date.now(),
        })
      }
      
      if (final) {
        const newFinalText = finalText + final
        setFinalText(newFinalText)
        setInterimText('')
        onResult?.({
          text: final,
          isFinal: true,
          confidence: event.results[event.resultIndex]?.[0]?.confidence,
          timestamp: Date.now(),
        })
        onFinalResult?.(newFinalText)
      }
    }
    
    // é”™è¯¯å¤„ç†
    recognition.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error)
      
      switch (event.error) {
        case 'no-speech':
          handleError(VoiceErrorType.NO_SPEECH)
          break
        case 'audio-capture':
          handleError(VoiceErrorType.AUDIO_CAPTURE)
          break
        case 'not-allowed':
          handleError(VoiceErrorType.NO_PERMISSION)
          break
        case 'network':
          handleError(VoiceErrorType.NETWORK)
          break
        case 'service-not-allowed':
          handleError(VoiceErrorType.SERVICE_NOT_ALLOWED)
          break
        default:
          handleError(VoiceErrorType.UNKNOWN, `è¯†åˆ«é”™è¯¯: ${event.error}`)
      }
    }
    
    // è¯†åˆ«ç»“æŸ
    recognition.onend = () => {
      if (status === VoiceStatus.LISTENING) {
        // å¦‚æœæ˜¯è¿ç»­æ¨¡å¼ä¸”æœªæ‰‹åŠ¨åœæ­¢ï¼Œåˆ™é‡æ–°å¼€å§‹
        if (continuous && !disabled) {
          try {
            recognition.start()
          } catch (error) {
            console.error('Failed to restart recognition:', error)
          }
        } else {
          stopRecording()
        }
      }
    }
    
    // è¯†åˆ«å¼€å§‹
    recognition.onstart = () => {
      updateStatus(VoiceStatus.LISTENING)
      startTimeRef.current = Date.now()
      
      // å¼€å§‹è®¡æ—¶
      durationTimerRef.current = window.setInterval(() => {
        const elapsed = Date.now() - startTimeRef.current
        setDuration(elapsed)
      }, 100)
      
      // è®¾ç½®æœ€å¤§æ—¶é•¿å®šæ—¶å™¨
      if (maxDuration > 0) {
        maxDurationTimerRef.current = window.setTimeout(() => {
          stopRecording()
        }, maxDuration)
      }
    }
    
    return recognition
  }, [
    isSupported,
    currentLanguage,
    continuous,
    showInterimResults,
    maxDuration,
    status,
    disabled,
    finalText,
    updateStatus,
    handleError,
    onResult,
    onFinalResult,
  ])
  
  // ==================== å¼€å§‹å½•éŸ³ ====================
  
  const startRecording = useCallback(async () => {
    if (!isSupported || disabled || status === VoiceStatus.LISTENING) return
    
    try {
      // æ¸…ç†ä¹‹å‰çš„çŠ¶æ€
      setInterimText('')
      setFinalText('')
      setErrorMessage('')
      setDuration(0)
      
      // è®¾ç½®éŸ³é¢‘å¯è§†åŒ–
      await setupAudioVisualizer()
      
      // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
      const recognition = initializeSpeechRecognition()
      if (!recognition) {
        handleError(VoiceErrorType.NOT_SUPPORTED)
        return
      }
      
      recognitionRef.current = recognition
      
      // å¼€å§‹è¯†åˆ«
      recognition.start()
      onStart?.()
    } catch (error) {
      console.error('Failed to start recording:', error)
      handleError(VoiceErrorType.UNKNOWN, 'å¯åŠ¨å½•éŸ³å¤±è´¥')
    }
  }, [
    isSupported,
    disabled,
    status,
    setupAudioVisualizer,
    initializeSpeechRecognition,
    handleError,
    onStart,
  ])
  
  // ==================== åœæ­¢å½•éŸ³ ====================
  
  const stopRecording = useCallback(() => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop()
        recognitionRef.current = null
      } catch (error) {
        console.error('Failed to stop recognition:', error)
      }
    }
    
    // æ¸…ç†å®šæ—¶å™¨
    if (durationTimerRef.current) {
      clearInterval(durationTimerRef.current)
      durationTimerRef.current = null
    }
    
    if (maxDurationTimerRef.current) {
      clearTimeout(maxDurationTimerRef.current)
      maxDurationTimerRef.current = null
    }
    
    // æ¸…ç†éŸ³é¢‘èµ„æº
    cleanupAudioResources()
    
    updateStatus(VoiceStatus.IDLE)
    onStop?.()
  }, [cleanupAudioResources, updateStatus, onStop])
  
  // ==================== åˆ‡æ¢å½•éŸ³çŠ¶æ€ ====================
  
  const toggleRecording = useCallback(() => {
    if (status === VoiceStatus.LISTENING) {
      stopRecording()
    } else {
      startRecording()
    }
  }, [status, startRecording, stopRecording])
  
  // ==================== è‡ªåŠ¨å¼€å§‹ ====================
  
  useEffect(() => {
    if (autoStart && isSupported && !disabled) {
      startRecording()
    }
    
    return () => {
      stopRecording()
    }
  }, []) // åªåœ¨æŒ‚è½½æ—¶æ‰§è¡Œ
  
  // ==================== ç»„ä»¶å¸è½½æ—¶æ¸…ç† ====================
  
  useEffect(() => {
    return () => {
      stopRecording()
      cleanupAudioResources()
    }
  }, [stopRecording, cleanupAudioResources])
  
  // ==================== è®¡ç®—æ ·å¼ ====================
  
  const buttonClassName = useMemo(() => {
    return [
      styles.voiceButton,
      styles[`status-${status}`],
      disabled && styles.disabled,
      className,
    ].filter(Boolean).join(' ')
  }, [status, disabled, className])
  
  // ==================== éŸ³é¢‘æ³¢å½¢å¯è§†åŒ– ====================
  
  const renderVisualizer = () => {
    if (!showVisualizer || status !== VoiceStatus.LISTENING) return null
    
    const bars = Array.from({ length: 5 }, (_, i) => {
      const height = Math.max(20, volume * 100 * (1 - i * 0.15))
      const delay = i * 0.1
      
      return (
        <div
          key={i}
          className={styles.visualizerBar}
          style={{
            height: `${height}%`,
            animationDelay: `${delay}s`,
          }}
        />
      )
    })
    
    return <div className={styles.visualizer}>{bars}</div>
  }
  
  // ==================== æ¸²æŸ“å›¾æ ‡ ====================
  
  const renderIcon = () => {
    switch (status) {
      case VoiceStatus.LISTENING:
        return icons.listening || 'ğŸ™ï¸'
      case VoiceStatus.PROCESSING:
        return icons.processing || 'â³'
      case VoiceStatus.ERROR:
        return icons.error || 'âŒ'
      default:
        return icons.idle || 'ğŸ¤'
    }
  }
  
  // ==================== æ¸²æŸ“çŠ¶æ€æ–‡æœ¬ ====================
  
  const renderStatusText = () => {
    switch (status) {
      case VoiceStatus.LISTENING:
        return (
          <div className={styles.statusText}>
            <span className={styles.listeningIndicator}>â—</span>
            æ­£åœ¨ç›‘å¬...
            {maxDuration > 0 && (
              <span className={styles.duration}>
                {formatDuration(duration)} / {formatDuration(maxDuration)}
              </span>
            )}
          </div>
        )
      case VoiceStatus.PROCESSING:
        return <div className={styles.statusText}>å¤„ç†ä¸­...</div>
      case VoiceStatus.ERROR:
        return <div className={styles.errorText}>{errorMessage}</div>
      default:
        return null
    }
  }
  
  // ==================== æ¸²æŸ“è¯†åˆ«æ–‡æœ¬ ====================
  
  const renderTranscript = () => {
    if (!interimText && !finalText) return null
    
    return (
      <div className={styles.transcript}>
        {finalText && (
          <span className={styles.finalText}>{finalText}</span>
        )}
        {interimText && (
          <span className={styles.interimText}>{interimText}</span>
        )}
      </div>
    )
  }
  
  // ==================== ä¸æ”¯æŒæç¤º ====================
  
  if (!isSupported) {
    return (
      <div className={styles.unsupportedContainer}>
        <div className={styles.unsupportedIcon}>ğŸš«</div>
        <div className={styles.unsupportedText}>
          æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½
        </div>
        <div className={styles.unsupportedHint}>
          è¯·ä½¿ç”¨ Chromeã€Edge æˆ– Safari æµè§ˆå™¨
        </div>
      </div>
    )
  }
  
  // ==================== ä¸»æ¸²æŸ“ ====================
  
  return (
    <div className={styles.container} style={style}>
      {/* ä¸»æŒ‰é’® */}
      <button
        type="button"
        className={buttonClassName}
        onClick={toggleRecording}
        disabled={disabled}
        aria-label={ariaLabel}
        aria-pressed={status === VoiceStatus.LISTENING}
        title={status === VoiceStatus.LISTENING ? 'åœæ­¢å½•éŸ³' : 'å¼€å§‹å½•éŸ³'}
      >
        <span className={styles.icon}>{renderIcon()}</span>
        {renderVisualizer()}
      </button>
      
      {/* çŠ¶æ€ä¿¡æ¯ */}
      {renderStatusText()}
      
      {/* è¯†åˆ«æ–‡æœ¬ */}
      {renderTranscript()}
      
      {/* è¯­è¨€é€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰ */}
      {supportedLanguages.length > 1 && status === VoiceStatus.IDLE && (
        <select
          className={styles.languageSelect}
          value={currentLanguage}
          onChange={(e) => setCurrentLanguage(e.target.value)}
          disabled={disabled || status !== VoiceStatus.IDLE}
          aria-label={ARIA_LABELS.languageSelect}
        >
          {supportedLanguages.map((lang) => (
            <option key={lang.code} value={lang.code}>
              {lang.name}
            </option>
          ))}
        </select>
      )}
    </div>
  )
}

export default VoiceInput

